>> Start! : Sun Aug  1 06:43:49 2021
>>> seed = 42
>> data preprocessing
>> X_train, X_test, y_train, y_test --(save)--> *.npy, *.csv
labels_to_csv at ./data/y_train.csv
labels_to_csv at ./data/y_test.csv
features_to_npy at ./data/X_train : 250/2500
features_to_npy at ./data/X_train : 500/2500
features_to_npy at ./data/X_train : 750/2500
features_to_npy at ./data/X_train : 1000/2500
features_to_npy at ./data/X_train : 1250/2500
features_to_npy at ./data/X_train : 1500/2500
features_to_npy at ./data/X_train : 1750/2500
features_to_npy at ./data/X_train : 2000/2500
features_to_npy at ./data/X_train : 2250/2500
features_to_npy at ./data/X_train : 2500/2500
features_to_npy at ./data/X_test : 62/625
features_to_npy at ./data/X_test : 124/625
features_to_npy at ./data/X_test : 186/625
features_to_npy at ./data/X_test : 248/625
features_to_npy at ./data/X_test : 310/625
features_to_npy at ./data/X_test : 372/625
features_to_npy at ./data/X_test : 434/625
features_to_npy at ./data/X_test : 496/625
features_to_npy at ./data/X_test : 558/625
features_to_npy at ./data/X_test : 620/625
features_to_npy at ./data/X_test : 625/625
>> model building
>>> file_base_name : CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam
>>> model : CNN_RNN_Resnet
>>> model_parameter :
{'leakyrelu_alpha': 0.1, 'input_kernels': 20, 'input_kernel_width': 3, 'input_regularize_coeff': 0.001, 'res_kernels': 60, 'res_kernel_width': 3, 'res_regularize_coeff': 0.1, 'res_num': 5}
>>> num_of_fold : 5
>>> optimizer : Adam {'learning_rate': 0.001}
>>> rot_prob: 0.5, perm_prob: 0.5
>> model training
>> start - Process-1 at Sun Aug  1 06:44:26 2021
Epoch 1/1000
30/30 - 78s - loss: 200.6660 - accuracy: 0.3036 - val_loss: 133.3587 - val_accuracy: 0.4434

Epoch 00001: val_loss improved from inf to 133.35870, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 2/1000
30/30 - 13s - loss: 100.1497 - accuracy: 0.5292 - val_loss: 69.4933 - val_accuracy: 0.4648

Epoch 00002: val_loss improved from 133.35870 to 69.49331, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 3/1000
30/30 - 13s - loss: 54.5630 - accuracy: 0.5776 - val_loss: 39.1525 - val_accuracy: 0.4688

Epoch 00003: val_loss improved from 69.49331 to 39.15246, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 4/1000
30/30 - 14s - loss: 32.2257 - accuracy: 0.6068 - val_loss: 24.2833 - val_accuracy: 0.4629

Epoch 00004: val_loss improved from 39.15246 to 24.28325, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 5/1000
30/30 - 13s - loss: 21.3382 - accuracy: 0.6271 - val_loss: 17.2817 - val_accuracy: 0.4648

Epoch 00005: val_loss improved from 24.28325 to 17.28173, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 6/1000
30/30 - 13s - loss: 16.0921 - accuracy: 0.6135 - val_loss: 13.5738 - val_accuracy: 0.4590

Epoch 00006: val_loss improved from 17.28173 to 13.57383, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 7/1000
30/30 - 13s - loss: 13.0237 - accuracy: 0.6411 - val_loss: 11.6803 - val_accuracy: 0.4570

Epoch 00007: val_loss improved from 13.57383 to 11.68034, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 8/1000
30/30 - 14s - loss: 11.3015 - accuracy: 0.6755 - val_loss: 10.4370 - val_accuracy: 0.4609

Epoch 00008: val_loss improved from 11.68034 to 10.43695, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 9/1000
30/30 - 14s - loss: 10.2610 - accuracy: 0.6797 - val_loss: 9.8039 - val_accuracy: 0.4570

Epoch 00009: val_loss improved from 10.43695 to 9.80392, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 10/1000
30/30 - 13s - loss: 9.7321 - accuracy: 0.6755 - val_loss: 9.3108 - val_accuracy: 0.4648

Epoch 00010: val_loss improved from 9.80392 to 9.31084, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 11/1000
30/30 - 13s - loss: 8.9669 - accuracy: 0.6865 - val_loss: 8.9599 - val_accuracy: 0.4629

Epoch 00011: val_loss improved from 9.31084 to 8.95986, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 12/1000
30/30 - 14s - loss: 8.4331 - accuracy: 0.7021 - val_loss: 8.4437 - val_accuracy: 0.4375

Epoch 00012: val_loss improved from 8.95986 to 8.44366, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 13/1000
30/30 - 13s - loss: 7.8403 - accuracy: 0.7240 - val_loss: 8.2132 - val_accuracy: 0.4688

Epoch 00013: val_loss improved from 8.44366 to 8.21317, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 14/1000
30/30 - 13s - loss: 7.5674 - accuracy: 0.7286 - val_loss: 7.8686 - val_accuracy: 0.4863

Epoch 00014: val_loss improved from 8.21317 to 7.86860, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 15/1000
30/30 - 13s - loss: 7.3596 - accuracy: 0.7198 - val_loss: 7.8442 - val_accuracy: 0.2832

Epoch 00015: val_loss improved from 7.86860 to 7.84415, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 16/1000
30/30 - 13s - loss: 7.1155 - accuracy: 0.7271 - val_loss: 6.8454 - val_accuracy: 0.4883

Epoch 00016: val_loss improved from 7.84415 to 6.84537, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 17/1000
30/30 - 12s - loss: 6.7706 - accuracy: 0.7521 - val_loss: 7.6770 - val_accuracy: 0.2422

Epoch 00017: val_loss did not improve from 6.84537
Epoch 18/1000
30/30 - 13s - loss: 6.2951 - accuracy: 0.7615 - val_loss: 6.1598 - val_accuracy: 0.5293

Epoch 00018: val_loss improved from 6.84537 to 6.15984, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 19/1000
30/30 - 13s - loss: 6.1453 - accuracy: 0.7823 - val_loss: 6.2424 - val_accuracy: 0.4668

Epoch 00019: val_loss did not improve from 6.15984
Epoch 20/1000
30/30 - 12s - loss: 5.9382 - accuracy: 0.7656 - val_loss: 5.4627 - val_accuracy: 0.5625

Epoch 00020: val_loss improved from 6.15984 to 5.46266, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 21/1000
30/30 - 12s - loss: 5.9222 - accuracy: 0.7510 - val_loss: 6.7952 - val_accuracy: 0.2324

Epoch 00021: val_loss did not improve from 5.46266
Epoch 22/1000
30/30 - 12s - loss: 5.6672 - accuracy: 0.7708 - val_loss: 5.7531 - val_accuracy: 0.5449

Epoch 00022: val_loss did not improve from 5.46266
Epoch 23/1000
30/30 - 13s - loss: 5.2673 - accuracy: 0.7969 - val_loss: 5.9357 - val_accuracy: 0.3828

Epoch 00023: val_loss did not improve from 5.46266
Epoch 24/1000
30/30 - 13s - loss: 5.5342 - accuracy: 0.7505 - val_loss: 5.7599 - val_accuracy: 0.4492

Epoch 00024: val_loss did not improve from 5.46266

Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 25/1000
30/30 - 12s - loss: 4.7711 - accuracy: 0.8115 - val_loss: 5.4258 - val_accuracy: 0.4199

Epoch 00025: val_loss improved from 5.46266 to 5.42583, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 26/1000
30/30 - 12s - loss: 4.2521 - accuracy: 0.8354 - val_loss: 4.6016 - val_accuracy: 0.5527

Epoch 00026: val_loss improved from 5.42583 to 4.60156, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 27/1000
30/30 - 12s - loss: 4.1736 - accuracy: 0.8286 - val_loss: 4.9336 - val_accuracy: 0.4980

Epoch 00027: val_loss did not improve from 4.60156
Epoch 28/1000
30/30 - 13s - loss: 4.0747 - accuracy: 0.8406 - val_loss: 4.3472 - val_accuracy: 0.5723

Epoch 00028: val_loss improved from 4.60156 to 4.34725, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 29/1000
30/30 - 13s - loss: 4.0143 - accuracy: 0.8469 - val_loss: 3.7304 - val_accuracy: 0.6582

Epoch 00029: val_loss improved from 4.34725 to 3.73039, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 30/1000
30/30 - 12s - loss: 4.0658 - accuracy: 0.8177 - val_loss: 5.5029 - val_accuracy: 0.3203

Epoch 00030: val_loss did not improve from 3.73039
Epoch 31/1000
30/30 - 12s - loss: 4.0115 - accuracy: 0.8406 - val_loss: 4.1515 - val_accuracy: 0.6055

Epoch 00031: val_loss did not improve from 3.73039
Epoch 32/1000
30/30 - 12s - loss: 3.6813 - accuracy: 0.8656 - val_loss: 4.0759 - val_accuracy: 0.5898

Epoch 00032: val_loss did not improve from 3.73039
Epoch 33/1000
30/30 - 13s - loss: 3.9251 - accuracy: 0.8286 - val_loss: 4.0231 - val_accuracy: 0.5723

Epoch 00033: val_loss did not improve from 3.73039

Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 34/1000
30/30 - 13s - loss: 3.5866 - accuracy: 0.8703 - val_loss: 3.8583 - val_accuracy: 0.5918

Epoch 00034: val_loss did not improve from 3.73039
Epoch 35/1000
30/30 - 14s - loss: 3.3665 - accuracy: 0.8865 - val_loss: 3.6359 - val_accuracy: 0.6328

Epoch 00035: val_loss improved from 3.73039 to 3.63592, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 36/1000
30/30 - 12s - loss: 3.2768 - accuracy: 0.8682 - val_loss: 3.5409 - val_accuracy: 0.6680

Epoch 00036: val_loss improved from 3.63592 to 3.54086, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 37/1000
30/30 - 13s - loss: 3.0780 - accuracy: 0.8943 - val_loss: 3.3605 - val_accuracy: 0.7109

Epoch 00037: val_loss improved from 3.54086 to 3.36048, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 38/1000
30/30 - 12s - loss: 2.9610 - accuracy: 0.9010 - val_loss: 3.0863 - val_accuracy: 0.7129

Epoch 00038: val_loss improved from 3.36048 to 3.08633, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 39/1000
30/30 - 13s - loss: 3.0980 - accuracy: 0.8740 - val_loss: 3.1610 - val_accuracy: 0.6875

Epoch 00039: val_loss did not improve from 3.08633
Epoch 40/1000
30/30 - 13s - loss: 3.0378 - accuracy: 0.8854 - val_loss: 3.5497 - val_accuracy: 0.6367

Epoch 00040: val_loss did not improve from 3.08633
Epoch 41/1000
30/30 - 12s - loss: 2.8625 - accuracy: 0.9010 - val_loss: 3.2959 - val_accuracy: 0.6914

Epoch 00041: val_loss did not improve from 3.08633
Epoch 42/1000
30/30 - 12s - loss: 2.8703 - accuracy: 0.9005 - val_loss: 3.2630 - val_accuracy: 0.6641

Epoch 00042: val_loss did not improve from 3.08633

Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 43/1000
30/30 - 13s - loss: 2.7370 - accuracy: 0.9182 - val_loss: 3.0834 - val_accuracy: 0.7207

Epoch 00043: val_loss improved from 3.08633 to 3.08337, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 44/1000
30/30 - 13s - loss: 2.4993 - accuracy: 0.9365 - val_loss: 3.0710 - val_accuracy: 0.7051

Epoch 00044: val_loss improved from 3.08337 to 3.07098, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 45/1000
30/30 - 12s - loss: 2.6764 - accuracy: 0.9115 - val_loss: 3.0138 - val_accuracy: 0.7129

Epoch 00045: val_loss improved from 3.07098 to 3.01382, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 46/1000
30/30 - 12s - loss: 2.6271 - accuracy: 0.9260 - val_loss: 2.9493 - val_accuracy: 0.7539

Epoch 00046: val_loss improved from 3.01382 to 2.94934, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 47/1000
30/30 - 12s - loss: 2.6166 - accuracy: 0.9115 - val_loss: 2.7606 - val_accuracy: 0.7695

Epoch 00047: val_loss improved from 2.94934 to 2.76056, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 48/1000
30/30 - 13s - loss: 2.4888 - accuracy: 0.9260 - val_loss: 2.7136 - val_accuracy: 0.7871

Epoch 00048: val_loss improved from 2.76056 to 2.71361, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 49/1000
30/30 - 13s - loss: 2.5121 - accuracy: 0.9214 - val_loss: 2.5991 - val_accuracy: 0.7988

Epoch 00049: val_loss improved from 2.71361 to 2.59915, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 50/1000
30/30 - 12s - loss: 2.5166 - accuracy: 0.9151 - val_loss: 2.7439 - val_accuracy: 0.7598

Epoch 00050: val_loss did not improve from 2.59915
Epoch 51/1000
30/30 - 12s - loss: 2.5668 - accuracy: 0.9109 - val_loss: 3.0444 - val_accuracy: 0.7168

Epoch 00051: val_loss did not improve from 2.59915
Epoch 52/1000
30/30 - 12s - loss: 2.4425 - accuracy: 0.9219 - val_loss: 3.0895 - val_accuracy: 0.7383

Epoch 00052: val_loss did not improve from 2.59915
Epoch 53/1000
30/30 - 13s - loss: 2.3210 - accuracy: 0.9385 - val_loss: 2.7557 - val_accuracy: 0.7598

Epoch 00053: val_loss did not improve from 2.59915

Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 54/1000
30/30 - 12s - loss: 2.3390 - accuracy: 0.9417 - val_loss: 2.5519 - val_accuracy: 0.7949

Epoch 00054: val_loss improved from 2.59915 to 2.55194, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 55/1000
30/30 - 12s - loss: 2.2430 - accuracy: 0.9417 - val_loss: 2.5537 - val_accuracy: 0.7910

Epoch 00055: val_loss did not improve from 2.55194
Epoch 56/1000
30/30 - 12s - loss: 2.1461 - accuracy: 0.9594 - val_loss: 2.4815 - val_accuracy: 0.7949

Epoch 00056: val_loss improved from 2.55194 to 2.48145, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 57/1000
30/30 - 13s - loss: 2.1739 - accuracy: 0.9542 - val_loss: 2.4233 - val_accuracy: 0.7988

Epoch 00057: val_loss improved from 2.48145 to 2.42328, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 58/1000
30/30 - 13s - loss: 2.1340 - accuracy: 0.9573 - val_loss: 2.3274 - val_accuracy: 0.8398

Epoch 00058: val_loss improved from 2.42328 to 2.32737, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 59/1000
30/30 - 12s - loss: 2.0554 - accuracy: 0.9630 - val_loss: 2.3764 - val_accuracy: 0.8066

Epoch 00059: val_loss did not improve from 2.32737
Epoch 60/1000
30/30 - 12s - loss: 2.2516 - accuracy: 0.9323 - val_loss: 2.4088 - val_accuracy: 0.7969

Epoch 00060: val_loss did not improve from 2.32737
Epoch 61/1000
30/30 - 12s - loss: 2.1657 - accuracy: 0.9526 - val_loss: 2.3392 - val_accuracy: 0.8301

Epoch 00061: val_loss did not improve from 2.32737
Epoch 62/1000
30/30 - 13s - loss: 2.0393 - accuracy: 0.9641 - val_loss: 2.2642 - val_accuracy: 0.8320

Epoch 00062: val_loss improved from 2.32737 to 2.26421, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 63/1000
30/30 - 12s - loss: 2.1834 - accuracy: 0.9453 - val_loss: 2.4204 - val_accuracy: 0.8125

Epoch 00063: val_loss did not improve from 2.26421
Epoch 64/1000
30/30 - 13s - loss: 2.1736 - accuracy: 0.9401 - val_loss: 2.4678 - val_accuracy: 0.7949

Epoch 00064: val_loss did not improve from 2.26421
Epoch 65/1000
30/30 - 12s - loss: 2.1072 - accuracy: 0.9531 - val_loss: 2.4761 - val_accuracy: 0.8027

Epoch 00065: val_loss did not improve from 2.26421
Epoch 66/1000
30/30 - 12s - loss: 2.0944 - accuracy: 0.9495 - val_loss: 2.4676 - val_accuracy: 0.8008

Epoch 00066: val_loss did not improve from 2.26421

Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 67/1000
30/30 - 12s - loss: 1.9370 - accuracy: 0.9724 - val_loss: 2.2911 - val_accuracy: 0.8398

Epoch 00067: val_loss did not improve from 2.26421
Epoch 68/1000
30/30 - 13s - loss: 1.9062 - accuracy: 0.9714 - val_loss: 2.2790 - val_accuracy: 0.8457

Epoch 00068: val_loss did not improve from 2.26421
Epoch 69/1000
30/30 - 12s - loss: 1.9671 - accuracy: 0.9651 - val_loss: 2.2674 - val_accuracy: 0.8340

Epoch 00069: val_loss did not improve from 2.26421
Epoch 70/1000
30/30 - 12s - loss: 1.9879 - accuracy: 0.9594 - val_loss: 2.3014 - val_accuracy: 0.8359

Epoch 00070: val_loss did not improve from 2.26421

Epoch 00070: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 71/1000
30/30 - 13s - loss: 1.9831 - accuracy: 0.9635 - val_loss: 2.2782 - val_accuracy: 0.8496

Epoch 00071: val_loss did not improve from 2.26421
Epoch 72/1000
30/30 - 12s - loss: 2.0450 - accuracy: 0.9516 - val_loss: 2.2574 - val_accuracy: 0.8438

Epoch 00072: val_loss improved from 2.26421 to 2.25745, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 73/1000
30/30 - 12s - loss: 1.9534 - accuracy: 0.9661 - val_loss: 2.2327 - val_accuracy: 0.8438

Epoch 00073: val_loss improved from 2.25745 to 2.23267, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 74/1000
30/30 - 12s - loss: 1.8720 - accuracy: 0.9766 - val_loss: 2.2429 - val_accuracy: 0.8496

Epoch 00074: val_loss did not improve from 2.23267
Epoch 75/1000
30/30 - 11s - loss: 1.9038 - accuracy: 0.9698 - val_loss: 2.2349 - val_accuracy: 0.8457

Epoch 00075: val_loss did not improve from 2.23267
Epoch 76/1000
30/30 - 12s - loss: 1.9330 - accuracy: 0.9641 - val_loss: 2.1999 - val_accuracy: 0.8477

Epoch 00076: val_loss improved from 2.23267 to 2.19994, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 77/1000
30/30 - 12s - loss: 1.8427 - accuracy: 0.9771 - val_loss: 2.2311 - val_accuracy: 0.8438

Epoch 00077: val_loss did not improve from 2.19994
Epoch 78/1000
30/30 - 11s - loss: 1.8094 - accuracy: 0.9854 - val_loss: 2.1955 - val_accuracy: 0.8574

Epoch 00078: val_loss improved from 2.19994 to 2.19552, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 79/1000
30/30 - 12s - loss: 1.9609 - accuracy: 0.9589 - val_loss: 2.2078 - val_accuracy: 0.8594

Epoch 00079: val_loss did not improve from 2.19552
Epoch 80/1000
30/30 - 12s - loss: 1.9612 - accuracy: 0.9609 - val_loss: 2.2320 - val_accuracy: 0.8418

Epoch 00080: val_loss did not improve from 2.19552
Epoch 81/1000
30/30 - 12s - loss: 1.9005 - accuracy: 0.9703 - val_loss: 2.2091 - val_accuracy: 0.8418

Epoch 00081: val_loss did not improve from 2.19552
Epoch 82/1000
30/30 - 12s - loss: 1.8571 - accuracy: 0.9729 - val_loss: 2.1781 - val_accuracy: 0.8555

Epoch 00082: val_loss improved from 2.19552 to 2.17814, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 83/1000
30/30 - 12s - loss: 1.9554 - accuracy: 0.9625 - val_loss: 2.1871 - val_accuracy: 0.8418

Epoch 00083: val_loss did not improve from 2.17814
Epoch 84/1000
30/30 - 12s - loss: 1.8422 - accuracy: 0.9719 - val_loss: 2.2014 - val_accuracy: 0.8496

Epoch 00084: val_loss did not improve from 2.17814
Epoch 85/1000
30/30 - 11s - loss: 1.8754 - accuracy: 0.9688 - val_loss: 2.2013 - val_accuracy: 0.8496

Epoch 00085: val_loss did not improve from 2.17814
Epoch 86/1000
30/30 - 11s - loss: 1.8557 - accuracy: 0.9740 - val_loss: 2.2214 - val_accuracy: 0.8574

Epoch 00086: val_loss did not improve from 2.17814

Epoch 00086: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 87/1000
30/30 - 11s - loss: 1.8563 - accuracy: 0.9714 - val_loss: 2.1722 - val_accuracy: 0.8613

Epoch 00087: val_loss improved from 2.17814 to 2.17221, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 88/1000
30/30 - 12s - loss: 1.7787 - accuracy: 0.9802 - val_loss: 2.1747 - val_accuracy: 0.8594

Epoch 00088: val_loss did not improve from 2.17221
Epoch 89/1000
30/30 - 12s - loss: 1.8731 - accuracy: 0.9693 - val_loss: 2.1805 - val_accuracy: 0.8477

Epoch 00089: val_loss did not improve from 2.17221
Epoch 90/1000
30/30 - 11s - loss: 1.8453 - accuracy: 0.9734 - val_loss: 2.1678 - val_accuracy: 0.8516

Epoch 00090: val_loss improved from 2.17221 to 2.16776, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 91/1000
30/30 - 12s - loss: 1.7469 - accuracy: 0.9839 - val_loss: 2.1872 - val_accuracy: 0.8516

Epoch 00091: val_loss did not improve from 2.16776
Epoch 92/1000
30/30 - 12s - loss: 1.8161 - accuracy: 0.9766 - val_loss: 2.1620 - val_accuracy: 0.8555

Epoch 00092: val_loss improved from 2.16776 to 2.16202, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 93/1000
30/30 - 12s - loss: 1.8315 - accuracy: 0.9792 - val_loss: 2.1915 - val_accuracy: 0.8418

Epoch 00093: val_loss did not improve from 2.16202
Epoch 94/1000
30/30 - 11s - loss: 1.7904 - accuracy: 0.9792 - val_loss: 2.1698 - val_accuracy: 0.8496

Epoch 00094: val_loss did not improve from 2.16202
Epoch 95/1000
30/30 - 11s - loss: 1.8590 - accuracy: 0.9708 - val_loss: 2.1815 - val_accuracy: 0.8457

Epoch 00095: val_loss did not improve from 2.16202
Epoch 96/1000
30/30 - 12s - loss: 1.8118 - accuracy: 0.9771 - val_loss: 2.1936 - val_accuracy: 0.8379

Epoch 00096: val_loss did not improve from 2.16202

Epoch 00096: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 97/1000
30/30 - 12s - loss: 1.8197 - accuracy: 0.9750 - val_loss: 2.1726 - val_accuracy: 0.8477

Epoch 00097: val_loss did not improve from 2.16202
Epoch 98/1000
30/30 - 12s - loss: 1.8224 - accuracy: 0.9729 - val_loss: 2.1923 - val_accuracy: 0.8477

Epoch 00098: val_loss did not improve from 2.16202
Epoch 99/1000
30/30 - 11s - loss: 1.8141 - accuracy: 0.9708 - val_loss: 2.1844 - val_accuracy: 0.8438

Epoch 00099: val_loss did not improve from 2.16202
Epoch 100/1000
30/30 - 11s - loss: 1.7953 - accuracy: 0.9797 - val_loss: 2.1558 - val_accuracy: 0.8535

Epoch 00100: val_loss improved from 2.16202 to 2.15577, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 101/1000
30/30 - 12s - loss: 1.8052 - accuracy: 0.9724 - val_loss: 2.1586 - val_accuracy: 0.8535

Epoch 00101: val_loss did not improve from 2.15577
Epoch 102/1000
30/30 - 12s - loss: 1.7659 - accuracy: 0.9839 - val_loss: 2.1890 - val_accuracy: 0.8516

Epoch 00102: val_loss did not improve from 2.15577
Epoch 103/1000
30/30 - 11s - loss: 1.7872 - accuracy: 0.9766 - val_loss: 2.1740 - val_accuracy: 0.8535

Epoch 00103: val_loss did not improve from 2.15577
Epoch 104/1000
30/30 - 11s - loss: 1.8042 - accuracy: 0.9734 - val_loss: 2.1882 - val_accuracy: 0.8438

Epoch 00104: val_loss did not improve from 2.15577

Epoch 00104: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 105/1000
30/30 - 11s - loss: 1.7535 - accuracy: 0.9839 - val_loss: 2.1804 - val_accuracy: 0.8477

Epoch 00105: val_loss did not improve from 2.15577
Epoch 106/1000
30/30 - 12s - loss: 1.7290 - accuracy: 0.9865 - val_loss: 2.1574 - val_accuracy: 0.8535

Epoch 00106: val_loss did not improve from 2.15577
Epoch 107/1000
30/30 - 12s - loss: 1.7785 - accuracy: 0.9786 - val_loss: 2.1697 - val_accuracy: 0.8535

Epoch 00107: val_loss did not improve from 2.15577
Epoch 108/1000
30/30 - 12s - loss: 1.7722 - accuracy: 0.9786 - val_loss: 2.1656 - val_accuracy: 0.8555

Epoch 00108: val_loss did not improve from 2.15577

Epoch 00108: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
Epoch 109/1000
30/30 - 12s - loss: 1.7503 - accuracy: 0.9849 - val_loss: 2.1589 - val_accuracy: 0.8496

Epoch 00109: val_loss did not improve from 2.15577
Epoch 110/1000
30/30 - 12s - loss: 1.7188 - accuracy: 0.9891 - val_loss: 2.1431 - val_accuracy: 0.8555

Epoch 00110: val_loss improved from 2.15577 to 2.14310, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_0.hdf5
Epoch 111/1000
30/30 - 12s - loss: 1.7989 - accuracy: 0.9792 - val_loss: 2.1868 - val_accuracy: 0.8438

Epoch 00111: val_loss did not improve from 2.14310
Epoch 112/1000
30/30 - 11s - loss: 1.8041 - accuracy: 0.9724 - val_loss: 2.1692 - val_accuracy: 0.8457

Epoch 00112: val_loss did not improve from 2.14310
Epoch 113/1000
30/30 - 12s - loss: 1.7709 - accuracy: 0.9833 - val_loss: 2.1653 - val_accuracy: 0.8477

Epoch 00113: val_loss did not improve from 2.14310
Epoch 114/1000
30/30 - 11s - loss: 1.7350 - accuracy: 0.9828 - val_loss: 2.1660 - val_accuracy: 0.8516

Epoch 00114: val_loss did not improve from 2.14310

Epoch 00114: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
Epoch 115/1000
30/30 - 11s - loss: 1.7548 - accuracy: 0.9823 - val_loss: 2.1558 - val_accuracy: 0.8496

Epoch 00115: val_loss did not improve from 2.14310
Epoch 116/1000
30/30 - 11s - loss: 1.7886 - accuracy: 0.9781 - val_loss: 2.1842 - val_accuracy: 0.8477

Epoch 00116: val_loss did not improve from 2.14310
Epoch 117/1000
30/30 - 12s - loss: 1.8343 - accuracy: 0.9703 - val_loss: 2.1829 - val_accuracy: 0.8477

Epoch 00117: val_loss did not improve from 2.14310
Epoch 118/1000
30/30 - 12s - loss: 1.7382 - accuracy: 0.9844 - val_loss: 2.1719 - val_accuracy: 0.8496

Epoch 00118: val_loss did not improve from 2.14310

Epoch 00118: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
Epoch 119/1000
30/30 - 11s - loss: 1.7977 - accuracy: 0.9755 - val_loss: 2.1959 - val_accuracy: 0.8457

Epoch 00119: val_loss did not improve from 2.14310
Epoch 120/1000
30/30 - 11s - loss: 1.8675 - accuracy: 0.9693 - val_loss: 2.1988 - val_accuracy: 0.8438

Epoch 00120: val_loss did not improve from 2.14310
Epoch 121/1000
30/30 - 11s - loss: 1.8026 - accuracy: 0.9776 - val_loss: 2.1880 - val_accuracy: 0.8477

Epoch 00121: val_loss did not improve from 2.14310
Epoch 122/1000
30/30 - 12s - loss: 1.8291 - accuracy: 0.9698 - val_loss: 2.1812 - val_accuracy: 0.8496

Epoch 00122: val_loss did not improve from 2.14310

Epoch 00122: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.
Epoch 123/1000
30/30 - 12s - loss: 1.7351 - accuracy: 0.9865 - val_loss: 2.1493 - val_accuracy: 0.8535

Epoch 00123: val_loss did not improve from 2.14310
Epoch 124/1000
30/30 - 12s - loss: 1.8030 - accuracy: 0.9781 - val_loss: 2.1856 - val_accuracy: 0.8457

Epoch 00124: val_loss did not improve from 2.14310
Epoch 125/1000
30/30 - 11s - loss: 1.8035 - accuracy: 0.9818 - val_loss: 2.1963 - val_accuracy: 0.8477

Epoch 00125: val_loss did not improve from 2.14310
Epoch 126/1000
30/30 - 11s - loss: 1.7890 - accuracy: 0.9760 - val_loss: 2.1759 - val_accuracy: 0.8477

Epoch 00126: val_loss did not improve from 2.14310

Epoch 00126: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.
Epoch 127/1000
30/30 - 11s - loss: 1.7465 - accuracy: 0.9828 - val_loss: 2.1917 - val_accuracy: 0.8496

Epoch 00127: val_loss did not improve from 2.14310
Epoch 128/1000
30/30 - 12s - loss: 1.7165 - accuracy: 0.9865 - val_loss: 2.1641 - val_accuracy: 0.8535

Epoch 00128: val_loss did not improve from 2.14310
Epoch 129/1000
30/30 - 12s - loss: 1.7357 - accuracy: 0.9849 - val_loss: 2.1770 - val_accuracy: 0.8477

Epoch 00129: val_loss did not improve from 2.14310
Epoch 130/1000
30/30 - 12s - loss: 1.7578 - accuracy: 0.9802 - val_loss: 2.1724 - val_accuracy: 0.8496

Epoch 00130: val_loss did not improve from 2.14310

Epoch 00130: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.
Epoch 131/1000
30/30 - 12s - loss: 1.8041 - accuracy: 0.9797 - val_loss: 2.1752 - val_accuracy: 0.8535

Epoch 00131: val_loss did not improve from 2.14310
Epoch 132/1000
30/30 - 12s - loss: 1.8175 - accuracy: 0.9729 - val_loss: 2.1694 - val_accuracy: 0.8516

Epoch 00132: val_loss did not improve from 2.14310
Epoch 133/1000
30/30 - 11s - loss: 1.7937 - accuracy: 0.9792 - val_loss: 2.1903 - val_accuracy: 0.8477

Epoch 00133: val_loss did not improve from 2.14310
Epoch 134/1000
30/30 - 11s - loss: 1.8385 - accuracy: 0.9677 - val_loss: 2.1975 - val_accuracy: 0.8477

Epoch 00134: val_loss did not improve from 2.14310

Epoch 00134: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.
Epoch 135/1000
30/30 - 11s - loss: 1.7524 - accuracy: 0.9849 - val_loss: 2.1669 - val_accuracy: 0.8535

Epoch 00135: val_loss did not improve from 2.14310
Epoch 136/1000
30/30 - 12s - loss: 1.7415 - accuracy: 0.9833 - val_loss: 2.1837 - val_accuracy: 0.8496

Epoch 00136: val_loss did not improve from 2.14310
Epoch 137/1000
30/30 - 12s - loss: 1.7201 - accuracy: 0.9854 - val_loss: 2.1883 - val_accuracy: 0.8457

Epoch 00137: val_loss did not improve from 2.14310
Epoch 138/1000
30/30 - 11s - loss: 1.8151 - accuracy: 0.9740 - val_loss: 2.1860 - val_accuracy: 0.8555

Epoch 00138: val_loss did not improve from 2.14310

Epoch 00138: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.
Epoch 139/1000
30/30 - 11s - loss: 1.8138 - accuracy: 0.9776 - val_loss: 2.2032 - val_accuracy: 0.8477

Epoch 00139: val_loss did not improve from 2.14310
Epoch 140/1000
30/30 - 12s - loss: 1.7346 - accuracy: 0.9875 - val_loss: 2.2028 - val_accuracy: 0.8438

Epoch 00140: val_loss did not improve from 2.14310
Epoch 00140: early stopping
>> join - Process-1 at Sun Aug  1 07:19:32 2021
>> Process-1 spend 0h 35m 5.9s

>> start - Process-2 at Sun Aug  1 07:19:36 2021
Epoch 1/1000
31/31 - 66s - loss: 198.4413 - accuracy: 0.2954 - val_loss: 129.6975 - val_accuracy: 0.4819

Epoch 00001: val_loss improved from inf to 129.69754, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 2/1000
31/31 - 12s - loss: 97.5931 - accuracy: 0.5071 - val_loss: 66.6032 - val_accuracy: 0.4778

Epoch 00002: val_loss improved from 129.69754 to 66.60317, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 3/1000
31/31 - 12s - loss: 52.2781 - accuracy: 0.5484 - val_loss: 37.1179 - val_accuracy: 0.4778

Epoch 00003: val_loss improved from 66.60317 to 37.11794, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 4/1000
31/31 - 12s - loss: 30.6542 - accuracy: 0.5932 - val_loss: 23.2086 - val_accuracy: 0.4698

Epoch 00004: val_loss improved from 37.11794 to 23.20859, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 5/1000
31/31 - 12s - loss: 20.4464 - accuracy: 0.6079 - val_loss: 16.2887 - val_accuracy: 0.4798

Epoch 00005: val_loss improved from 23.20859 to 16.28875, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 6/1000
31/31 - 12s - loss: 15.2274 - accuracy: 0.6381 - val_loss: 13.0196 - val_accuracy: 0.4859

Epoch 00006: val_loss improved from 16.28875 to 13.01957, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 7/1000
31/31 - 12s - loss: 12.4858 - accuracy: 0.6653 - val_loss: 11.3878 - val_accuracy: 0.4718

Epoch 00007: val_loss improved from 13.01957 to 11.38776, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 8/1000
31/31 - 12s - loss: 10.9929 - accuracy: 0.6739 - val_loss: 10.7260 - val_accuracy: 0.4738

Epoch 00008: val_loss improved from 11.38776 to 10.72601, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 9/1000
31/31 - 12s - loss: 9.9518 - accuracy: 0.6830 - val_loss: 9.9291 - val_accuracy: 0.3105

Epoch 00009: val_loss improved from 10.72601 to 9.92914, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 10/1000
31/31 - 12s - loss: 9.4810 - accuracy: 0.6825 - val_loss: 9.4452 - val_accuracy: 0.4798

Epoch 00010: val_loss improved from 9.92914 to 9.44521, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 11/1000
31/31 - 12s - loss: 9.0829 - accuracy: 0.6709 - val_loss: 9.7150 - val_accuracy: 0.1331

Epoch 00011: val_loss did not improve from 9.44521
Epoch 12/1000
31/31 - 12s - loss: 8.2388 - accuracy: 0.7006 - val_loss: 8.8801 - val_accuracy: 0.3629

Epoch 00012: val_loss improved from 9.44521 to 8.88011, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 13/1000
31/31 - 12s - loss: 7.9459 - accuracy: 0.7077 - val_loss: 8.4497 - val_accuracy: 0.4798

Epoch 00013: val_loss improved from 8.88011 to 8.44971, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 14/1000
31/31 - 12s - loss: 7.5142 - accuracy: 0.7102 - val_loss: 7.5365 - val_accuracy: 0.4839

Epoch 00014: val_loss improved from 8.44971 to 7.53653, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 15/1000
31/31 - 12s - loss: 7.1837 - accuracy: 0.7268 - val_loss: 7.5234 - val_accuracy: 0.4456

Epoch 00015: val_loss improved from 7.53653 to 7.52336, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 16/1000
31/31 - 12s - loss: 6.8763 - accuracy: 0.7188 - val_loss: 6.6020 - val_accuracy: 0.5060

Epoch 00016: val_loss improved from 7.52336 to 6.60203, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 17/1000
31/31 - 12s - loss: 6.4891 - accuracy: 0.7500 - val_loss: 7.0575 - val_accuracy: 0.5121

Epoch 00017: val_loss did not improve from 6.60203
Epoch 18/1000
31/31 - 12s - loss: 6.3193 - accuracy: 0.7530 - val_loss: 7.3645 - val_accuracy: 0.0887

Epoch 00018: val_loss did not improve from 6.60203
Epoch 19/1000
31/31 - 12s - loss: 6.0994 - accuracy: 0.7540 - val_loss: 5.5667 - val_accuracy: 0.5242

Epoch 00019: val_loss improved from 6.60203 to 5.56675, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 20/1000
31/31 - 12s - loss: 5.9433 - accuracy: 0.7555 - val_loss: 6.4706 - val_accuracy: 0.4940

Epoch 00020: val_loss did not improve from 5.56675
Epoch 21/1000
31/31 - 12s - loss: 5.8121 - accuracy: 0.7480 - val_loss: 5.2062 - val_accuracy: 0.5585

Epoch 00021: val_loss improved from 5.56675 to 5.20618, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 22/1000
31/31 - 12s - loss: 5.4689 - accuracy: 0.7681 - val_loss: 6.2140 - val_accuracy: 0.4677

Epoch 00022: val_loss did not improve from 5.20618
Epoch 23/1000
31/31 - 12s - loss: 5.3228 - accuracy: 0.7908 - val_loss: 5.2390 - val_accuracy: 0.5383

Epoch 00023: val_loss did not improve from 5.20618
Epoch 24/1000
31/31 - 12s - loss: 5.2780 - accuracy: 0.7737 - val_loss: 6.0665 - val_accuracy: 0.3065

Epoch 00024: val_loss did not improve from 5.20618
Epoch 25/1000
31/31 - 11s - loss: 5.2320 - accuracy: 0.7737 - val_loss: 5.0056 - val_accuracy: 0.5706

Epoch 00025: val_loss improved from 5.20618 to 5.00557, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 26/1000
31/31 - 12s - loss: 5.1022 - accuracy: 0.7802 - val_loss: 4.7018 - val_accuracy: 0.5484

Epoch 00026: val_loss improved from 5.00557 to 4.70185, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 27/1000
31/31 - 12s - loss: 4.8762 - accuracy: 0.7792 - val_loss: 4.3311 - val_accuracy: 0.6190

Epoch 00027: val_loss improved from 4.70185 to 4.33114, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 28/1000
31/31 - 11s - loss: 4.8766 - accuracy: 0.7762 - val_loss: 5.6557 - val_accuracy: 0.2560

Epoch 00028: val_loss did not improve from 4.33114
Epoch 29/1000
31/31 - 11s - loss: 4.6888 - accuracy: 0.8009 - val_loss: 8.9716 - val_accuracy: 0.0927

Epoch 00029: val_loss did not improve from 4.33114
Epoch 30/1000
31/31 - 12s - loss: 4.2904 - accuracy: 0.8216 - val_loss: 7.0465 - val_accuracy: 0.1230

Epoch 00030: val_loss did not improve from 4.33114
Epoch 31/1000
31/31 - 12s - loss: 4.5367 - accuracy: 0.7787 - val_loss: 5.6467 - val_accuracy: 0.5444

Epoch 00031: val_loss did not improve from 4.33114

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 32/1000
31/31 - 12s - loss: 4.1560 - accuracy: 0.8251 - val_loss: 4.0492 - val_accuracy: 0.5665

Epoch 00032: val_loss improved from 4.33114 to 4.04920, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 33/1000
31/31 - 12s - loss: 3.5969 - accuracy: 0.8523 - val_loss: 3.8130 - val_accuracy: 0.5887

Epoch 00033: val_loss improved from 4.04920 to 3.81301, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 34/1000
31/31 - 12s - loss: 3.5006 - accuracy: 0.8432 - val_loss: 4.1304 - val_accuracy: 0.5484

Epoch 00034: val_loss did not improve from 3.81301
Epoch 35/1000
31/31 - 12s - loss: 3.2631 - accuracy: 0.8619 - val_loss: 3.7375 - val_accuracy: 0.5867

Epoch 00035: val_loss improved from 3.81301 to 3.73751, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 36/1000
31/31 - 12s - loss: 3.3188 - accuracy: 0.8533 - val_loss: 3.5224 - val_accuracy: 0.6411

Epoch 00036: val_loss improved from 3.73751 to 3.52244, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 37/1000
31/31 - 12s - loss: 3.2419 - accuracy: 0.8720 - val_loss: 3.3915 - val_accuracy: 0.6573

Epoch 00037: val_loss improved from 3.52244 to 3.39153, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 38/1000
31/31 - 12s - loss: 3.2250 - accuracy: 0.8594 - val_loss: 3.9861 - val_accuracy: 0.5827

Epoch 00038: val_loss did not improve from 3.39153
Epoch 39/1000
31/31 - 12s - loss: 3.1901 - accuracy: 0.8604 - val_loss: 3.4031 - val_accuracy: 0.6149

Epoch 00039: val_loss did not improve from 3.39153
Epoch 40/1000
31/31 - 12s - loss: 2.9850 - accuracy: 0.8745 - val_loss: 3.4794 - val_accuracy: 0.5968

Epoch 00040: val_loss did not improve from 3.39153
Epoch 41/1000
31/31 - 12s - loss: 3.0823 - accuracy: 0.8584 - val_loss: 3.2303 - val_accuracy: 0.6210

Epoch 00041: val_loss improved from 3.39153 to 3.23032, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 42/1000
31/31 - 12s - loss: 3.0289 - accuracy: 0.8629 - val_loss: 3.6721 - val_accuracy: 0.5383

Epoch 00042: val_loss did not improve from 3.23032
Epoch 43/1000
31/31 - 12s - loss: 2.8299 - accuracy: 0.8921 - val_loss: 3.2048 - val_accuracy: 0.6593

Epoch 00043: val_loss improved from 3.23032 to 3.20477, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 44/1000
31/31 - 12s - loss: 2.8830 - accuracy: 0.8780 - val_loss: 3.0554 - val_accuracy: 0.6794

Epoch 00044: val_loss improved from 3.20477 to 3.05537, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 45/1000
31/31 - 12s - loss: 2.9948 - accuracy: 0.8730 - val_loss: 4.2356 - val_accuracy: 0.3528

Epoch 00045: val_loss did not improve from 3.05537
Epoch 46/1000
31/31 - 12s - loss: 2.9965 - accuracy: 0.8604 - val_loss: 3.3806 - val_accuracy: 0.5867

Epoch 00046: val_loss did not improve from 3.05537
Epoch 47/1000
31/31 - 12s - loss: 2.7141 - accuracy: 0.9002 - val_loss: 3.0476 - val_accuracy: 0.6653

Epoch 00047: val_loss improved from 3.05537 to 3.04760, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 48/1000
31/31 - 15s - loss: 2.9145 - accuracy: 0.8705 - val_loss: 3.1727 - val_accuracy: 0.6593

Epoch 00048: val_loss did not improve from 3.04760
Epoch 49/1000
31/31 - 12s - loss: 2.7641 - accuracy: 0.8916 - val_loss: 3.9144 - val_accuracy: 0.4798

Epoch 00049: val_loss did not improve from 3.04760
Epoch 50/1000
31/31 - 12s - loss: 2.7997 - accuracy: 0.8800 - val_loss: 4.4374 - val_accuracy: 0.4133

Epoch 00050: val_loss did not improve from 3.04760
Epoch 51/1000
31/31 - 11s - loss: 2.6664 - accuracy: 0.8886 - val_loss: 3.8189 - val_accuracy: 0.4819

Epoch 00051: val_loss did not improve from 3.04760

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 52/1000
31/31 - 12s - loss: 2.4155 - accuracy: 0.9103 - val_loss: 3.4205 - val_accuracy: 0.5040

Epoch 00052: val_loss did not improve from 3.04760
Epoch 53/1000
31/31 - 12s - loss: 2.4070 - accuracy: 0.9007 - val_loss: 4.4487 - val_accuracy: 0.3024

Epoch 00053: val_loss did not improve from 3.04760
Epoch 54/1000
31/31 - 12s - loss: 2.2849 - accuracy: 0.9088 - val_loss: 4.2322 - val_accuracy: 0.3407

Epoch 00054: val_loss did not improve from 3.04760
Epoch 55/1000
31/31 - 12s - loss: 2.0304 - accuracy: 0.9435 - val_loss: 3.1898 - val_accuracy: 0.5484

Epoch 00055: val_loss did not improve from 3.04760

Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 56/1000
31/31 - 12s - loss: 1.9959 - accuracy: 0.9330 - val_loss: 2.5572 - val_accuracy: 0.7077

Epoch 00056: val_loss improved from 3.04760 to 2.55725, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 57/1000
31/31 - 12s - loss: 1.9288 - accuracy: 0.9385 - val_loss: 2.4549 - val_accuracy: 0.7419

Epoch 00057: val_loss improved from 2.55725 to 2.45494, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 58/1000
31/31 - 12s - loss: 1.8875 - accuracy: 0.9380 - val_loss: 2.1876 - val_accuracy: 0.7621

Epoch 00058: val_loss improved from 2.45494 to 2.18762, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 59/1000
31/31 - 12s - loss: 1.8042 - accuracy: 0.9496 - val_loss: 2.2749 - val_accuracy: 0.7500

Epoch 00059: val_loss did not improve from 2.18762
Epoch 60/1000
31/31 - 12s - loss: 1.8415 - accuracy: 0.9365 - val_loss: 2.1098 - val_accuracy: 0.7722

Epoch 00060: val_loss improved from 2.18762 to 2.10980, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 61/1000
31/31 - 12s - loss: 1.8402 - accuracy: 0.9375 - val_loss: 2.0474 - val_accuracy: 0.7923

Epoch 00061: val_loss improved from 2.10980 to 2.04738, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 62/1000
31/31 - 12s - loss: 1.9175 - accuracy: 0.9294 - val_loss: 2.1449 - val_accuracy: 0.7863

Epoch 00062: val_loss did not improve from 2.04738
Epoch 63/1000
31/31 - 11s - loss: 1.8019 - accuracy: 0.9466 - val_loss: 2.0927 - val_accuracy: 0.7863

Epoch 00063: val_loss did not improve from 2.04738
Epoch 64/1000
31/31 - 12s - loss: 1.7539 - accuracy: 0.9476 - val_loss: 2.1551 - val_accuracy: 0.7641

Epoch 00064: val_loss did not improve from 2.04738
Epoch 65/1000
31/31 - 12s - loss: 1.6793 - accuracy: 0.9567 - val_loss: 2.0447 - val_accuracy: 0.7802

Epoch 00065: val_loss improved from 2.04738 to 2.04474, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 66/1000
31/31 - 12s - loss: 1.7954 - accuracy: 0.9410 - val_loss: 2.2242 - val_accuracy: 0.7540

Epoch 00066: val_loss did not improve from 2.04474
Epoch 67/1000
31/31 - 13s - loss: 1.7518 - accuracy: 0.9395 - val_loss: 2.5818 - val_accuracy: 0.6694

Epoch 00067: val_loss did not improve from 2.04474
Epoch 68/1000
31/31 - 12s - loss: 1.7401 - accuracy: 0.9481 - val_loss: 2.2131 - val_accuracy: 0.7581

Epoch 00068: val_loss did not improve from 2.04474
Epoch 69/1000
31/31 - 12s - loss: 1.6825 - accuracy: 0.9531 - val_loss: 2.1675 - val_accuracy: 0.7540

Epoch 00069: val_loss did not improve from 2.04474

Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 70/1000
31/31 - 12s - loss: 1.5992 - accuracy: 0.9652 - val_loss: 2.1054 - val_accuracy: 0.7742

Epoch 00070: val_loss did not improve from 2.04474
Epoch 71/1000
31/31 - 12s - loss: 1.7244 - accuracy: 0.9395 - val_loss: 1.9700 - val_accuracy: 0.7923

Epoch 00071: val_loss improved from 2.04474 to 1.97001, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 72/1000
31/31 - 12s - loss: 1.5805 - accuracy: 0.9561 - val_loss: 1.9983 - val_accuracy: 0.7843

Epoch 00072: val_loss did not improve from 1.97001
Epoch 73/1000
31/31 - 12s - loss: 1.5916 - accuracy: 0.9536 - val_loss: 2.0410 - val_accuracy: 0.7702

Epoch 00073: val_loss did not improve from 1.97001
Epoch 74/1000
31/31 - 12s - loss: 1.6199 - accuracy: 0.9521 - val_loss: 1.9380 - val_accuracy: 0.7923

Epoch 00074: val_loss improved from 1.97001 to 1.93796, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 75/1000
31/31 - 12s - loss: 1.5221 - accuracy: 0.9733 - val_loss: 1.9447 - val_accuracy: 0.7863

Epoch 00075: val_loss did not improve from 1.93796
Epoch 76/1000
31/31 - 12s - loss: 1.5004 - accuracy: 0.9672 - val_loss: 1.9286 - val_accuracy: 0.7964

Epoch 00076: val_loss improved from 1.93796 to 1.92863, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 77/1000
31/31 - 12s - loss: 1.5006 - accuracy: 0.9693 - val_loss: 1.9257 - val_accuracy: 0.7883

Epoch 00077: val_loss improved from 1.92863 to 1.92574, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 78/1000
31/31 - 12s - loss: 1.4861 - accuracy: 0.9667 - val_loss: 1.9909 - val_accuracy: 0.7964

Epoch 00078: val_loss did not improve from 1.92574
Epoch 79/1000
31/31 - 12s - loss: 1.5036 - accuracy: 0.9592 - val_loss: 1.7967 - val_accuracy: 0.8427

Epoch 00079: val_loss improved from 1.92574 to 1.79669, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 80/1000
31/31 - 12s - loss: 1.6503 - accuracy: 0.9451 - val_loss: 2.1656 - val_accuracy: 0.7379

Epoch 00080: val_loss did not improve from 1.79669
Epoch 81/1000
31/31 - 12s - loss: 1.5273 - accuracy: 0.9627 - val_loss: 1.9930 - val_accuracy: 0.7903

Epoch 00081: val_loss did not improve from 1.79669
Epoch 82/1000
31/31 - 12s - loss: 1.5105 - accuracy: 0.9637 - val_loss: 1.8295 - val_accuracy: 0.8206

Epoch 00082: val_loss did not improve from 1.79669
Epoch 83/1000
31/31 - 12s - loss: 1.4742 - accuracy: 0.9703 - val_loss: 1.7843 - val_accuracy: 0.8367

Epoch 00083: val_loss improved from 1.79669 to 1.78430, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 84/1000
31/31 - 12s - loss: 1.4754 - accuracy: 0.9647 - val_loss: 1.8214 - val_accuracy: 0.8286

Epoch 00084: val_loss did not improve from 1.78430
Epoch 85/1000
31/31 - 12s - loss: 1.5194 - accuracy: 0.9612 - val_loss: 1.8935 - val_accuracy: 0.8206

Epoch 00085: val_loss did not improve from 1.78430
Epoch 86/1000
31/31 - 12s - loss: 1.4846 - accuracy: 0.9592 - val_loss: 1.8935 - val_accuracy: 0.8085

Epoch 00086: val_loss did not improve from 1.78430
Epoch 87/1000
31/31 - 12s - loss: 1.4200 - accuracy: 0.9698 - val_loss: 1.9072 - val_accuracy: 0.7984

Epoch 00087: val_loss did not improve from 1.78430

Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 88/1000
31/31 - 12s - loss: 1.4307 - accuracy: 0.9622 - val_loss: 1.8762 - val_accuracy: 0.8105

Epoch 00088: val_loss did not improve from 1.78430
Epoch 89/1000
31/31 - 12s - loss: 1.4669 - accuracy: 0.9627 - val_loss: 1.8229 - val_accuracy: 0.8387

Epoch 00089: val_loss did not improve from 1.78430
Epoch 90/1000
31/31 - 11s - loss: 1.4475 - accuracy: 0.9637 - val_loss: 1.7813 - val_accuracy: 0.8347

Epoch 00090: val_loss improved from 1.78430 to 1.78130, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 91/1000
31/31 - 12s - loss: 1.4164 - accuracy: 0.9733 - val_loss: 1.8093 - val_accuracy: 0.8327

Epoch 00091: val_loss did not improve from 1.78130
Epoch 92/1000
31/31 - 12s - loss: 1.3831 - accuracy: 0.9718 - val_loss: 1.7560 - val_accuracy: 0.8427

Epoch 00092: val_loss improved from 1.78130 to 1.75599, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 93/1000
31/31 - 12s - loss: 1.4130 - accuracy: 0.9627 - val_loss: 1.7519 - val_accuracy: 0.8286

Epoch 00093: val_loss improved from 1.75599 to 1.75193, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 94/1000
31/31 - 12s - loss: 1.3602 - accuracy: 0.9753 - val_loss: 1.7401 - val_accuracy: 0.8327

Epoch 00094: val_loss improved from 1.75193 to 1.74012, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 95/1000
31/31 - 12s - loss: 1.3621 - accuracy: 0.9743 - val_loss: 1.7490 - val_accuracy: 0.8266

Epoch 00095: val_loss did not improve from 1.74012
Epoch 96/1000
31/31 - 12s - loss: 1.3600 - accuracy: 0.9753 - val_loss: 1.7409 - val_accuracy: 0.8266

Epoch 00096: val_loss did not improve from 1.74012
Epoch 97/1000
31/31 - 12s - loss: 1.3180 - accuracy: 0.9824 - val_loss: 1.7970 - val_accuracy: 0.8145

Epoch 00097: val_loss did not improve from 1.74012
Epoch 98/1000
31/31 - 12s - loss: 1.3921 - accuracy: 0.9677 - val_loss: 1.7482 - val_accuracy: 0.8266

Epoch 00098: val_loss did not improve from 1.74012

Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 99/1000
31/31 - 12s - loss: 1.3371 - accuracy: 0.9763 - val_loss: 1.7351 - val_accuracy: 0.8286

Epoch 00099: val_loss improved from 1.74012 to 1.73510, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 100/1000
31/31 - 12s - loss: 1.3464 - accuracy: 0.9738 - val_loss: 1.7400 - val_accuracy: 0.8367

Epoch 00100: val_loss did not improve from 1.73510
Epoch 101/1000
31/31 - 12s - loss: 1.3162 - accuracy: 0.9738 - val_loss: 1.7402 - val_accuracy: 0.8327

Epoch 00101: val_loss did not improve from 1.73510
Epoch 102/1000
31/31 - 12s - loss: 1.2970 - accuracy: 0.9814 - val_loss: 1.7149 - val_accuracy: 0.8407

Epoch 00102: val_loss improved from 1.73510 to 1.71494, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 103/1000
31/31 - 12s - loss: 1.3208 - accuracy: 0.9733 - val_loss: 1.7231 - val_accuracy: 0.8448

Epoch 00103: val_loss did not improve from 1.71494
Epoch 104/1000
31/31 - 12s - loss: 1.2941 - accuracy: 0.9788 - val_loss: 1.7276 - val_accuracy: 0.8306

Epoch 00104: val_loss did not improve from 1.71494
Epoch 105/1000
31/31 - 12s - loss: 1.3142 - accuracy: 0.9763 - val_loss: 1.7055 - val_accuracy: 0.8387

Epoch 00105: val_loss improved from 1.71494 to 1.70548, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 106/1000
31/31 - 12s - loss: 1.2530 - accuracy: 0.9844 - val_loss: 1.7032 - val_accuracy: 0.8468

Epoch 00106: val_loss improved from 1.70548 to 1.70315, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 107/1000
31/31 - 12s - loss: 1.3300 - accuracy: 0.9688 - val_loss: 1.7191 - val_accuracy: 0.8367

Epoch 00107: val_loss did not improve from 1.70315
Epoch 108/1000
31/31 - 12s - loss: 1.2972 - accuracy: 0.9773 - val_loss: 1.6906 - val_accuracy: 0.8427

Epoch 00108: val_loss improved from 1.70315 to 1.69056, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 109/1000
31/31 - 12s - loss: 1.3370 - accuracy: 0.9743 - val_loss: 1.7186 - val_accuracy: 0.8468

Epoch 00109: val_loss did not improve from 1.69056
Epoch 110/1000
31/31 - 12s - loss: 1.3080 - accuracy: 0.9743 - val_loss: 1.6954 - val_accuracy: 0.8448

Epoch 00110: val_loss did not improve from 1.69056
Epoch 111/1000
31/31 - 12s - loss: 1.3187 - accuracy: 0.9743 - val_loss: 1.7137 - val_accuracy: 0.8387

Epoch 00111: val_loss did not improve from 1.69056
Epoch 112/1000
31/31 - 11s - loss: 1.2913 - accuracy: 0.9793 - val_loss: 1.7009 - val_accuracy: 0.8407

Epoch 00112: val_loss did not improve from 1.69056

Epoch 00112: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 113/1000
31/31 - 12s - loss: 1.3358 - accuracy: 0.9733 - val_loss: 1.7146 - val_accuracy: 0.8286

Epoch 00113: val_loss did not improve from 1.69056
Epoch 114/1000
31/31 - 12s - loss: 1.2279 - accuracy: 0.9884 - val_loss: 1.7044 - val_accuracy: 0.8407

Epoch 00114: val_loss did not improve from 1.69056
Epoch 115/1000
31/31 - 12s - loss: 1.3074 - accuracy: 0.9728 - val_loss: 1.7180 - val_accuracy: 0.8488

Epoch 00115: val_loss did not improve from 1.69056
Epoch 116/1000
31/31 - 12s - loss: 1.2350 - accuracy: 0.9819 - val_loss: 1.7114 - val_accuracy: 0.8387

Epoch 00116: val_loss did not improve from 1.69056

Epoch 00116: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 117/1000
31/31 - 12s - loss: 1.2724 - accuracy: 0.9819 - val_loss: 1.7019 - val_accuracy: 0.8407

Epoch 00117: val_loss did not improve from 1.69056
Epoch 118/1000
31/31 - 12s - loss: 1.2408 - accuracy: 0.9814 - val_loss: 1.6703 - val_accuracy: 0.8448

Epoch 00118: val_loss improved from 1.69056 to 1.67026, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 119/1000
31/31 - 12s - loss: 1.2515 - accuracy: 0.9814 - val_loss: 1.6534 - val_accuracy: 0.8468

Epoch 00119: val_loss improved from 1.67026 to 1.65344, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 120/1000
31/31 - 12s - loss: 1.2256 - accuracy: 0.9859 - val_loss: 1.6517 - val_accuracy: 0.8488

Epoch 00120: val_loss improved from 1.65344 to 1.65169, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 121/1000
31/31 - 12s - loss: 1.2364 - accuracy: 0.9834 - val_loss: 1.6819 - val_accuracy: 0.8488

Epoch 00121: val_loss did not improve from 1.65169
Epoch 122/1000
31/31 - 12s - loss: 1.2278 - accuracy: 0.9829 - val_loss: 1.6656 - val_accuracy: 0.8427

Epoch 00122: val_loss did not improve from 1.65169
Epoch 123/1000
31/31 - 12s - loss: 1.2968 - accuracy: 0.9788 - val_loss: 1.6903 - val_accuracy: 0.8347

Epoch 00123: val_loss did not improve from 1.65169
Epoch 124/1000
31/31 - 12s - loss: 1.2549 - accuracy: 0.9808 - val_loss: 1.6911 - val_accuracy: 0.8367

Epoch 00124: val_loss did not improve from 1.65169

Epoch 00124: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 125/1000
31/31 - 12s - loss: 1.2599 - accuracy: 0.9798 - val_loss: 1.6990 - val_accuracy: 0.8387

Epoch 00125: val_loss did not improve from 1.65169
Epoch 126/1000
31/31 - 12s - loss: 1.2613 - accuracy: 0.9793 - val_loss: 1.6843 - val_accuracy: 0.8407

Epoch 00126: val_loss did not improve from 1.65169
Epoch 127/1000
31/31 - 12s - loss: 1.2429 - accuracy: 0.9819 - val_loss: 1.7079 - val_accuracy: 0.8327

Epoch 00127: val_loss did not improve from 1.65169
Epoch 128/1000
31/31 - 12s - loss: 1.2376 - accuracy: 0.9844 - val_loss: 1.6955 - val_accuracy: 0.8407

Epoch 00128: val_loss did not improve from 1.65169

Epoch 00128: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
Epoch 129/1000
31/31 - 12s - loss: 1.2793 - accuracy: 0.9778 - val_loss: 1.6703 - val_accuracy: 0.8468

Epoch 00129: val_loss did not improve from 1.65169
Epoch 130/1000
31/31 - 12s - loss: 1.2313 - accuracy: 0.9864 - val_loss: 1.6998 - val_accuracy: 0.8427

Epoch 00130: val_loss did not improve from 1.65169
Epoch 131/1000
31/31 - 12s - loss: 1.2360 - accuracy: 0.9834 - val_loss: 1.6828 - val_accuracy: 0.8387

Epoch 00131: val_loss did not improve from 1.65169
Epoch 132/1000
31/31 - 12s - loss: 1.2563 - accuracy: 0.9793 - val_loss: 1.6957 - val_accuracy: 0.8347

Epoch 00132: val_loss did not improve from 1.65169

Epoch 00132: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
Epoch 133/1000
31/31 - 12s - loss: 1.2452 - accuracy: 0.9824 - val_loss: 1.7020 - val_accuracy: 0.8407

Epoch 00133: val_loss did not improve from 1.65169
Epoch 134/1000
31/31 - 12s - loss: 1.2538 - accuracy: 0.9798 - val_loss: 1.6788 - val_accuracy: 0.8448

Epoch 00134: val_loss did not improve from 1.65169
Epoch 135/1000
31/31 - 12s - loss: 1.2032 - accuracy: 0.9884 - val_loss: 1.6949 - val_accuracy: 0.8448

Epoch 00135: val_loss did not improve from 1.65169
Epoch 136/1000
31/31 - 12s - loss: 1.2017 - accuracy: 0.9854 - val_loss: 1.6748 - val_accuracy: 0.8448

Epoch 00136: val_loss did not improve from 1.65169

Epoch 00136: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
Epoch 137/1000
31/31 - 12s - loss: 1.2751 - accuracy: 0.9788 - val_loss: 1.6474 - val_accuracy: 0.8488

Epoch 00137: val_loss improved from 1.65169 to 1.64741, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_1.hdf5
Epoch 138/1000
31/31 - 12s - loss: 1.2005 - accuracy: 0.9849 - val_loss: 1.6739 - val_accuracy: 0.8448

Epoch 00138: val_loss did not improve from 1.64741
Epoch 139/1000
31/31 - 12s - loss: 1.2331 - accuracy: 0.9819 - val_loss: 1.6574 - val_accuracy: 0.8548

Epoch 00139: val_loss did not improve from 1.64741
Epoch 140/1000
31/31 - 12s - loss: 1.2392 - accuracy: 0.9798 - val_loss: 1.6598 - val_accuracy: 0.8488

Epoch 00140: val_loss did not improve from 1.64741
Epoch 141/1000
31/31 - 12s - loss: 1.2199 - accuracy: 0.9879 - val_loss: 1.6775 - val_accuracy: 0.8468

Epoch 00141: val_loss did not improve from 1.64741

Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.
Epoch 142/1000
31/31 - 12s - loss: 1.2635 - accuracy: 0.9808 - val_loss: 1.6937 - val_accuracy: 0.8387

Epoch 00142: val_loss did not improve from 1.64741
Epoch 143/1000
31/31 - 12s - loss: 1.2138 - accuracy: 0.9879 - val_loss: 1.6907 - val_accuracy: 0.8427

Epoch 00143: val_loss did not improve from 1.64741
Epoch 144/1000
31/31 - 12s - loss: 1.2615 - accuracy: 0.9783 - val_loss: 1.6824 - val_accuracy: 0.8427

Epoch 00144: val_loss did not improve from 1.64741
Epoch 145/1000
31/31 - 12s - loss: 1.2469 - accuracy: 0.9844 - val_loss: 1.6869 - val_accuracy: 0.8427

Epoch 00145: val_loss did not improve from 1.64741

Epoch 00145: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.
Epoch 146/1000
31/31 - 12s - loss: 1.2406 - accuracy: 0.9829 - val_loss: 1.6868 - val_accuracy: 0.8427

Epoch 00146: val_loss did not improve from 1.64741
Epoch 147/1000
31/31 - 12s - loss: 1.2546 - accuracy: 0.9819 - val_loss: 1.6872 - val_accuracy: 0.8387

Epoch 00147: val_loss did not improve from 1.64741
Epoch 148/1000
31/31 - 12s - loss: 1.2413 - accuracy: 0.9793 - val_loss: 1.6852 - val_accuracy: 0.8448

Epoch 00148: val_loss did not improve from 1.64741
Epoch 149/1000
31/31 - 12s - loss: 1.2585 - accuracy: 0.9834 - val_loss: 1.6938 - val_accuracy: 0.8407

Epoch 00149: val_loss did not improve from 1.64741

Epoch 00149: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.
Epoch 150/1000
31/31 - 12s - loss: 1.2428 - accuracy: 0.9814 - val_loss: 1.6706 - val_accuracy: 0.8448

Epoch 00150: val_loss did not improve from 1.64741
Epoch 151/1000
31/31 - 12s - loss: 1.2246 - accuracy: 0.9844 - val_loss: 1.6817 - val_accuracy: 0.8488

Epoch 00151: val_loss did not improve from 1.64741
Epoch 152/1000
31/31 - 12s - loss: 1.1851 - accuracy: 0.9884 - val_loss: 1.6689 - val_accuracy: 0.8407

Epoch 00152: val_loss did not improve from 1.64741
Epoch 153/1000
31/31 - 12s - loss: 1.2553 - accuracy: 0.9814 - val_loss: 1.6878 - val_accuracy: 0.8488

Epoch 00153: val_loss did not improve from 1.64741

Epoch 00153: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.
Epoch 154/1000
31/31 - 12s - loss: 1.2662 - accuracy: 0.9783 - val_loss: 1.6887 - val_accuracy: 0.8427

Epoch 00154: val_loss did not improve from 1.64741
Epoch 155/1000
31/31 - 12s - loss: 1.2658 - accuracy: 0.9814 - val_loss: 1.6941 - val_accuracy: 0.8387

Epoch 00155: val_loss did not improve from 1.64741
Epoch 156/1000
31/31 - 11s - loss: 1.2035 - accuracy: 0.9879 - val_loss: 1.6856 - val_accuracy: 0.8448

Epoch 00156: val_loss did not improve from 1.64741
Epoch 157/1000
31/31 - 12s - loss: 1.2392 - accuracy: 0.9839 - val_loss: 1.6868 - val_accuracy: 0.8427

Epoch 00157: val_loss did not improve from 1.64741

Epoch 00157: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.
Epoch 158/1000
31/31 - 12s - loss: 1.2431 - accuracy: 0.9798 - val_loss: 1.6852 - val_accuracy: 0.8488

Epoch 00158: val_loss did not improve from 1.64741
Epoch 159/1000
31/31 - 12s - loss: 1.2433 - accuracy: 0.9839 - val_loss: 1.6938 - val_accuracy: 0.8407

Epoch 00159: val_loss did not improve from 1.64741
Epoch 160/1000
31/31 - 12s - loss: 1.2296 - accuracy: 0.9864 - val_loss: 1.6783 - val_accuracy: 0.8448

Epoch 00160: val_loss did not improve from 1.64741
Epoch 161/1000
31/31 - 12s - loss: 1.2279 - accuracy: 0.9864 - val_loss: 1.6820 - val_accuracy: 0.8448

Epoch 00161: val_loss did not improve from 1.64741

Epoch 00161: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.
Epoch 162/1000
31/31 - 12s - loss: 1.2763 - accuracy: 0.9773 - val_loss: 1.6694 - val_accuracy: 0.8387

Epoch 00162: val_loss did not improve from 1.64741
Epoch 163/1000
31/31 - 12s - loss: 1.2435 - accuracy: 0.9829 - val_loss: 1.6979 - val_accuracy: 0.8427

Epoch 00163: val_loss did not improve from 1.64741
Epoch 164/1000
31/31 - 12s - loss: 1.2476 - accuracy: 0.9839 - val_loss: 1.6719 - val_accuracy: 0.8387

Epoch 00164: val_loss did not improve from 1.64741
Epoch 165/1000
31/31 - 12s - loss: 1.2382 - accuracy: 0.9849 - val_loss: 1.6834 - val_accuracy: 0.8508

Epoch 00165: val_loss did not improve from 1.64741

Epoch 00165: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.
Epoch 166/1000
31/31 - 12s - loss: 1.2480 - accuracy: 0.9793 - val_loss: 1.6910 - val_accuracy: 0.8427

Epoch 00166: val_loss did not improve from 1.64741
Epoch 167/1000
31/31 - 12s - loss: 1.2419 - accuracy: 0.9824 - val_loss: 1.6891 - val_accuracy: 0.8448

Epoch 00167: val_loss did not improve from 1.64741
Epoch 00167: early stopping
>> join - Process-2 at Sun Aug  1 07:58:41 2021
>> Process-2 spend 0h 39m 5.0s

>> start - Process-3 at Sun Aug  1 07:58:45 2021
Epoch 1/1000
31/31 - 67s - loss: 198.9441 - accuracy: 0.3115 - val_loss: 131.6973 - val_accuracy: 0.0161

Epoch 00001: val_loss improved from inf to 131.69734, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 2/1000
31/31 - 12s - loss: 97.9447 - accuracy: 0.5277 - val_loss: 67.6005 - val_accuracy: 0.2036

Epoch 00002: val_loss improved from 131.69734 to 67.60054, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 3/1000
31/31 - 12s - loss: 52.7378 - accuracy: 0.5615 - val_loss: 37.8896 - val_accuracy: 0.4960

Epoch 00003: val_loss improved from 67.60054 to 37.88956, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 4/1000
31/31 - 13s - loss: 31.0011 - accuracy: 0.5917 - val_loss: 23.1993 - val_accuracy: 0.4899

Epoch 00004: val_loss improved from 37.88956 to 23.19932, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 5/1000
31/31 - 12s - loss: 20.6186 - accuracy: 0.6114 - val_loss: 16.5316 - val_accuracy: 0.4879

Epoch 00005: val_loss improved from 23.19932 to 16.53163, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 6/1000
31/31 - 12s - loss: 15.2010 - accuracy: 0.6462 - val_loss: 13.0304 - val_accuracy: 0.4879

Epoch 00006: val_loss improved from 16.53163 to 13.03037, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 7/1000
31/31 - 12s - loss: 12.5503 - accuracy: 0.6663 - val_loss: 11.2063 - val_accuracy: 0.4839

Epoch 00007: val_loss improved from 13.03037 to 11.20627, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 8/1000
31/31 - 12s - loss: 11.0531 - accuracy: 0.6799 - val_loss: 10.4172 - val_accuracy: 0.4859

Epoch 00008: val_loss improved from 11.20627 to 10.41724, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 9/1000
31/31 - 12s - loss: 10.1566 - accuracy: 0.6779 - val_loss: 10.1079 - val_accuracy: 0.3327

Epoch 00009: val_loss improved from 10.41724 to 10.10790, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 10/1000
31/31 - 12s - loss: 9.3918 - accuracy: 0.7016 - val_loss: 9.5800 - val_accuracy: 0.4375

Epoch 00010: val_loss improved from 10.10790 to 9.57997, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 11/1000
31/31 - 12s - loss: 8.7446 - accuracy: 0.7051 - val_loss: 9.5414 - val_accuracy: 0.1089

Epoch 00011: val_loss improved from 9.57997 to 9.54138, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 12/1000
31/31 - 12s - loss: 8.4451 - accuracy: 0.7011 - val_loss: 8.2819 - val_accuracy: 0.4940

Epoch 00012: val_loss improved from 9.54138 to 8.28188, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 13/1000
31/31 - 12s - loss: 8.0537 - accuracy: 0.7056 - val_loss: 8.6741 - val_accuracy: 0.1956

Epoch 00013: val_loss did not improve from 8.28188
Epoch 14/1000
31/31 - 12s - loss: 7.3674 - accuracy: 0.7404 - val_loss: 7.2225 - val_accuracy: 0.5121

Epoch 00014: val_loss improved from 8.28188 to 7.22249, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 15/1000
31/31 - 12s - loss: 7.3683 - accuracy: 0.7077 - val_loss: 7.0260 - val_accuracy: 0.5060

Epoch 00015: val_loss improved from 7.22249 to 7.02597, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 16/1000
31/31 - 12s - loss: 6.8403 - accuracy: 0.7429 - val_loss: 7.4643 - val_accuracy: 0.4960

Epoch 00016: val_loss did not improve from 7.02597
Epoch 17/1000
31/31 - 12s - loss: 6.6387 - accuracy: 0.7344 - val_loss: 7.1304 - val_accuracy: 0.5181

Epoch 00017: val_loss did not improve from 7.02597
Epoch 18/1000
31/31 - 12s - loss: 6.4232 - accuracy: 0.7505 - val_loss: 6.4343 - val_accuracy: 0.5222

Epoch 00018: val_loss improved from 7.02597 to 6.43428, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 19/1000
31/31 - 12s - loss: 6.2160 - accuracy: 0.7581 - val_loss: 5.7397 - val_accuracy: 0.5706

Epoch 00019: val_loss improved from 6.43428 to 5.73966, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 20/1000
31/31 - 12s - loss: 6.0803 - accuracy: 0.7419 - val_loss: 5.1413 - val_accuracy: 0.5948

Epoch 00020: val_loss improved from 5.73966 to 5.14129, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 21/1000
31/31 - 12s - loss: 5.7310 - accuracy: 0.7676 - val_loss: 5.5507 - val_accuracy: 0.5746

Epoch 00021: val_loss did not improve from 5.14129
Epoch 22/1000
31/31 - 12s - loss: 5.4658 - accuracy: 0.7727 - val_loss: 4.9198 - val_accuracy: 0.6411

Epoch 00022: val_loss improved from 5.14129 to 4.91984, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 23/1000
31/31 - 12s - loss: 5.2535 - accuracy: 0.7838 - val_loss: 4.8468 - val_accuracy: 0.5907

Epoch 00023: val_loss improved from 4.91984 to 4.84680, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 24/1000
31/31 - 12s - loss: 5.4138 - accuracy: 0.7697 - val_loss: 5.5373 - val_accuracy: 0.5565

Epoch 00024: val_loss did not improve from 4.84680
Epoch 25/1000
31/31 - 12s - loss: 5.3128 - accuracy: 0.7772 - val_loss: 5.4610 - val_accuracy: 0.4597

Epoch 00025: val_loss did not improve from 4.84680
Epoch 26/1000
31/31 - 12s - loss: 4.9236 - accuracy: 0.7969 - val_loss: 4.6299 - val_accuracy: 0.5948

Epoch 00026: val_loss improved from 4.84680 to 4.62988, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 27/1000
31/31 - 12s - loss: 4.6952 - accuracy: 0.8155 - val_loss: 4.4559 - val_accuracy: 0.6109

Epoch 00027: val_loss improved from 4.62988 to 4.45594, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 28/1000
31/31 - 12s - loss: 4.4570 - accuracy: 0.8175 - val_loss: 5.0584 - val_accuracy: 0.4435

Epoch 00028: val_loss did not improve from 4.45594
Epoch 29/1000
31/31 - 12s - loss: 4.6526 - accuracy: 0.8034 - val_loss: 4.4773 - val_accuracy: 0.6331

Epoch 00029: val_loss did not improve from 4.45594
Epoch 30/1000
31/31 - 12s - loss: 4.6325 - accuracy: 0.7969 - val_loss: 5.6160 - val_accuracy: 0.4496

Epoch 00030: val_loss did not improve from 4.45594
Epoch 31/1000
31/31 - 12s - loss: 4.4837 - accuracy: 0.8085 - val_loss: 4.6634 - val_accuracy: 0.6109

Epoch 00031: val_loss did not improve from 4.45594

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 32/1000
31/31 - 12s - loss: 3.9387 - accuracy: 0.8412 - val_loss: 4.0997 - val_accuracy: 0.5847

Epoch 00032: val_loss improved from 4.45594 to 4.09972, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 33/1000
31/31 - 12s - loss: 3.3005 - accuracy: 0.8750 - val_loss: 3.9835 - val_accuracy: 0.5726

Epoch 00033: val_loss improved from 4.09972 to 3.98352, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 34/1000
31/31 - 12s - loss: 3.5004 - accuracy: 0.8458 - val_loss: 3.3383 - val_accuracy: 0.6673

Epoch 00034: val_loss improved from 3.98352 to 3.33834, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 35/1000
31/31 - 12s - loss: 3.4407 - accuracy: 0.8594 - val_loss: 3.1630 - val_accuracy: 0.6875

Epoch 00035: val_loss improved from 3.33834 to 3.16299, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 36/1000
31/31 - 12s - loss: 3.2906 - accuracy: 0.8614 - val_loss: 3.7467 - val_accuracy: 0.6290

Epoch 00036: val_loss did not improve from 3.16299
Epoch 37/1000
31/31 - 12s - loss: 2.9564 - accuracy: 0.8931 - val_loss: 3.5115 - val_accuracy: 0.6109

Epoch 00037: val_loss did not improve from 3.16299
Epoch 38/1000
31/31 - 12s - loss: 3.2469 - accuracy: 0.8508 - val_loss: 3.6824 - val_accuracy: 0.5927

Epoch 00038: val_loss did not improve from 3.16299
Epoch 39/1000
31/31 - 12s - loss: 3.1463 - accuracy: 0.8765 - val_loss: 3.9215 - val_accuracy: 0.5685

Epoch 00039: val_loss did not improve from 3.16299

Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 40/1000
31/31 - 12s - loss: 2.8655 - accuracy: 0.8911 - val_loss: 3.1694 - val_accuracy: 0.6673

Epoch 00040: val_loss did not improve from 3.16299
Epoch 41/1000
31/31 - 12s - loss: 2.6737 - accuracy: 0.9027 - val_loss: 3.0202 - val_accuracy: 0.6613

Epoch 00041: val_loss improved from 3.16299 to 3.02016, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 42/1000
31/31 - 12s - loss: 2.5452 - accuracy: 0.9073 - val_loss: 3.3758 - val_accuracy: 0.6230

Epoch 00042: val_loss did not improve from 3.02016
Epoch 43/1000
31/31 - 12s - loss: 2.6151 - accuracy: 0.8916 - val_loss: 3.1763 - val_accuracy: 0.5766

Epoch 00043: val_loss did not improve from 3.02016
Epoch 44/1000
31/31 - 12s - loss: 2.6166 - accuracy: 0.8992 - val_loss: 3.0391 - val_accuracy: 0.6956

Epoch 00044: val_loss did not improve from 3.02016
Epoch 45/1000
31/31 - 12s - loss: 2.6216 - accuracy: 0.9032 - val_loss: 2.9035 - val_accuracy: 0.7097

Epoch 00045: val_loss improved from 3.02016 to 2.90350, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 46/1000
31/31 - 12s - loss: 2.6044 - accuracy: 0.8921 - val_loss: 2.9136 - val_accuracy: 0.7339

Epoch 00046: val_loss did not improve from 2.90350
Epoch 47/1000
31/31 - 12s - loss: 2.5535 - accuracy: 0.9037 - val_loss: 3.0194 - val_accuracy: 0.7117

Epoch 00047: val_loss did not improve from 2.90350
Epoch 48/1000
31/31 - 12s - loss: 2.4400 - accuracy: 0.9108 - val_loss: 2.6825 - val_accuracy: 0.7298

Epoch 00048: val_loss improved from 2.90350 to 2.68252, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 49/1000
31/31 - 12s - loss: 2.4092 - accuracy: 0.9037 - val_loss: 2.6322 - val_accuracy: 0.7419

Epoch 00049: val_loss improved from 2.68252 to 2.63224, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 50/1000
31/31 - 12s - loss: 2.5763 - accuracy: 0.8982 - val_loss: 3.5183 - val_accuracy: 0.6532

Epoch 00050: val_loss did not improve from 2.63224
Epoch 51/1000
31/31 - 12s - loss: 2.4956 - accuracy: 0.9073 - val_loss: 2.5866 - val_accuracy: 0.7440

Epoch 00051: val_loss improved from 2.63224 to 2.58662, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 52/1000
31/31 - 12s - loss: 2.3685 - accuracy: 0.9214 - val_loss: 2.5441 - val_accuracy: 0.7742

Epoch 00052: val_loss improved from 2.58662 to 2.54411, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 53/1000
31/31 - 12s - loss: 2.2735 - accuracy: 0.9259 - val_loss: 2.4548 - val_accuracy: 0.7702

Epoch 00053: val_loss improved from 2.54411 to 2.45476, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 54/1000
31/31 - 12s - loss: 2.4284 - accuracy: 0.9007 - val_loss: 2.6991 - val_accuracy: 0.7278

Epoch 00054: val_loss did not improve from 2.45476
Epoch 55/1000
31/31 - 12s - loss: 2.2762 - accuracy: 0.9239 - val_loss: 2.5573 - val_accuracy: 0.7440

Epoch 00055: val_loss did not improve from 2.45476
Epoch 56/1000
31/31 - 12s - loss: 2.2347 - accuracy: 0.9199 - val_loss: 2.9968 - val_accuracy: 0.6794

Epoch 00056: val_loss did not improve from 2.45476
Epoch 57/1000
31/31 - 12s - loss: 2.4089 - accuracy: 0.8997 - val_loss: 2.9218 - val_accuracy: 0.7137

Epoch 00057: val_loss did not improve from 2.45476

Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 58/1000
31/31 - 12s - loss: 2.1056 - accuracy: 0.9430 - val_loss: 2.5351 - val_accuracy: 0.7762

Epoch 00058: val_loss did not improve from 2.45476
Epoch 59/1000
31/31 - 12s - loss: 2.0334 - accuracy: 0.9345 - val_loss: 2.2073 - val_accuracy: 0.8044

Epoch 00059: val_loss improved from 2.45476 to 2.20734, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 60/1000
31/31 - 12s - loss: 1.9701 - accuracy: 0.9461 - val_loss: 2.6609 - val_accuracy: 0.7560

Epoch 00060: val_loss did not improve from 2.20734
Epoch 61/1000
31/31 - 12s - loss: 1.9031 - accuracy: 0.9506 - val_loss: 2.3993 - val_accuracy: 0.7742

Epoch 00061: val_loss did not improve from 2.20734
Epoch 62/1000
31/31 - 12s - loss: 1.9850 - accuracy: 0.9355 - val_loss: 2.3467 - val_accuracy: 0.7843

Epoch 00062: val_loss did not improve from 2.20734
Epoch 63/1000
31/31 - 12s - loss: 1.9799 - accuracy: 0.9325 - val_loss: 2.4963 - val_accuracy: 0.7702

Epoch 00063: val_loss did not improve from 2.20734

Epoch 00063: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 64/1000
31/31 - 12s - loss: 1.8725 - accuracy: 0.9501 - val_loss: 2.3467 - val_accuracy: 0.7762

Epoch 00064: val_loss did not improve from 2.20734
Epoch 65/1000
31/31 - 12s - loss: 1.8773 - accuracy: 0.9451 - val_loss: 2.2187 - val_accuracy: 0.7883

Epoch 00065: val_loss did not improve from 2.20734
Epoch 66/1000
31/31 - 12s - loss: 1.8506 - accuracy: 0.9435 - val_loss: 2.1517 - val_accuracy: 0.7964

Epoch 00066: val_loss improved from 2.20734 to 2.15173, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 67/1000
31/31 - 12s - loss: 1.7692 - accuracy: 0.9582 - val_loss: 2.0863 - val_accuracy: 0.8044

Epoch 00067: val_loss improved from 2.15173 to 2.08628, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 68/1000
31/31 - 12s - loss: 1.7425 - accuracy: 0.9582 - val_loss: 2.0278 - val_accuracy: 0.8246

Epoch 00068: val_loss improved from 2.08628 to 2.02783, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 69/1000
31/31 - 12s - loss: 1.7474 - accuracy: 0.9567 - val_loss: 2.0993 - val_accuracy: 0.8165

Epoch 00069: val_loss did not improve from 2.02783
Epoch 70/1000
31/31 - 12s - loss: 1.7808 - accuracy: 0.9567 - val_loss: 2.0333 - val_accuracy: 0.8105

Epoch 00070: val_loss did not improve from 2.02783
Epoch 71/1000
31/31 - 12s - loss: 1.7076 - accuracy: 0.9561 - val_loss: 2.0459 - val_accuracy: 0.8125

Epoch 00071: val_loss did not improve from 2.02783
Epoch 72/1000
31/31 - 12s - loss: 1.7322 - accuracy: 0.9541 - val_loss: 2.1481 - val_accuracy: 0.8165

Epoch 00072: val_loss did not improve from 2.02783

Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 73/1000
31/31 - 12s - loss: 1.6875 - accuracy: 0.9602 - val_loss: 2.0633 - val_accuracy: 0.8125

Epoch 00073: val_loss did not improve from 2.02783
Epoch 74/1000
31/31 - 12s - loss: 1.5836 - accuracy: 0.9753 - val_loss: 1.9875 - val_accuracy: 0.8246

Epoch 00074: val_loss improved from 2.02783 to 1.98751, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 75/1000
31/31 - 12s - loss: 1.6934 - accuracy: 0.9607 - val_loss: 1.9998 - val_accuracy: 0.8145

Epoch 00075: val_loss did not improve from 1.98751
Epoch 76/1000
31/31 - 12s - loss: 1.5822 - accuracy: 0.9723 - val_loss: 1.9987 - val_accuracy: 0.8165

Epoch 00076: val_loss did not improve from 1.98751
Epoch 77/1000
31/31 - 12s - loss: 1.6243 - accuracy: 0.9698 - val_loss: 1.9840 - val_accuracy: 0.8226

Epoch 00077: val_loss improved from 1.98751 to 1.98398, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 78/1000
31/31 - 12s - loss: 1.6995 - accuracy: 0.9597 - val_loss: 2.0998 - val_accuracy: 0.8105

Epoch 00078: val_loss did not improve from 1.98398
Epoch 79/1000
31/31 - 12s - loss: 1.6417 - accuracy: 0.9642 - val_loss: 2.0741 - val_accuracy: 0.8145

Epoch 00079: val_loss did not improve from 1.98398
Epoch 80/1000
31/31 - 11s - loss: 1.6826 - accuracy: 0.9607 - val_loss: 2.0006 - val_accuracy: 0.8105

Epoch 00080: val_loss did not improve from 1.98398
Epoch 81/1000
31/31 - 12s - loss: 1.5721 - accuracy: 0.9698 - val_loss: 1.9340 - val_accuracy: 0.8206

Epoch 00081: val_loss improved from 1.98398 to 1.93396, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 82/1000
31/31 - 12s - loss: 1.6322 - accuracy: 0.9617 - val_loss: 1.9430 - val_accuracy: 0.8185

Epoch 00082: val_loss did not improve from 1.93396
Epoch 83/1000
31/31 - 12s - loss: 1.6547 - accuracy: 0.9567 - val_loss: 2.0365 - val_accuracy: 0.8125

Epoch 00083: val_loss did not improve from 1.93396
Epoch 84/1000
31/31 - 12s - loss: 1.5794 - accuracy: 0.9698 - val_loss: 1.9160 - val_accuracy: 0.8347

Epoch 00084: val_loss improved from 1.93396 to 1.91598, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 85/1000
31/31 - 12s - loss: 1.5547 - accuracy: 0.9728 - val_loss: 1.9082 - val_accuracy: 0.8427

Epoch 00085: val_loss improved from 1.91598 to 1.90823, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 86/1000
31/31 - 12s - loss: 1.5628 - accuracy: 0.9693 - val_loss: 1.8920 - val_accuracy: 0.8448

Epoch 00086: val_loss improved from 1.90823 to 1.89200, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 87/1000
31/31 - 12s - loss: 1.5139 - accuracy: 0.9788 - val_loss: 1.8935 - val_accuracy: 0.8407

Epoch 00087: val_loss did not improve from 1.89200
Epoch 88/1000
31/31 - 12s - loss: 1.5942 - accuracy: 0.9667 - val_loss: 1.8877 - val_accuracy: 0.8367

Epoch 00088: val_loss improved from 1.89200 to 1.88770, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 89/1000
31/31 - 12s - loss: 1.5616 - accuracy: 0.9693 - val_loss: 1.8800 - val_accuracy: 0.8468

Epoch 00089: val_loss improved from 1.88770 to 1.88002, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 90/1000
31/31 - 12s - loss: 1.5320 - accuracy: 0.9708 - val_loss: 1.8582 - val_accuracy: 0.8427

Epoch 00090: val_loss improved from 1.88002 to 1.85818, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 91/1000
31/31 - 12s - loss: 1.5295 - accuracy: 0.9728 - val_loss: 1.8737 - val_accuracy: 0.8448

Epoch 00091: val_loss did not improve from 1.85818
Epoch 92/1000
31/31 - 12s - loss: 1.5537 - accuracy: 0.9612 - val_loss: 1.9972 - val_accuracy: 0.8145

Epoch 00092: val_loss did not improve from 1.85818
Epoch 93/1000
31/31 - 12s - loss: 1.5842 - accuracy: 0.9592 - val_loss: 2.1150 - val_accuracy: 0.8024

Epoch 00093: val_loss did not improve from 1.85818
Epoch 94/1000
31/31 - 12s - loss: 1.6210 - accuracy: 0.9577 - val_loss: 2.1704 - val_accuracy: 0.8065

Epoch 00094: val_loss did not improve from 1.85818

Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 95/1000
31/31 - 12s - loss: 1.4637 - accuracy: 0.9803 - val_loss: 2.0702 - val_accuracy: 0.8105

Epoch 00095: val_loss did not improve from 1.85818
Epoch 96/1000
31/31 - 12s - loss: 1.5143 - accuracy: 0.9723 - val_loss: 1.9398 - val_accuracy: 0.8185

Epoch 00096: val_loss did not improve from 1.85818
Epoch 97/1000
31/31 - 12s - loss: 1.4571 - accuracy: 0.9829 - val_loss: 1.9316 - val_accuracy: 0.8206

Epoch 00097: val_loss did not improve from 1.85818
Epoch 98/1000
31/31 - 12s - loss: 1.4873 - accuracy: 0.9748 - val_loss: 1.9100 - val_accuracy: 0.8347

Epoch 00098: val_loss did not improve from 1.85818

Epoch 00098: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 99/1000
31/31 - 12s - loss: 1.4147 - accuracy: 0.9834 - val_loss: 1.8734 - val_accuracy: 0.8387

Epoch 00099: val_loss did not improve from 1.85818
Epoch 100/1000
31/31 - 12s - loss: 1.4974 - accuracy: 0.9733 - val_loss: 1.8602 - val_accuracy: 0.8306

Epoch 00100: val_loss did not improve from 1.85818
Epoch 101/1000
31/31 - 12s - loss: 1.4785 - accuracy: 0.9758 - val_loss: 1.8808 - val_accuracy: 0.8266

Epoch 00101: val_loss did not improve from 1.85818
Epoch 102/1000
31/31 - 12s - loss: 1.4258 - accuracy: 0.9829 - val_loss: 1.8755 - val_accuracy: 0.8306

Epoch 00102: val_loss did not improve from 1.85818

Epoch 00102: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 103/1000
31/31 - 12s - loss: 1.4410 - accuracy: 0.9788 - val_loss: 1.8481 - val_accuracy: 0.8468

Epoch 00103: val_loss improved from 1.85818 to 1.84806, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 104/1000
31/31 - 12s - loss: 1.4275 - accuracy: 0.9839 - val_loss: 1.8447 - val_accuracy: 0.8407

Epoch 00104: val_loss improved from 1.84806 to 1.84471, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 105/1000
31/31 - 12s - loss: 1.4666 - accuracy: 0.9748 - val_loss: 1.8427 - val_accuracy: 0.8427

Epoch 00105: val_loss improved from 1.84471 to 1.84272, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 106/1000
31/31 - 12s - loss: 1.4183 - accuracy: 0.9808 - val_loss: 1.8364 - val_accuracy: 0.8528

Epoch 00106: val_loss improved from 1.84272 to 1.83640, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 107/1000
31/31 - 12s - loss: 1.4046 - accuracy: 0.9844 - val_loss: 1.8401 - val_accuracy: 0.8448

Epoch 00107: val_loss did not improve from 1.83640
Epoch 108/1000
31/31 - 12s - loss: 1.3824 - accuracy: 0.9869 - val_loss: 1.8242 - val_accuracy: 0.8508

Epoch 00108: val_loss improved from 1.83640 to 1.82424, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 109/1000
31/31 - 12s - loss: 1.4589 - accuracy: 0.9748 - val_loss: 1.8240 - val_accuracy: 0.8468

Epoch 00109: val_loss improved from 1.82424 to 1.82400, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 110/1000
31/31 - 12s - loss: 1.4572 - accuracy: 0.9753 - val_loss: 1.8432 - val_accuracy: 0.8548

Epoch 00110: val_loss did not improve from 1.82400
Epoch 111/1000
31/31 - 12s - loss: 1.4522 - accuracy: 0.9788 - val_loss: 1.8437 - val_accuracy: 0.8528

Epoch 00111: val_loss did not improve from 1.82400
Epoch 112/1000
31/31 - 12s - loss: 1.4398 - accuracy: 0.9803 - val_loss: 1.8384 - val_accuracy: 0.8508

Epoch 00112: val_loss did not improve from 1.82400
Epoch 113/1000
31/31 - 12s - loss: 1.4565 - accuracy: 0.9733 - val_loss: 1.8527 - val_accuracy: 0.8468

Epoch 00113: val_loss did not improve from 1.82400

Epoch 00113: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 114/1000
31/31 - 12s - loss: 1.4309 - accuracy: 0.9819 - val_loss: 1.8449 - val_accuracy: 0.8528

Epoch 00114: val_loss did not improve from 1.82400
Epoch 115/1000
31/31 - 12s - loss: 1.4228 - accuracy: 0.9803 - val_loss: 1.8296 - val_accuracy: 0.8528

Epoch 00115: val_loss did not improve from 1.82400
Epoch 116/1000
31/31 - 12s - loss: 1.3927 - accuracy: 0.9854 - val_loss: 1.8354 - val_accuracy: 0.8528

Epoch 00116: val_loss did not improve from 1.82400
Epoch 117/1000
31/31 - 12s - loss: 1.4060 - accuracy: 0.9834 - val_loss: 1.8276 - val_accuracy: 0.8508

Epoch 00117: val_loss did not improve from 1.82400

Epoch 00117: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
Epoch 118/1000
31/31 - 12s - loss: 1.5159 - accuracy: 0.9693 - val_loss: 1.8446 - val_accuracy: 0.8548

Epoch 00118: val_loss did not improve from 1.82400
Epoch 119/1000
31/31 - 12s - loss: 1.4085 - accuracy: 0.9859 - val_loss: 1.8311 - val_accuracy: 0.8569

Epoch 00119: val_loss did not improve from 1.82400
Epoch 120/1000
31/31 - 12s - loss: 1.4287 - accuracy: 0.9824 - val_loss: 1.8396 - val_accuracy: 0.8508

Epoch 00120: val_loss did not improve from 1.82400
Epoch 121/1000
31/31 - 12s - loss: 1.3725 - accuracy: 0.9904 - val_loss: 1.8243 - val_accuracy: 0.8508

Epoch 00121: val_loss did not improve from 1.82400

Epoch 00121: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
Epoch 122/1000
31/31 - 12s - loss: 1.4412 - accuracy: 0.9814 - val_loss: 1.8330 - val_accuracy: 0.8508

Epoch 00122: val_loss did not improve from 1.82400
Epoch 123/1000
31/31 - 12s - loss: 1.3621 - accuracy: 0.9889 - val_loss: 1.8333 - val_accuracy: 0.8528

Epoch 00123: val_loss did not improve from 1.82400
Epoch 124/1000
31/31 - 11s - loss: 1.4348 - accuracy: 0.9763 - val_loss: 1.8372 - val_accuracy: 0.8528

Epoch 00124: val_loss did not improve from 1.82400
Epoch 125/1000
31/31 - 12s - loss: 1.4047 - accuracy: 0.9844 - val_loss: 1.8147 - val_accuracy: 0.8569

Epoch 00125: val_loss improved from 1.82400 to 1.81471, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_2.hdf5
Epoch 126/1000
31/31 - 12s - loss: 1.4310 - accuracy: 0.9834 - val_loss: 1.8268 - val_accuracy: 0.8548

Epoch 00126: val_loss did not improve from 1.81471
Epoch 127/1000
31/31 - 12s - loss: 1.4146 - accuracy: 0.9803 - val_loss: 1.8368 - val_accuracy: 0.8508

Epoch 00127: val_loss did not improve from 1.81471
Epoch 128/1000
31/31 - 12s - loss: 1.4279 - accuracy: 0.9798 - val_loss: 1.8316 - val_accuracy: 0.8569

Epoch 00128: val_loss did not improve from 1.81471
Epoch 129/1000
31/31 - 12s - loss: 1.3991 - accuracy: 0.9854 - val_loss: 1.8300 - val_accuracy: 0.8569

Epoch 00129: val_loss did not improve from 1.81471

Epoch 00129: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
Epoch 130/1000
31/31 - 12s - loss: 1.4071 - accuracy: 0.9834 - val_loss: 1.8341 - val_accuracy: 0.8508

Epoch 00130: val_loss did not improve from 1.81471
Epoch 131/1000
31/31 - 12s - loss: 1.4309 - accuracy: 0.9783 - val_loss: 1.8153 - val_accuracy: 0.8548

Epoch 00131: val_loss did not improve from 1.81471
Epoch 132/1000
31/31 - 12s - loss: 1.4384 - accuracy: 0.9803 - val_loss: 1.8325 - val_accuracy: 0.8528

Epoch 00132: val_loss did not improve from 1.81471
Epoch 133/1000
31/31 - 12s - loss: 1.4103 - accuracy: 0.9788 - val_loss: 1.8280 - val_accuracy: 0.8569

Epoch 00133: val_loss did not improve from 1.81471

Epoch 00133: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.
Epoch 134/1000
31/31 - 12s - loss: 1.3826 - accuracy: 0.9884 - val_loss: 1.8235 - val_accuracy: 0.8548

Epoch 00134: val_loss did not improve from 1.81471
Epoch 135/1000
31/31 - 12s - loss: 1.4343 - accuracy: 0.9778 - val_loss: 1.8157 - val_accuracy: 0.8548

Epoch 00135: val_loss did not improve from 1.81471
Epoch 136/1000
31/31 - 12s - loss: 1.3873 - accuracy: 0.9879 - val_loss: 1.8379 - val_accuracy: 0.8528

Epoch 00136: val_loss did not improve from 1.81471
Epoch 137/1000
31/31 - 12s - loss: 1.3815 - accuracy: 0.9874 - val_loss: 1.8346 - val_accuracy: 0.8548

Epoch 00137: val_loss did not improve from 1.81471

Epoch 00137: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.
Epoch 138/1000
31/31 - 12s - loss: 1.3877 - accuracy: 0.9803 - val_loss: 1.8294 - val_accuracy: 0.8548

Epoch 00138: val_loss did not improve from 1.81471
Epoch 139/1000
31/31 - 12s - loss: 1.4151 - accuracy: 0.9803 - val_loss: 1.8308 - val_accuracy: 0.8569

Epoch 00139: val_loss did not improve from 1.81471
Epoch 140/1000
31/31 - 12s - loss: 1.4014 - accuracy: 0.9844 - val_loss: 1.8285 - val_accuracy: 0.8548

Epoch 00140: val_loss did not improve from 1.81471
Epoch 141/1000
31/31 - 12s - loss: 1.3974 - accuracy: 0.9834 - val_loss: 1.8320 - val_accuracy: 0.8569

Epoch 00141: val_loss did not improve from 1.81471

Epoch 00141: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.
Epoch 142/1000
31/31 - 12s - loss: 1.3873 - accuracy: 0.9854 - val_loss: 1.8222 - val_accuracy: 0.8569

Epoch 00142: val_loss did not improve from 1.81471
Epoch 143/1000
31/31 - 12s - loss: 1.4470 - accuracy: 0.9798 - val_loss: 1.8334 - val_accuracy: 0.8548

Epoch 00143: val_loss did not improve from 1.81471
Epoch 144/1000
31/31 - 12s - loss: 1.4393 - accuracy: 0.9753 - val_loss: 1.8364 - val_accuracy: 0.8569

Epoch 00144: val_loss did not improve from 1.81471
Epoch 145/1000
31/31 - 12s - loss: 1.4241 - accuracy: 0.9834 - val_loss: 1.8470 - val_accuracy: 0.8508

Epoch 00145: val_loss did not improve from 1.81471

Epoch 00145: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.
Epoch 146/1000
31/31 - 12s - loss: 1.4053 - accuracy: 0.9819 - val_loss: 1.8425 - val_accuracy: 0.8528

Epoch 00146: val_loss did not improve from 1.81471
Epoch 147/1000
31/31 - 12s - loss: 1.4075 - accuracy: 0.9829 - val_loss: 1.8383 - val_accuracy: 0.8548

Epoch 00147: val_loss did not improve from 1.81471
Epoch 148/1000
31/31 - 12s - loss: 1.4215 - accuracy: 0.9808 - val_loss: 1.8309 - val_accuracy: 0.8569

Epoch 00148: val_loss did not improve from 1.81471
Epoch 149/1000
31/31 - 12s - loss: 1.4190 - accuracy: 0.9808 - val_loss: 1.8477 - val_accuracy: 0.8508

Epoch 00149: val_loss did not improve from 1.81471

Epoch 00149: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.
Epoch 150/1000
31/31 - 12s - loss: 1.4651 - accuracy: 0.9758 - val_loss: 1.8493 - val_accuracy: 0.8488

Epoch 00150: val_loss did not improve from 1.81471
Epoch 151/1000
31/31 - 12s - loss: 1.3782 - accuracy: 0.9879 - val_loss: 1.8272 - val_accuracy: 0.8548

Epoch 00151: val_loss did not improve from 1.81471
Epoch 152/1000
31/31 - 12s - loss: 1.4357 - accuracy: 0.9778 - val_loss: 1.8223 - val_accuracy: 0.8569

Epoch 00152: val_loss did not improve from 1.81471
Epoch 153/1000
31/31 - 12s - loss: 1.4217 - accuracy: 0.9798 - val_loss: 1.8389 - val_accuracy: 0.8548

Epoch 00153: val_loss did not improve from 1.81471

Epoch 00153: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.
Epoch 154/1000
31/31 - 12s - loss: 1.4188 - accuracy: 0.9819 - val_loss: 1.8379 - val_accuracy: 0.8528

Epoch 00154: val_loss did not improve from 1.81471
Epoch 155/1000
31/31 - 12s - loss: 1.3954 - accuracy: 0.9854 - val_loss: 1.8327 - val_accuracy: 0.8569

Epoch 00155: val_loss did not improve from 1.81471
Epoch 00155: early stopping
>> join - Process-3 at Sun Aug  1 08:36:02 2021
>> Process-3 spend 0h 37m 16.5s

>> start - Process-4 at Sun Aug  1 08:36:06 2021
Epoch 1/1000
31/31 - 67s - loss: 199.2214 - accuracy: 0.3034 - val_loss: 131.8263 - val_accuracy: 0.0312

Epoch 00001: val_loss improved from inf to 131.82626, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 2/1000
31/31 - 12s - loss: 98.6342 - accuracy: 0.5121 - val_loss: 67.4514 - val_accuracy: 0.4979

Epoch 00002: val_loss improved from 131.82626 to 67.45139, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 3/1000
31/31 - 12s - loss: 52.7408 - accuracy: 0.5721 - val_loss: 37.8056 - val_accuracy: 0.5063

Epoch 00003: val_loss improved from 67.45139 to 37.80560, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 4/1000
31/31 - 12s - loss: 31.0396 - accuracy: 0.5917 - val_loss: 24.3579 - val_accuracy: 0.0042

Epoch 00004: val_loss improved from 37.80560 to 24.35793, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 5/1000
31/31 - 12s - loss: 20.6681 - accuracy: 0.6109 - val_loss: 16.6557 - val_accuracy: 0.4979

Epoch 00005: val_loss improved from 24.35793 to 16.65568, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 6/1000
31/31 - 11s - loss: 15.4250 - accuracy: 0.6351 - val_loss: 13.0725 - val_accuracy: 0.4958

Epoch 00006: val_loss improved from 16.65568 to 13.07253, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 7/1000
31/31 - 12s - loss: 12.6302 - accuracy: 0.6502 - val_loss: 11.3711 - val_accuracy: 0.5000

Epoch 00007: val_loss improved from 13.07253 to 11.37107, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 8/1000
31/31 - 12s - loss: 11.1263 - accuracy: 0.6714 - val_loss: 10.3564 - val_accuracy: 0.4979

Epoch 00008: val_loss improved from 11.37107 to 10.35637, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 9/1000
31/31 - 12s - loss: 10.0226 - accuracy: 0.6825 - val_loss: 10.5109 - val_accuracy: 0.5000

Epoch 00009: val_loss did not improve from 10.35637
Epoch 10/1000
31/31 - 12s - loss: 9.4955 - accuracy: 0.6724 - val_loss: 9.4207 - val_accuracy: 0.4958

Epoch 00010: val_loss improved from 10.35637 to 9.42069, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 11/1000
31/31 - 12s - loss: 8.7573 - accuracy: 0.6809 - val_loss: 8.8382 - val_accuracy: 0.4979

Epoch 00011: val_loss improved from 9.42069 to 8.83816, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 12/1000
31/31 - 12s - loss: 8.2711 - accuracy: 0.7087 - val_loss: 8.1324 - val_accuracy: 0.5000

Epoch 00012: val_loss improved from 8.83816 to 8.13240, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 13/1000
31/31 - 11s - loss: 7.6861 - accuracy: 0.7132 - val_loss: 7.5619 - val_accuracy: 0.5104

Epoch 00013: val_loss improved from 8.13240 to 7.56185, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 14/1000
31/31 - 12s - loss: 7.4805 - accuracy: 0.7223 - val_loss: 7.0188 - val_accuracy: 0.5479

Epoch 00014: val_loss improved from 7.56185 to 7.01876, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 15/1000
31/31 - 12s - loss: 7.3097 - accuracy: 0.7122 - val_loss: 6.9032 - val_accuracy: 0.5333

Epoch 00015: val_loss improved from 7.01876 to 6.90318, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 16/1000
31/31 - 11s - loss: 6.8896 - accuracy: 0.7379 - val_loss: 7.4833 - val_accuracy: 0.3958

Epoch 00016: val_loss did not improve from 6.90318
Epoch 17/1000
31/31 - 11s - loss: 6.7329 - accuracy: 0.7389 - val_loss: 10.1437 - val_accuracy: 0.0896

Epoch 00017: val_loss did not improve from 6.90318
Epoch 18/1000
31/31 - 11s - loss: 6.2853 - accuracy: 0.7571 - val_loss: 7.5532 - val_accuracy: 0.1312

Epoch 00018: val_loss did not improve from 6.90318
Epoch 19/1000
31/31 - 11s - loss: 6.2452 - accuracy: 0.7278 - val_loss: 7.3442 - val_accuracy: 0.1437

Epoch 00019: val_loss did not improve from 6.90318

Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 20/1000
31/31 - 11s - loss: 5.6942 - accuracy: 0.7969 - val_loss: 6.8154 - val_accuracy: 0.1917

Epoch 00020: val_loss improved from 6.90318 to 6.81538, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 21/1000
31/31 - 11s - loss: 5.0129 - accuracy: 0.8070 - val_loss: 5.5133 - val_accuracy: 0.4437

Epoch 00021: val_loss improved from 6.81538 to 5.51332, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 22/1000
31/31 - 11s - loss: 4.7879 - accuracy: 0.8095 - val_loss: 6.2044 - val_accuracy: 0.2458

Epoch 00022: val_loss did not improve from 5.51332
Epoch 23/1000
31/31 - 11s - loss: 4.6952 - accuracy: 0.8155 - val_loss: 6.9013 - val_accuracy: 0.1312

Epoch 00023: val_loss did not improve from 5.51332
Epoch 24/1000
31/31 - 11s - loss: 4.8534 - accuracy: 0.7959 - val_loss: 6.4565 - val_accuracy: 0.2062

Epoch 00024: val_loss did not improve from 5.51332
Epoch 25/1000
31/31 - 11s - loss: 4.5954 - accuracy: 0.8241 - val_loss: 4.0583 - val_accuracy: 0.6896

Epoch 00025: val_loss improved from 5.51332 to 4.05827, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 26/1000
31/31 - 11s - loss: 4.4835 - accuracy: 0.8256 - val_loss: 4.2675 - val_accuracy: 0.6208

Epoch 00026: val_loss did not improve from 4.05827
Epoch 27/1000
31/31 - 11s - loss: 4.3591 - accuracy: 0.8256 - val_loss: 4.8331 - val_accuracy: 0.5625

Epoch 00027: val_loss did not improve from 4.05827
Epoch 28/1000
31/31 - 10s - loss: 4.4973 - accuracy: 0.8070 - val_loss: 4.7474 - val_accuracy: 0.5146

Epoch 00028: val_loss did not improve from 4.05827
Epoch 29/1000
31/31 - 11s - loss: 4.1415 - accuracy: 0.8362 - val_loss: 4.2793 - val_accuracy: 0.6167

Epoch 00029: val_loss did not improve from 4.05827

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 30/1000
31/31 - 11s - loss: 3.7697 - accuracy: 0.8755 - val_loss: 4.0839 - val_accuracy: 0.6271

Epoch 00030: val_loss did not improve from 4.05827
Epoch 31/1000
31/31 - 11s - loss: 3.6133 - accuracy: 0.8760 - val_loss: 3.6955 - val_accuracy: 0.6875

Epoch 00031: val_loss improved from 4.05827 to 3.69550, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 32/1000
31/31 - 11s - loss: 3.6564 - accuracy: 0.8584 - val_loss: 3.2862 - val_accuracy: 0.7396

Epoch 00032: val_loss improved from 3.69550 to 3.28619, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 33/1000
31/31 - 11s - loss: 3.2937 - accuracy: 0.8916 - val_loss: 3.3783 - val_accuracy: 0.6979

Epoch 00033: val_loss did not improve from 3.28619
Epoch 34/1000
31/31 - 11s - loss: 3.4247 - accuracy: 0.8705 - val_loss: 3.3378 - val_accuracy: 0.7146

Epoch 00034: val_loss did not improve from 3.28619
Epoch 35/1000
31/31 - 11s - loss: 3.4840 - accuracy: 0.8604 - val_loss: 3.3078 - val_accuracy: 0.7063

Epoch 00035: val_loss did not improve from 3.28619
Epoch 36/1000
31/31 - 11s - loss: 3.4480 - accuracy: 0.8690 - val_loss: 3.3438 - val_accuracy: 0.7250

Epoch 00036: val_loss did not improve from 3.28619

Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 37/1000
31/31 - 11s - loss: 3.2632 - accuracy: 0.8836 - val_loss: 3.3598 - val_accuracy: 0.7354

Epoch 00037: val_loss did not improve from 3.28619
Epoch 38/1000
31/31 - 11s - loss: 3.0561 - accuracy: 0.9078 - val_loss: 3.0710 - val_accuracy: 0.7750

Epoch 00038: val_loss improved from 3.28619 to 3.07104, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 39/1000
31/31 - 11s - loss: 3.0445 - accuracy: 0.9027 - val_loss: 3.0835 - val_accuracy: 0.7396

Epoch 00039: val_loss did not improve from 3.07104
Epoch 40/1000
31/31 - 11s - loss: 2.8990 - accuracy: 0.9163 - val_loss: 3.0092 - val_accuracy: 0.7521

Epoch 00040: val_loss improved from 3.07104 to 3.00919, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 41/1000
31/31 - 11s - loss: 2.9101 - accuracy: 0.9037 - val_loss: 2.9117 - val_accuracy: 0.7917

Epoch 00041: val_loss improved from 3.00919 to 2.91171, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 42/1000
31/31 - 11s - loss: 2.8519 - accuracy: 0.9103 - val_loss: 2.9798 - val_accuracy: 0.7792

Epoch 00042: val_loss did not improve from 2.91171
Epoch 43/1000
31/31 - 11s - loss: 2.8545 - accuracy: 0.9118 - val_loss: 2.8756 - val_accuracy: 0.7500

Epoch 00043: val_loss improved from 2.91171 to 2.87556, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 44/1000
31/31 - 11s - loss: 2.8191 - accuracy: 0.9158 - val_loss: 2.8406 - val_accuracy: 0.7833

Epoch 00044: val_loss improved from 2.87556 to 2.84055, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 45/1000
31/31 - 11s - loss: 2.9663 - accuracy: 0.8997 - val_loss: 2.8480 - val_accuracy: 0.7604

Epoch 00045: val_loss did not improve from 2.84055
Epoch 46/1000
31/31 - 11s - loss: 2.9662 - accuracy: 0.8957 - val_loss: 2.9263 - val_accuracy: 0.7437

Epoch 00046: val_loss did not improve from 2.84055
Epoch 47/1000
31/31 - 11s - loss: 2.7420 - accuracy: 0.9173 - val_loss: 2.8567 - val_accuracy: 0.7688

Epoch 00047: val_loss did not improve from 2.84055
Epoch 48/1000
31/31 - 11s - loss: 2.8055 - accuracy: 0.9143 - val_loss: 2.8918 - val_accuracy: 0.7688

Epoch 00048: val_loss did not improve from 2.84055

Epoch 00048: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 49/1000
31/31 - 11s - loss: 2.6313 - accuracy: 0.9345 - val_loss: 2.7360 - val_accuracy: 0.7979

Epoch 00049: val_loss improved from 2.84055 to 2.73598, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 50/1000
31/31 - 11s - loss: 2.4878 - accuracy: 0.9506 - val_loss: 2.7224 - val_accuracy: 0.7937

Epoch 00050: val_loss improved from 2.73598 to 2.72236, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 51/1000
31/31 - 11s - loss: 2.4828 - accuracy: 0.9435 - val_loss: 2.6757 - val_accuracy: 0.8083

Epoch 00051: val_loss improved from 2.72236 to 2.67573, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 52/1000
31/31 - 11s - loss: 2.3864 - accuracy: 0.9551 - val_loss: 2.6037 - val_accuracy: 0.8313

Epoch 00052: val_loss improved from 2.67573 to 2.60373, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 53/1000
31/31 - 11s - loss: 2.5225 - accuracy: 0.9335 - val_loss: 2.5996 - val_accuracy: 0.8229

Epoch 00053: val_loss improved from 2.60373 to 2.59959, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 54/1000
31/31 - 11s - loss: 2.4923 - accuracy: 0.9385 - val_loss: 2.5216 - val_accuracy: 0.8292

Epoch 00054: val_loss improved from 2.59959 to 2.52159, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 55/1000
31/31 - 11s - loss: 2.3708 - accuracy: 0.9501 - val_loss: 2.5231 - val_accuracy: 0.8146

Epoch 00055: val_loss did not improve from 2.52159
Epoch 56/1000
31/31 - 11s - loss: 2.4305 - accuracy: 0.9430 - val_loss: 2.4351 - val_accuracy: 0.8271

Epoch 00056: val_loss improved from 2.52159 to 2.43512, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 57/1000
31/31 - 11s - loss: 2.4490 - accuracy: 0.9365 - val_loss: 2.4496 - val_accuracy: 0.8396

Epoch 00057: val_loss did not improve from 2.43512
Epoch 58/1000
31/31 - 11s - loss: 2.4645 - accuracy: 0.9325 - val_loss: 2.3984 - val_accuracy: 0.8458

Epoch 00058: val_loss improved from 2.43512 to 2.39840, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 59/1000
31/31 - 11s - loss: 2.4255 - accuracy: 0.9325 - val_loss: 2.4558 - val_accuracy: 0.8396

Epoch 00059: val_loss did not improve from 2.39840
Epoch 60/1000
31/31 - 11s - loss: 2.4450 - accuracy: 0.9309 - val_loss: 2.5094 - val_accuracy: 0.8083

Epoch 00060: val_loss did not improve from 2.39840
Epoch 61/1000
31/31 - 11s - loss: 2.2576 - accuracy: 0.9592 - val_loss: 2.4149 - val_accuracy: 0.8396

Epoch 00061: val_loss did not improve from 2.39840
Epoch 62/1000
31/31 - 11s - loss: 2.3185 - accuracy: 0.9491 - val_loss: 2.3825 - val_accuracy: 0.8542

Epoch 00062: val_loss improved from 2.39840 to 2.38246, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 63/1000
31/31 - 11s - loss: 2.2852 - accuracy: 0.9511 - val_loss: 2.3856 - val_accuracy: 0.8458

Epoch 00063: val_loss did not improve from 2.38246
Epoch 64/1000
31/31 - 11s - loss: 2.3985 - accuracy: 0.9405 - val_loss: 2.3740 - val_accuracy: 0.8479

Epoch 00064: val_loss improved from 2.38246 to 2.37403, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 65/1000
31/31 - 11s - loss: 2.2796 - accuracy: 0.9486 - val_loss: 2.5168 - val_accuracy: 0.8292

Epoch 00065: val_loss did not improve from 2.37403
Epoch 66/1000
31/31 - 11s - loss: 2.3386 - accuracy: 0.9430 - val_loss: 2.4521 - val_accuracy: 0.8354

Epoch 00066: val_loss did not improve from 2.37403
Epoch 67/1000
31/31 - 11s - loss: 2.2248 - accuracy: 0.9607 - val_loss: 2.4947 - val_accuracy: 0.8333

Epoch 00067: val_loss did not improve from 2.37403
Epoch 68/1000
31/31 - 11s - loss: 2.2054 - accuracy: 0.9577 - val_loss: 2.4115 - val_accuracy: 0.8375

Epoch 00068: val_loss did not improve from 2.37403

Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 69/1000
31/31 - 10s - loss: 2.1326 - accuracy: 0.9662 - val_loss: 2.3476 - val_accuracy: 0.8521

Epoch 00069: val_loss improved from 2.37403 to 2.34756, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 70/1000
31/31 - 11s - loss: 2.1550 - accuracy: 0.9612 - val_loss: 2.3557 - val_accuracy: 0.8500

Epoch 00070: val_loss did not improve from 2.34756
Epoch 71/1000
31/31 - 11s - loss: 2.1911 - accuracy: 0.9551 - val_loss: 2.3393 - val_accuracy: 0.8521

Epoch 00071: val_loss improved from 2.34756 to 2.33934, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 72/1000
31/31 - 11s - loss: 2.1107 - accuracy: 0.9682 - val_loss: 2.3146 - val_accuracy: 0.8479

Epoch 00072: val_loss improved from 2.33934 to 2.31459, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 73/1000
31/31 - 11s - loss: 2.0866 - accuracy: 0.9713 - val_loss: 2.2755 - val_accuracy: 0.8542

Epoch 00073: val_loss improved from 2.31459 to 2.27552, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 74/1000
31/31 - 11s - loss: 2.0397 - accuracy: 0.9718 - val_loss: 2.2655 - val_accuracy: 0.8583

Epoch 00074: val_loss improved from 2.27552 to 2.26546, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 75/1000
31/31 - 11s - loss: 2.1075 - accuracy: 0.9632 - val_loss: 2.2722 - val_accuracy: 0.8458

Epoch 00075: val_loss did not improve from 2.26546
Epoch 76/1000
31/31 - 11s - loss: 2.1152 - accuracy: 0.9622 - val_loss: 2.2744 - val_accuracy: 0.8604

Epoch 00076: val_loss did not improve from 2.26546
Epoch 77/1000
31/31 - 11s - loss: 2.0632 - accuracy: 0.9698 - val_loss: 2.2511 - val_accuracy: 0.8625

Epoch 00077: val_loss improved from 2.26546 to 2.25109, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 78/1000
31/31 - 11s - loss: 2.0527 - accuracy: 0.9672 - val_loss: 2.2494 - val_accuracy: 0.8646

Epoch 00078: val_loss improved from 2.25109 to 2.24941, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 79/1000
31/31 - 11s - loss: 2.0882 - accuracy: 0.9622 - val_loss: 2.2810 - val_accuracy: 0.8438

Epoch 00079: val_loss did not improve from 2.24941
Epoch 80/1000
31/31 - 11s - loss: 2.0943 - accuracy: 0.9597 - val_loss: 2.2551 - val_accuracy: 0.8562

Epoch 00080: val_loss did not improve from 2.24941
Epoch 81/1000
31/31 - 11s - loss: 2.0396 - accuracy: 0.9703 - val_loss: 2.2630 - val_accuracy: 0.8458

Epoch 00081: val_loss did not improve from 2.24941
Epoch 82/1000
31/31 - 11s - loss: 2.0214 - accuracy: 0.9672 - val_loss: 2.2423 - val_accuracy: 0.8562

Epoch 00082: val_loss improved from 2.24941 to 2.24229, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 83/1000
31/31 - 11s - loss: 2.0320 - accuracy: 0.9672 - val_loss: 2.2443 - val_accuracy: 0.8458

Epoch 00083: val_loss did not improve from 2.24229
Epoch 84/1000
31/31 - 11s - loss: 2.0400 - accuracy: 0.9693 - val_loss: 2.3393 - val_accuracy: 0.8354

Epoch 00084: val_loss did not improve from 2.24229
Epoch 85/1000
31/31 - 11s - loss: 2.0388 - accuracy: 0.9693 - val_loss: 2.2582 - val_accuracy: 0.8646

Epoch 00085: val_loss did not improve from 2.24229
Epoch 86/1000
31/31 - 11s - loss: 2.0311 - accuracy: 0.9718 - val_loss: 2.2864 - val_accuracy: 0.8438

Epoch 00086: val_loss did not improve from 2.24229

Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 87/1000
31/31 - 11s - loss: 1.9908 - accuracy: 0.9682 - val_loss: 2.2560 - val_accuracy: 0.8604

Epoch 00087: val_loss did not improve from 2.24229
Epoch 88/1000
31/31 - 11s - loss: 1.9517 - accuracy: 0.9788 - val_loss: 2.2227 - val_accuracy: 0.8521

Epoch 00088: val_loss improved from 2.24229 to 2.22266, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 89/1000
31/31 - 11s - loss: 2.0019 - accuracy: 0.9693 - val_loss: 2.2169 - val_accuracy: 0.8646

Epoch 00089: val_loss improved from 2.22266 to 2.21695, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 90/1000
31/31 - 11s - loss: 1.9595 - accuracy: 0.9763 - val_loss: 2.2607 - val_accuracy: 0.8583

Epoch 00090: val_loss did not improve from 2.21695
Epoch 91/1000
31/31 - 11s - loss: 2.0041 - accuracy: 0.9713 - val_loss: 2.2305 - val_accuracy: 0.8625

Epoch 00091: val_loss did not improve from 2.21695
Epoch 92/1000
31/31 - 11s - loss: 2.0544 - accuracy: 0.9652 - val_loss: 2.2337 - val_accuracy: 0.8458

Epoch 00092: val_loss did not improve from 2.21695
Epoch 93/1000
31/31 - 11s - loss: 1.9630 - accuracy: 0.9698 - val_loss: 2.2408 - val_accuracy: 0.8521

Epoch 00093: val_loss did not improve from 2.21695

Epoch 00093: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 94/1000
31/31 - 11s - loss: 1.9916 - accuracy: 0.9698 - val_loss: 2.2320 - val_accuracy: 0.8542

Epoch 00094: val_loss did not improve from 2.21695
Epoch 95/1000
31/31 - 11s - loss: 1.9659 - accuracy: 0.9753 - val_loss: 2.2254 - val_accuracy: 0.8604

Epoch 00095: val_loss did not improve from 2.21695
Epoch 96/1000
31/31 - 11s - loss: 1.9453 - accuracy: 0.9763 - val_loss: 2.2426 - val_accuracy: 0.8562

Epoch 00096: val_loss did not improve from 2.21695
Epoch 97/1000
31/31 - 11s - loss: 1.8699 - accuracy: 0.9844 - val_loss: 2.2320 - val_accuracy: 0.8687

Epoch 00097: val_loss did not improve from 2.21695

Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 98/1000
31/31 - 11s - loss: 2.0044 - accuracy: 0.9693 - val_loss: 2.2091 - val_accuracy: 0.8667

Epoch 00098: val_loss improved from 2.21695 to 2.20909, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 99/1000
31/31 - 11s - loss: 1.9707 - accuracy: 0.9763 - val_loss: 2.2368 - val_accuracy: 0.8583

Epoch 00099: val_loss did not improve from 2.20909
Epoch 100/1000
31/31 - 11s - loss: 1.8824 - accuracy: 0.9798 - val_loss: 2.2219 - val_accuracy: 0.8667

Epoch 00100: val_loss did not improve from 2.20909
Epoch 101/1000
31/31 - 11s - loss: 1.8834 - accuracy: 0.9829 - val_loss: 2.2211 - val_accuracy: 0.8562

Epoch 00101: val_loss did not improve from 2.20909
Epoch 102/1000
31/31 - 11s - loss: 1.9158 - accuracy: 0.9743 - val_loss: 2.2207 - val_accuracy: 0.8583

Epoch 00102: val_loss did not improve from 2.20909

Epoch 00102: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 103/1000
31/31 - 11s - loss: 1.8721 - accuracy: 0.9859 - val_loss: 2.1870 - val_accuracy: 0.8646

Epoch 00103: val_loss improved from 2.20909 to 2.18701, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_3.hdf5
Epoch 104/1000
31/31 - 11s - loss: 1.9201 - accuracy: 0.9803 - val_loss: 2.2142 - val_accuracy: 0.8604

Epoch 00104: val_loss did not improve from 2.18701
Epoch 105/1000
31/31 - 11s - loss: 1.9558 - accuracy: 0.9738 - val_loss: 2.2183 - val_accuracy: 0.8667

Epoch 00105: val_loss did not improve from 2.18701
Epoch 106/1000
31/31 - 11s - loss: 1.9044 - accuracy: 0.9808 - val_loss: 2.2264 - val_accuracy: 0.8604

Epoch 00106: val_loss did not improve from 2.18701
Epoch 107/1000
31/31 - 11s - loss: 1.9418 - accuracy: 0.9733 - val_loss: 2.2305 - val_accuracy: 0.8583

Epoch 00107: val_loss did not improve from 2.18701

Epoch 00107: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
Epoch 108/1000
31/31 - 11s - loss: 1.8875 - accuracy: 0.9798 - val_loss: 2.2078 - val_accuracy: 0.8667

Epoch 00108: val_loss did not improve from 2.18701
Epoch 109/1000
31/31 - 11s - loss: 1.9498 - accuracy: 0.9773 - val_loss: 2.2220 - val_accuracy: 0.8625

Epoch 00109: val_loss did not improve from 2.18701
Epoch 110/1000
31/31 - 11s - loss: 1.9080 - accuracy: 0.9753 - val_loss: 2.2261 - val_accuracy: 0.8646

Epoch 00110: val_loss did not improve from 2.18701
Epoch 111/1000
31/31 - 11s - loss: 1.8961 - accuracy: 0.9788 - val_loss: 2.2140 - val_accuracy: 0.8646

Epoch 00111: val_loss did not improve from 2.18701

Epoch 00111: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
Epoch 112/1000
31/31 - 11s - loss: 1.9141 - accuracy: 0.9733 - val_loss: 2.2254 - val_accuracy: 0.8604

Epoch 00112: val_loss did not improve from 2.18701
Epoch 113/1000
31/31 - 11s - loss: 1.8674 - accuracy: 0.9844 - val_loss: 2.2176 - val_accuracy: 0.8604

Epoch 00113: val_loss did not improve from 2.18701
Epoch 114/1000
31/31 - 11s - loss: 1.9004 - accuracy: 0.9773 - val_loss: 2.2134 - val_accuracy: 0.8562

Epoch 00114: val_loss did not improve from 2.18701
Epoch 115/1000
31/31 - 11s - loss: 1.9586 - accuracy: 0.9728 - val_loss: 2.2021 - val_accuracy: 0.8583

Epoch 00115: val_loss did not improve from 2.18701

Epoch 00115: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
Epoch 116/1000
31/31 - 11s - loss: 1.8980 - accuracy: 0.9788 - val_loss: 2.2086 - val_accuracy: 0.8562

Epoch 00116: val_loss did not improve from 2.18701
Epoch 117/1000
31/31 - 11s - loss: 1.9129 - accuracy: 0.9783 - val_loss: 2.2061 - val_accuracy: 0.8583

Epoch 00117: val_loss did not improve from 2.18701
Epoch 118/1000
31/31 - 11s - loss: 1.9195 - accuracy: 0.9798 - val_loss: 2.2189 - val_accuracy: 0.8583

Epoch 00118: val_loss did not improve from 2.18701
Epoch 119/1000
31/31 - 11s - loss: 1.8753 - accuracy: 0.9808 - val_loss: 2.2128 - val_accuracy: 0.8583

Epoch 00119: val_loss did not improve from 2.18701

Epoch 00119: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.
Epoch 120/1000
31/31 - 11s - loss: 1.8593 - accuracy: 0.9834 - val_loss: 2.2009 - val_accuracy: 0.8583

Epoch 00120: val_loss did not improve from 2.18701
Epoch 121/1000
31/31 - 11s - loss: 1.8670 - accuracy: 0.9844 - val_loss: 2.2114 - val_accuracy: 0.8562

Epoch 00121: val_loss did not improve from 2.18701
Epoch 122/1000
31/31 - 11s - loss: 1.8462 - accuracy: 0.9874 - val_loss: 2.2086 - val_accuracy: 0.8542

Epoch 00122: val_loss did not improve from 2.18701
Epoch 123/1000
31/31 - 11s - loss: 1.8683 - accuracy: 0.9829 - val_loss: 2.2048 - val_accuracy: 0.8562

Epoch 00123: val_loss did not improve from 2.18701

Epoch 00123: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.
Epoch 124/1000
31/31 - 11s - loss: 1.9215 - accuracy: 0.9723 - val_loss: 2.2035 - val_accuracy: 0.8583

Epoch 00124: val_loss did not improve from 2.18701
Epoch 125/1000
31/31 - 11s - loss: 1.8925 - accuracy: 0.9814 - val_loss: 2.2110 - val_accuracy: 0.8583

Epoch 00125: val_loss did not improve from 2.18701
Epoch 126/1000
31/31 - 11s - loss: 1.9334 - accuracy: 0.9753 - val_loss: 2.2202 - val_accuracy: 0.8583

Epoch 00126: val_loss did not improve from 2.18701
Epoch 127/1000
31/31 - 11s - loss: 1.8797 - accuracy: 0.9844 - val_loss: 2.2122 - val_accuracy: 0.8583

Epoch 00127: val_loss did not improve from 2.18701

Epoch 00127: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.
Epoch 128/1000
31/31 - 11s - loss: 1.9425 - accuracy: 0.9778 - val_loss: 2.2219 - val_accuracy: 0.8583

Epoch 00128: val_loss did not improve from 2.18701
Epoch 129/1000
31/31 - 11s - loss: 1.8370 - accuracy: 0.9864 - val_loss: 2.2169 - val_accuracy: 0.8562

Epoch 00129: val_loss did not improve from 2.18701
Epoch 130/1000
31/31 - 11s - loss: 1.8836 - accuracy: 0.9808 - val_loss: 2.2065 - val_accuracy: 0.8583

Epoch 00130: val_loss did not improve from 2.18701
Epoch 131/1000
31/31 - 11s - loss: 1.9301 - accuracy: 0.9728 - val_loss: 2.2229 - val_accuracy: 0.8625

Epoch 00131: val_loss did not improve from 2.18701

Epoch 00131: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.
Epoch 132/1000
31/31 - 11s - loss: 1.9353 - accuracy: 0.9738 - val_loss: 2.2070 - val_accuracy: 0.8667

Epoch 00132: val_loss did not improve from 2.18701
Epoch 133/1000
31/31 - 11s - loss: 1.9313 - accuracy: 0.9738 - val_loss: 2.2260 - val_accuracy: 0.8625

Epoch 00133: val_loss did not improve from 2.18701
Epoch 00133: early stopping
>> join - Process-4 at Sun Aug  1 09:03:08 2021
>> Process-4 spend 0h 27m 2.4s

>> start - Process-5 at Sun Aug  1 09:03:12 2021
Epoch 1/1000
31/31 - 58s - loss: 199.4529 - accuracy: 0.3004 - val_loss: 131.9607 - val_accuracy: 0.4806

Epoch 00001: val_loss improved from inf to 131.96074, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 2/1000
31/31 - 11s - loss: 99.2826 - accuracy: 0.5237 - val_loss: 67.9690 - val_accuracy: 0.5043

Epoch 00002: val_loss improved from 131.96074 to 67.96899, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 3/1000
31/31 - 11s - loss: 53.6366 - accuracy: 0.5706 - val_loss: 38.2954 - val_accuracy: 0.5043

Epoch 00003: val_loss improved from 67.96899 to 38.29537, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 4/1000
31/31 - 11s - loss: 31.8503 - accuracy: 0.5867 - val_loss: 23.7025 - val_accuracy: 0.5043

Epoch 00004: val_loss improved from 38.29537 to 23.70246, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 5/1000
31/31 - 11s - loss: 20.9753 - accuracy: 0.6159 - val_loss: 16.5672 - val_accuracy: 0.5108

Epoch 00005: val_loss improved from 23.70246 to 16.56724, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 6/1000
31/31 - 11s - loss: 15.6558 - accuracy: 0.6376 - val_loss: 13.3371 - val_accuracy: 0.5022

Epoch 00006: val_loss improved from 16.56724 to 13.33706, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 7/1000
31/31 - 11s - loss: 12.8082 - accuracy: 0.6321 - val_loss: 11.4389 - val_accuracy: 0.5086

Epoch 00007: val_loss improved from 13.33706 to 11.43889, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 8/1000
31/31 - 11s - loss: 11.3449 - accuracy: 0.6386 - val_loss: 10.3941 - val_accuracy: 0.5129

Epoch 00008: val_loss improved from 11.43889 to 10.39407, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 9/1000
31/31 - 11s - loss: 10.3311 - accuracy: 0.6668 - val_loss: 9.3933 - val_accuracy: 0.5086

Epoch 00009: val_loss improved from 10.39407 to 9.39334, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 10/1000
31/31 - 11s - loss: 9.4073 - accuracy: 0.6880 - val_loss: 8.9637 - val_accuracy: 0.4978

Epoch 00010: val_loss improved from 9.39334 to 8.96369, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 11/1000
31/31 - 11s - loss: 9.0604 - accuracy: 0.6804 - val_loss: 11.1294 - val_accuracy: 0.0345

Epoch 00011: val_loss did not improve from 8.96369
Epoch 12/1000
31/31 - 11s - loss: 8.6171 - accuracy: 0.6835 - val_loss: 8.7601 - val_accuracy: 0.5065

Epoch 00012: val_loss improved from 8.96369 to 8.76005, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 13/1000
31/31 - 11s - loss: 7.8589 - accuracy: 0.7349 - val_loss: 8.2369 - val_accuracy: 0.5216

Epoch 00013: val_loss improved from 8.76005 to 8.23690, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 14/1000
31/31 - 11s - loss: 7.5940 - accuracy: 0.7268 - val_loss: 7.3579 - val_accuracy: 0.5431

Epoch 00014: val_loss improved from 8.23690 to 7.35793, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 15/1000
31/31 - 11s - loss: 7.3856 - accuracy: 0.7092 - val_loss: 6.5682 - val_accuracy: 0.5388

Epoch 00015: val_loss improved from 7.35793 to 6.56822, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 16/1000
31/31 - 11s - loss: 6.8704 - accuracy: 0.7379 - val_loss: 6.4527 - val_accuracy: 0.5474

Epoch 00016: val_loss improved from 6.56822 to 6.45268, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 17/1000
31/31 - 11s - loss: 6.6893 - accuracy: 0.7369 - val_loss: 7.8569 - val_accuracy: 0.0582

Epoch 00017: val_loss did not improve from 6.45268
Epoch 18/1000
31/31 - 11s - loss: 6.3014 - accuracy: 0.7591 - val_loss: 8.8980 - val_accuracy: 0.1121

Epoch 00018: val_loss did not improve from 6.45268
Epoch 19/1000
31/31 - 11s - loss: 6.3486 - accuracy: 0.7445 - val_loss: 7.0360 - val_accuracy: 0.3168

Epoch 00019: val_loss did not improve from 6.45268
Epoch 20/1000
31/31 - 11s - loss: 6.3218 - accuracy: 0.7324 - val_loss: 5.3272 - val_accuracy: 0.6056

Epoch 00020: val_loss improved from 6.45268 to 5.32723, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 21/1000
31/31 - 11s - loss: 5.7192 - accuracy: 0.7692 - val_loss: 5.2786 - val_accuracy: 0.5474

Epoch 00021: val_loss improved from 5.32723 to 5.27855, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 22/1000
31/31 - 11s - loss: 5.7660 - accuracy: 0.7601 - val_loss: 5.6626 - val_accuracy: 0.5259

Epoch 00022: val_loss did not improve from 5.27855
Epoch 23/1000
31/31 - 11s - loss: 5.6290 - accuracy: 0.7646 - val_loss: 7.6884 - val_accuracy: 0.0690

Epoch 00023: val_loss did not improve from 5.27855
Epoch 24/1000
31/31 - 11s - loss: 5.1972 - accuracy: 0.7762 - val_loss: 6.1185 - val_accuracy: 0.1659

Epoch 00024: val_loss did not improve from 5.27855
Epoch 25/1000
31/31 - 11s - loss: 5.3136 - accuracy: 0.7661 - val_loss: 9.9783 - val_accuracy: 0.1034

Epoch 00025: val_loss did not improve from 5.27855

Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 26/1000
31/31 - 11s - loss: 4.5662 - accuracy: 0.8261 - val_loss: 5.3702 - val_accuracy: 0.2091

Epoch 00026: val_loss did not improve from 5.27855
Epoch 27/1000
31/31 - 11s - loss: 4.1189 - accuracy: 0.8337 - val_loss: 4.9365 - val_accuracy: 0.2608

Epoch 00027: val_loss improved from 5.27855 to 4.93649, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 28/1000
31/31 - 11s - loss: 4.1941 - accuracy: 0.8054 - val_loss: 5.4526 - val_accuracy: 0.2522

Epoch 00028: val_loss did not improve from 4.93649
Epoch 29/1000
31/31 - 11s - loss: 4.1194 - accuracy: 0.8201 - val_loss: 4.4528 - val_accuracy: 0.5194

Epoch 00029: val_loss improved from 4.93649 to 4.45280, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 30/1000
31/31 - 11s - loss: 3.9073 - accuracy: 0.8372 - val_loss: 4.7472 - val_accuracy: 0.4655

Epoch 00030: val_loss did not improve from 4.45280
Epoch 31/1000
31/31 - 11s - loss: 3.8917 - accuracy: 0.8317 - val_loss: 3.8278 - val_accuracy: 0.6487

Epoch 00031: val_loss improved from 4.45280 to 3.82784, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 32/1000
31/31 - 11s - loss: 3.8072 - accuracy: 0.8387 - val_loss: 4.2059 - val_accuracy: 0.5065

Epoch 00032: val_loss did not improve from 3.82784
Epoch 33/1000
31/31 - 11s - loss: 3.6535 - accuracy: 0.8488 - val_loss: 3.9530 - val_accuracy: 0.5323

Epoch 00033: val_loss did not improve from 3.82784
Epoch 34/1000
31/31 - 11s - loss: 3.8741 - accuracy: 0.8261 - val_loss: 4.9007 - val_accuracy: 0.4698

Epoch 00034: val_loss did not improve from 3.82784
Epoch 35/1000
31/31 - 11s - loss: 3.8684 - accuracy: 0.8306 - val_loss: 4.8347 - val_accuracy: 0.4591

Epoch 00035: val_loss did not improve from 3.82784

Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 36/1000
31/31 - 11s - loss: 3.3158 - accuracy: 0.8831 - val_loss: 3.2568 - val_accuracy: 0.7198

Epoch 00036: val_loss improved from 3.82784 to 3.25684, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 37/1000
31/31 - 11s - loss: 3.0528 - accuracy: 0.8977 - val_loss: 3.0364 - val_accuracy: 0.7134

Epoch 00037: val_loss improved from 3.25684 to 3.03637, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 38/1000
31/31 - 11s - loss: 3.1051 - accuracy: 0.8700 - val_loss: 3.0214 - val_accuracy: 0.7349

Epoch 00038: val_loss improved from 3.03637 to 3.02141, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 39/1000
31/31 - 11s - loss: 3.0274 - accuracy: 0.8785 - val_loss: 3.3799 - val_accuracy: 0.6724

Epoch 00039: val_loss did not improve from 3.02141
Epoch 40/1000
31/31 - 11s - loss: 2.9914 - accuracy: 0.8745 - val_loss: 3.0356 - val_accuracy: 0.7241

Epoch 00040: val_loss did not improve from 3.02141
Epoch 41/1000
31/31 - 11s - loss: 2.8080 - accuracy: 0.9027 - val_loss: 2.7519 - val_accuracy: 0.7651

Epoch 00041: val_loss improved from 3.02141 to 2.75188, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 42/1000
31/31 - 11s - loss: 2.9900 - accuracy: 0.8826 - val_loss: 2.8727 - val_accuracy: 0.7328

Epoch 00042: val_loss did not improve from 2.75188
Epoch 43/1000
31/31 - 11s - loss: 2.9549 - accuracy: 0.8790 - val_loss: 2.9685 - val_accuracy: 0.7155

Epoch 00043: val_loss did not improve from 2.75188
Epoch 44/1000
31/31 - 11s - loss: 2.8700 - accuracy: 0.8841 - val_loss: 2.8764 - val_accuracy: 0.7241

Epoch 00044: val_loss did not improve from 2.75188
Epoch 45/1000
31/31 - 11s - loss: 2.8306 - accuracy: 0.8891 - val_loss: 2.8198 - val_accuracy: 0.7759

Epoch 00045: val_loss did not improve from 2.75188

Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 46/1000
31/31 - 11s - loss: 2.6208 - accuracy: 0.9249 - val_loss: 2.6575 - val_accuracy: 0.7802

Epoch 00046: val_loss improved from 2.75188 to 2.65745, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 47/1000
31/31 - 11s - loss: 2.6999 - accuracy: 0.9037 - val_loss: 2.6078 - val_accuracy: 0.7694

Epoch 00047: val_loss improved from 2.65745 to 2.60784, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 48/1000
31/31 - 11s - loss: 2.4137 - accuracy: 0.9350 - val_loss: 2.4706 - val_accuracy: 0.7909

Epoch 00048: val_loss improved from 2.60784 to 2.47056, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 49/1000
31/31 - 11s - loss: 2.2830 - accuracy: 0.9486 - val_loss: 2.7099 - val_accuracy: 0.7629

Epoch 00049: val_loss did not improve from 2.47056
Epoch 50/1000
31/31 - 11s - loss: 2.4935 - accuracy: 0.9093 - val_loss: 2.8328 - val_accuracy: 0.7263

Epoch 00050: val_loss did not improve from 2.47056
Epoch 51/1000
31/31 - 11s - loss: 2.3432 - accuracy: 0.9309 - val_loss: 2.3995 - val_accuracy: 0.7974

Epoch 00051: val_loss improved from 2.47056 to 2.39953, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 52/1000
31/31 - 11s - loss: 2.3072 - accuracy: 0.9365 - val_loss: 2.3970 - val_accuracy: 0.7802

Epoch 00052: val_loss improved from 2.39953 to 2.39701, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 53/1000
31/31 - 11s - loss: 2.4504 - accuracy: 0.9158 - val_loss: 2.4079 - val_accuracy: 0.8125

Epoch 00053: val_loss did not improve from 2.39701
Epoch 54/1000
31/31 - 11s - loss: 2.4929 - accuracy: 0.9057 - val_loss: 2.6708 - val_accuracy: 0.7802

Epoch 00054: val_loss did not improve from 2.39701
Epoch 55/1000
31/31 - 11s - loss: 2.3803 - accuracy: 0.9204 - val_loss: 2.2841 - val_accuracy: 0.8384

Epoch 00055: val_loss improved from 2.39701 to 2.28406, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 56/1000
31/31 - 11s - loss: 2.3390 - accuracy: 0.9214 - val_loss: 2.3119 - val_accuracy: 0.8125

Epoch 00056: val_loss did not improve from 2.28406
Epoch 57/1000
31/31 - 11s - loss: 2.2563 - accuracy: 0.9320 - val_loss: 2.7697 - val_accuracy: 0.7780

Epoch 00057: val_loss did not improve from 2.28406
Epoch 58/1000
31/31 - 11s - loss: 2.3787 - accuracy: 0.9234 - val_loss: 2.3666 - val_accuracy: 0.7780

Epoch 00058: val_loss did not improve from 2.28406
Epoch 59/1000
31/31 - 11s - loss: 2.2736 - accuracy: 0.9335 - val_loss: 2.2511 - val_accuracy: 0.8060

Epoch 00059: val_loss improved from 2.28406 to 2.25109, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 60/1000
31/31 - 11s - loss: 2.1551 - accuracy: 0.9466 - val_loss: 2.2834 - val_accuracy: 0.8254

Epoch 00060: val_loss did not improve from 2.25109
Epoch 61/1000
31/31 - 11s - loss: 2.1535 - accuracy: 0.9446 - val_loss: 2.1535 - val_accuracy: 0.8448

Epoch 00061: val_loss improved from 2.25109 to 2.15353, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 62/1000
31/31 - 11s - loss: 2.2086 - accuracy: 0.9441 - val_loss: 2.2596 - val_accuracy: 0.8233

Epoch 00062: val_loss did not improve from 2.15353
Epoch 63/1000
31/31 - 11s - loss: 2.3018 - accuracy: 0.9244 - val_loss: 2.3643 - val_accuracy: 0.7931

Epoch 00063: val_loss did not improve from 2.15353
Epoch 64/1000
31/31 - 11s - loss: 2.1966 - accuracy: 0.9325 - val_loss: 2.4265 - val_accuracy: 0.7694

Epoch 00064: val_loss did not improve from 2.15353
Epoch 65/1000
31/31 - 11s - loss: 2.2728 - accuracy: 0.9194 - val_loss: 2.6175 - val_accuracy: 0.7651

Epoch 00065: val_loss did not improve from 2.15353

Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
Epoch 66/1000
31/31 - 11s - loss: 2.1706 - accuracy: 0.9385 - val_loss: 2.5207 - val_accuracy: 0.7759

Epoch 00066: val_loss did not improve from 2.15353
Epoch 67/1000
31/31 - 11s - loss: 2.0138 - accuracy: 0.9582 - val_loss: 2.4219 - val_accuracy: 0.7845

Epoch 00067: val_loss did not improve from 2.15353
Epoch 68/1000
31/31 - 11s - loss: 2.0010 - accuracy: 0.9501 - val_loss: 2.2355 - val_accuracy: 0.8190

Epoch 00068: val_loss did not improve from 2.15353
Epoch 69/1000
31/31 - 11s - loss: 2.0051 - accuracy: 0.9506 - val_loss: 2.1837 - val_accuracy: 0.8168

Epoch 00069: val_loss did not improve from 2.15353

Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
Epoch 70/1000
31/31 - 11s - loss: 1.9562 - accuracy: 0.9567 - val_loss: 2.1282 - val_accuracy: 0.8254

Epoch 00070: val_loss improved from 2.15353 to 2.12823, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 71/1000
31/31 - 11s - loss: 1.9156 - accuracy: 0.9592 - val_loss: 2.0285 - val_accuracy: 0.8491

Epoch 00071: val_loss improved from 2.12823 to 2.02847, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 72/1000
31/31 - 11s - loss: 1.9627 - accuracy: 0.9551 - val_loss: 2.0058 - val_accuracy: 0.8448

Epoch 00072: val_loss improved from 2.02847 to 2.00584, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 73/1000
31/31 - 11s - loss: 1.9369 - accuracy: 0.9556 - val_loss: 2.0361 - val_accuracy: 0.8405

Epoch 00073: val_loss did not improve from 2.00584
Epoch 74/1000
31/31 - 11s - loss: 1.9373 - accuracy: 0.9612 - val_loss: 1.9909 - val_accuracy: 0.8707

Epoch 00074: val_loss improved from 2.00584 to 1.99093, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 75/1000
31/31 - 11s - loss: 1.7526 - accuracy: 0.9824 - val_loss: 1.9871 - val_accuracy: 0.8621

Epoch 00075: val_loss improved from 1.99093 to 1.98715, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 76/1000
31/31 - 11s - loss: 1.9297 - accuracy: 0.9561 - val_loss: 1.9810 - val_accuracy: 0.8642

Epoch 00076: val_loss improved from 1.98715 to 1.98095, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 77/1000
31/31 - 11s - loss: 1.7460 - accuracy: 0.9788 - val_loss: 1.9905 - val_accuracy: 0.8642

Epoch 00077: val_loss did not improve from 1.98095
Epoch 78/1000
31/31 - 11s - loss: 1.8215 - accuracy: 0.9672 - val_loss: 1.9688 - val_accuracy: 0.8685

Epoch 00078: val_loss improved from 1.98095 to 1.96876, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 79/1000
31/31 - 11s - loss: 1.8379 - accuracy: 0.9617 - val_loss: 2.0115 - val_accuracy: 0.8556

Epoch 00079: val_loss did not improve from 1.96876
Epoch 80/1000
31/31 - 11s - loss: 1.8692 - accuracy: 0.9627 - val_loss: 1.9724 - val_accuracy: 0.8513

Epoch 00080: val_loss did not improve from 1.96876
Epoch 81/1000
31/31 - 11s - loss: 1.8468 - accuracy: 0.9647 - val_loss: 1.9647 - val_accuracy: 0.8556

Epoch 00081: val_loss improved from 1.96876 to 1.96471, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 82/1000
31/31 - 11s - loss: 1.8153 - accuracy: 0.9667 - val_loss: 1.9622 - val_accuracy: 0.8664

Epoch 00082: val_loss improved from 1.96471 to 1.96215, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 83/1000
31/31 - 11s - loss: 1.8523 - accuracy: 0.9597 - val_loss: 1.9677 - val_accuracy: 0.8664

Epoch 00083: val_loss did not improve from 1.96215
Epoch 84/1000
31/31 - 11s - loss: 1.9076 - accuracy: 0.9506 - val_loss: 2.0234 - val_accuracy: 0.8384

Epoch 00084: val_loss did not improve from 1.96215
Epoch 85/1000
31/31 - 11s - loss: 1.8538 - accuracy: 0.9617 - val_loss: 1.9349 - val_accuracy: 0.8685

Epoch 00085: val_loss improved from 1.96215 to 1.93494, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 86/1000
31/31 - 11s - loss: 1.7370 - accuracy: 0.9718 - val_loss: 1.9365 - val_accuracy: 0.8728

Epoch 00086: val_loss did not improve from 1.93494
Epoch 87/1000
31/31 - 11s - loss: 1.8098 - accuracy: 0.9647 - val_loss: 1.9723 - val_accuracy: 0.8621

Epoch 00087: val_loss did not improve from 1.93494
Epoch 88/1000
31/31 - 11s - loss: 1.7721 - accuracy: 0.9768 - val_loss: 1.9648 - val_accuracy: 0.8578

Epoch 00088: val_loss did not improve from 1.93494
Epoch 89/1000
31/31 - 11s - loss: 1.7441 - accuracy: 0.9723 - val_loss: 1.9879 - val_accuracy: 0.8427

Epoch 00089: val_loss did not improve from 1.93494

Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
Epoch 90/1000
31/31 - 11s - loss: 1.7649 - accuracy: 0.9677 - val_loss: 1.9775 - val_accuracy: 0.8448

Epoch 00090: val_loss did not improve from 1.93494
Epoch 91/1000
31/31 - 11s - loss: 1.6973 - accuracy: 0.9773 - val_loss: 1.9303 - val_accuracy: 0.8621

Epoch 00091: val_loss improved from 1.93494 to 1.93026, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 92/1000
31/31 - 11s - loss: 1.6946 - accuracy: 0.9788 - val_loss: 1.9434 - val_accuracy: 0.8621

Epoch 00092: val_loss did not improve from 1.93026
Epoch 93/1000
31/31 - 11s - loss: 1.7957 - accuracy: 0.9612 - val_loss: 1.9181 - val_accuracy: 0.8664

Epoch 00093: val_loss improved from 1.93026 to 1.91807, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 94/1000
31/31 - 11s - loss: 1.7903 - accuracy: 0.9632 - val_loss: 1.9199 - val_accuracy: 0.8621

Epoch 00094: val_loss did not improve from 1.91807
Epoch 95/1000
31/31 - 11s - loss: 1.7587 - accuracy: 0.9667 - val_loss: 1.9044 - val_accuracy: 0.8621

Epoch 00095: val_loss improved from 1.91807 to 1.90440, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 96/1000
31/31 - 11s - loss: 1.6754 - accuracy: 0.9788 - val_loss: 1.9044 - val_accuracy: 0.8685

Epoch 00096: val_loss did not improve from 1.90440
Epoch 97/1000
31/31 - 11s - loss: 1.7660 - accuracy: 0.9667 - val_loss: 1.9145 - val_accuracy: 0.8642

Epoch 00097: val_loss did not improve from 1.90440
Epoch 98/1000
31/31 - 11s - loss: 1.7005 - accuracy: 0.9763 - val_loss: 1.9115 - val_accuracy: 0.8664

Epoch 00098: val_loss did not improve from 1.90440
Epoch 99/1000
31/31 - 11s - loss: 1.6825 - accuracy: 0.9768 - val_loss: 1.8925 - val_accuracy: 0.8685

Epoch 00099: val_loss improved from 1.90440 to 1.89253, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 100/1000
31/31 - 11s - loss: 1.6612 - accuracy: 0.9778 - val_loss: 1.8883 - val_accuracy: 0.8642

Epoch 00100: val_loss improved from 1.89253 to 1.88830, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 101/1000
31/31 - 11s - loss: 1.7714 - accuracy: 0.9612 - val_loss: 1.9243 - val_accuracy: 0.8534

Epoch 00101: val_loss did not improve from 1.88830
Epoch 102/1000
31/31 - 11s - loss: 1.7369 - accuracy: 0.9723 - val_loss: 1.9054 - val_accuracy: 0.8685

Epoch 00102: val_loss did not improve from 1.88830
Epoch 103/1000
31/31 - 11s - loss: 1.7751 - accuracy: 0.9662 - val_loss: 1.9012 - val_accuracy: 0.8707

Epoch 00103: val_loss did not improve from 1.88830
Epoch 104/1000
31/31 - 11s - loss: 1.6634 - accuracy: 0.9793 - val_loss: 1.8877 - val_accuracy: 0.8728

Epoch 00104: val_loss improved from 1.88830 to 1.88767, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 105/1000
31/31 - 11s - loss: 1.6885 - accuracy: 0.9748 - val_loss: 1.9082 - val_accuracy: 0.8664

Epoch 00105: val_loss did not improve from 1.88767
Epoch 106/1000
31/31 - 11s - loss: 1.6945 - accuracy: 0.9758 - val_loss: 1.9029 - val_accuracy: 0.8578

Epoch 00106: val_loss did not improve from 1.88767
Epoch 107/1000
31/31 - 11s - loss: 1.6590 - accuracy: 0.9758 - val_loss: 1.8888 - val_accuracy: 0.8685

Epoch 00107: val_loss did not improve from 1.88767
Epoch 108/1000
31/31 - 11s - loss: 1.6554 - accuracy: 0.9788 - val_loss: 1.8837 - val_accuracy: 0.8599

Epoch 00108: val_loss improved from 1.88767 to 1.88372, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 109/1000
31/31 - 11s - loss: 1.6383 - accuracy: 0.9798 - val_loss: 1.8706 - val_accuracy: 0.8621

Epoch 00109: val_loss improved from 1.88372 to 1.87059, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 110/1000
31/31 - 11s - loss: 1.6384 - accuracy: 0.9834 - val_loss: 1.8675 - val_accuracy: 0.8642

Epoch 00110: val_loss improved from 1.87059 to 1.86747, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 111/1000
31/31 - 11s - loss: 1.6440 - accuracy: 0.9793 - val_loss: 1.8687 - val_accuracy: 0.8772

Epoch 00111: val_loss did not improve from 1.86747
Epoch 112/1000
31/31 - 11s - loss: 1.6380 - accuracy: 0.9753 - val_loss: 1.8648 - val_accuracy: 0.8664

Epoch 00112: val_loss improved from 1.86747 to 1.86482, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 113/1000
31/31 - 11s - loss: 1.6929 - accuracy: 0.9713 - val_loss: 1.8768 - val_accuracy: 0.8793

Epoch 00113: val_loss did not improve from 1.86482
Epoch 114/1000
31/31 - 11s - loss: 1.6577 - accuracy: 0.9733 - val_loss: 1.8445 - val_accuracy: 0.8793

Epoch 00114: val_loss improved from 1.86482 to 1.84452, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 115/1000
31/31 - 11s - loss: 1.6462 - accuracy: 0.9773 - val_loss: 1.8751 - val_accuracy: 0.8685

Epoch 00115: val_loss did not improve from 1.84452
Epoch 116/1000
31/31 - 11s - loss: 1.6584 - accuracy: 0.9773 - val_loss: 1.8682 - val_accuracy: 0.8793

Epoch 00116: val_loss did not improve from 1.84452
Epoch 117/1000
31/31 - 11s - loss: 1.6308 - accuracy: 0.9783 - val_loss: 1.8451 - val_accuracy: 0.8772

Epoch 00117: val_loss did not improve from 1.84452
Epoch 118/1000
31/31 - 11s - loss: 1.6642 - accuracy: 0.9748 - val_loss: 1.8495 - val_accuracy: 0.8772

Epoch 00118: val_loss did not improve from 1.84452

Epoch 00118: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
Epoch 119/1000
31/31 - 11s - loss: 1.6681 - accuracy: 0.9718 - val_loss: 1.8286 - val_accuracy: 0.8772

Epoch 00119: val_loss improved from 1.84452 to 1.82861, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 120/1000
31/31 - 11s - loss: 1.6127 - accuracy: 0.9793 - val_loss: 1.8315 - val_accuracy: 0.8793

Epoch 00120: val_loss did not improve from 1.82861
Epoch 121/1000
31/31 - 11s - loss: 1.6469 - accuracy: 0.9738 - val_loss: 1.8565 - val_accuracy: 0.8642

Epoch 00121: val_loss did not improve from 1.82861
Epoch 122/1000
31/31 - 11s - loss: 1.6302 - accuracy: 0.9758 - val_loss: 1.8114 - val_accuracy: 0.8728

Epoch 00122: val_loss improved from 1.82861 to 1.81137, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 123/1000
31/31 - 11s - loss: 1.6210 - accuracy: 0.9793 - val_loss: 1.8309 - val_accuracy: 0.8707

Epoch 00123: val_loss did not improve from 1.81137
Epoch 124/1000
31/31 - 11s - loss: 1.6237 - accuracy: 0.9788 - val_loss: 1.8381 - val_accuracy: 0.8793

Epoch 00124: val_loss did not improve from 1.81137
Epoch 125/1000
31/31 - 11s - loss: 1.6780 - accuracy: 0.9723 - val_loss: 1.8480 - val_accuracy: 0.8707

Epoch 00125: val_loss did not improve from 1.81137
Epoch 126/1000
31/31 - 11s - loss: 1.6379 - accuracy: 0.9778 - val_loss: 1.8584 - val_accuracy: 0.8728

Epoch 00126: val_loss did not improve from 1.81137

Epoch 00126: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.
Epoch 127/1000
31/31 - 11s - loss: 1.6299 - accuracy: 0.9753 - val_loss: 1.8440 - val_accuracy: 0.8750

Epoch 00127: val_loss did not improve from 1.81137
Epoch 128/1000
31/31 - 11s - loss: 1.6101 - accuracy: 0.9773 - val_loss: 1.8440 - val_accuracy: 0.8664

Epoch 00128: val_loss did not improve from 1.81137
Epoch 129/1000
31/31 - 11s - loss: 1.6421 - accuracy: 0.9758 - val_loss: 1.8426 - val_accuracy: 0.8685

Epoch 00129: val_loss did not improve from 1.81137
Epoch 130/1000
31/31 - 11s - loss: 1.5812 - accuracy: 0.9814 - val_loss: 1.8320 - val_accuracy: 0.8707

Epoch 00130: val_loss did not improve from 1.81137

Epoch 00130: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.
Epoch 131/1000
31/31 - 11s - loss: 1.5465 - accuracy: 0.9874 - val_loss: 1.8279 - val_accuracy: 0.8728

Epoch 00131: val_loss did not improve from 1.81137
Epoch 132/1000
31/31 - 11s - loss: 1.6396 - accuracy: 0.9798 - val_loss: 1.8288 - val_accuracy: 0.8664

Epoch 00132: val_loss did not improve from 1.81137
Epoch 133/1000
31/31 - 11s - loss: 1.6216 - accuracy: 0.9783 - val_loss: 1.8324 - val_accuracy: 0.8707

Epoch 00133: val_loss did not improve from 1.81137
Epoch 134/1000
31/31 - 11s - loss: 1.6151 - accuracy: 0.9763 - val_loss: 1.8266 - val_accuracy: 0.8728

Epoch 00134: val_loss did not improve from 1.81137

Epoch 00134: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.
Epoch 135/1000
31/31 - 11s - loss: 1.6356 - accuracy: 0.9783 - val_loss: 1.8343 - val_accuracy: 0.8664

Epoch 00135: val_loss did not improve from 1.81137
Epoch 136/1000
31/31 - 11s - loss: 1.5430 - accuracy: 0.9879 - val_loss: 1.8171 - val_accuracy: 0.8728

Epoch 00136: val_loss did not improve from 1.81137
Epoch 137/1000
31/31 - 11s - loss: 1.6251 - accuracy: 0.9798 - val_loss: 1.8320 - val_accuracy: 0.8685

Epoch 00137: val_loss did not improve from 1.81137
Epoch 138/1000
31/31 - 11s - loss: 1.5794 - accuracy: 0.9814 - val_loss: 1.8194 - val_accuracy: 0.8750

Epoch 00138: val_loss did not improve from 1.81137

Epoch 00138: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.
Epoch 139/1000
31/31 - 11s - loss: 1.5361 - accuracy: 0.9874 - val_loss: 1.8229 - val_accuracy: 0.8707

Epoch 00139: val_loss did not improve from 1.81137
Epoch 140/1000
31/31 - 11s - loss: 1.5682 - accuracy: 0.9859 - val_loss: 1.7785 - val_accuracy: 0.8815

Epoch 00140: val_loss improved from 1.81137 to 1.77852, saving model to ./save/CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam_fold_4.hdf5
Epoch 141/1000
31/31 - 11s - loss: 1.6009 - accuracy: 0.9814 - val_loss: 1.8037 - val_accuracy: 0.8728

Epoch 00141: val_loss did not improve from 1.77852
Epoch 142/1000
31/31 - 11s - loss: 1.5947 - accuracy: 0.9783 - val_loss: 1.7983 - val_accuracy: 0.8728

Epoch 00142: val_loss did not improve from 1.77852
Epoch 143/1000
31/31 - 11s - loss: 1.6256 - accuracy: 0.9763 - val_loss: 1.8171 - val_accuracy: 0.8728

Epoch 00143: val_loss did not improve from 1.77852
Epoch 144/1000
31/31 - 11s - loss: 1.5734 - accuracy: 0.9849 - val_loss: 1.8276 - val_accuracy: 0.8707

Epoch 00144: val_loss did not improve from 1.77852

Epoch 00144: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.
Epoch 145/1000
31/31 - 11s - loss: 1.5640 - accuracy: 0.9864 - val_loss: 1.8276 - val_accuracy: 0.8685

Epoch 00145: val_loss did not improve from 1.77852
Epoch 146/1000
31/31 - 11s - loss: 1.6030 - accuracy: 0.9808 - val_loss: 1.8108 - val_accuracy: 0.8707

Epoch 00146: val_loss did not improve from 1.77852
Epoch 147/1000
31/31 - 11s - loss: 1.5453 - accuracy: 0.9914 - val_loss: 1.8241 - val_accuracy: 0.8685

Epoch 00147: val_loss did not improve from 1.77852
Epoch 148/1000
31/31 - 11s - loss: 1.6264 - accuracy: 0.9798 - val_loss: 1.8240 - val_accuracy: 0.8728

Epoch 00148: val_loss did not improve from 1.77852

Epoch 00148: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.
Epoch 149/1000
31/31 - 11s - loss: 1.6098 - accuracy: 0.9763 - val_loss: 1.8173 - val_accuracy: 0.8728

Epoch 00149: val_loss did not improve from 1.77852
Epoch 150/1000
31/31 - 11s - loss: 1.5984 - accuracy: 0.9788 - val_loss: 1.8132 - val_accuracy: 0.8772

Epoch 00150: val_loss did not improve from 1.77852
Epoch 151/1000
31/31 - 11s - loss: 1.5279 - accuracy: 0.9894 - val_loss: 1.8235 - val_accuracy: 0.8750

Epoch 00151: val_loss did not improve from 1.77852
Epoch 152/1000
31/31 - 11s - loss: 1.5976 - accuracy: 0.9814 - val_loss: 1.8181 - val_accuracy: 0.8772

Epoch 00152: val_loss did not improve from 1.77852

Epoch 00152: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.
Epoch 153/1000
31/31 - 11s - loss: 1.5902 - accuracy: 0.9758 - val_loss: 1.8238 - val_accuracy: 0.8707

Epoch 00153: val_loss did not improve from 1.77852
Epoch 154/1000
31/31 - 11s - loss: 1.5943 - accuracy: 0.9773 - val_loss: 1.8230 - val_accuracy: 0.8728

Epoch 00154: val_loss did not improve from 1.77852
Epoch 155/1000
31/31 - 11s - loss: 1.5711 - accuracy: 0.9819 - val_loss: 1.8252 - val_accuracy: 0.8728

Epoch 00155: val_loss did not improve from 1.77852
Epoch 156/1000
31/31 - 12s - loss: 1.5759 - accuracy: 0.9864 - val_loss: 1.8146 - val_accuracy: 0.8772

Epoch 00156: val_loss did not improve from 1.77852

Epoch 00156: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.
Epoch 157/1000
31/31 - 11s - loss: 1.5534 - accuracy: 0.9874 - val_loss: 1.8211 - val_accuracy: 0.8728

Epoch 00157: val_loss did not improve from 1.77852
Epoch 158/1000
31/31 - 11s - loss: 1.6038 - accuracy: 0.9808 - val_loss: 1.8127 - val_accuracy: 0.8750

Epoch 00158: val_loss did not improve from 1.77852
Epoch 159/1000
31/31 - 11s - loss: 1.5341 - accuracy: 0.9884 - val_loss: 1.8160 - val_accuracy: 0.8728

Epoch 00159: val_loss did not improve from 1.77852
Epoch 160/1000
31/31 - 11s - loss: 1.6244 - accuracy: 0.9763 - val_loss: 1.8056 - val_accuracy: 0.8772

Epoch 00160: val_loss did not improve from 1.77852

Epoch 00160: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.
Epoch 161/1000
31/31 - 11s - loss: 1.6073 - accuracy: 0.9768 - val_loss: 1.8254 - val_accuracy: 0.8750

Epoch 00161: val_loss did not improve from 1.77852
Epoch 162/1000
31/31 - 11s - loss: 1.5605 - accuracy: 0.9869 - val_loss: 1.8202 - val_accuracy: 0.8772

Epoch 00162: val_loss did not improve from 1.77852
Epoch 163/1000
31/31 - 11s - loss: 1.6021 - accuracy: 0.9783 - val_loss: 1.8238 - val_accuracy: 0.8750

Epoch 00163: val_loss did not improve from 1.77852
Epoch 164/1000
31/31 - 11s - loss: 1.6005 - accuracy: 0.9788 - val_loss: 1.8254 - val_accuracy: 0.8707

Epoch 00164: val_loss did not improve from 1.77852

Epoch 00164: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.
Epoch 165/1000
31/31 - 11s - loss: 1.5910 - accuracy: 0.9798 - val_loss: 1.8141 - val_accuracy: 0.8793

Epoch 00165: val_loss did not improve from 1.77852
Epoch 166/1000
31/31 - 11s - loss: 1.6011 - accuracy: 0.9793 - val_loss: 1.8195 - val_accuracy: 0.8728

Epoch 00166: val_loss did not improve from 1.77852
Epoch 167/1000
31/31 - 11s - loss: 1.6329 - accuracy: 0.9728 - val_loss: 1.7962 - val_accuracy: 0.8772

Epoch 00167: val_loss did not improve from 1.77852
Epoch 168/1000
31/31 - 11s - loss: 1.5476 - accuracy: 0.9864 - val_loss: 1.8016 - val_accuracy: 0.8728

Epoch 00168: val_loss did not improve from 1.77852

Epoch 00168: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.
Epoch 169/1000
31/31 - 11s - loss: 1.5678 - accuracy: 0.9824 - val_loss: 1.8072 - val_accuracy: 0.8728

Epoch 00169: val_loss did not improve from 1.77852
Epoch 170/1000
31/31 - 11s - loss: 1.5737 - accuracy: 0.9844 - val_loss: 1.7975 - val_accuracy: 0.8750

Epoch 00170: val_loss did not improve from 1.77852
Epoch 00170: early stopping
>> join - Process-5 at Sun Aug  1 09:36:03 2021
>> Process-5 spend 0h 32m 51.2s

>> calc predict_test and score_test
>> start - Process-6 at Sun Aug  1 09:36:07 2021
>> join - Process-6 at Sun Aug  1 09:36:42 2021

>> start - Process-7 at Sun Aug  1 09:36:46 2021
>> join - Process-7 at Sun Aug  1 09:37:19 2021

>> start - Process-8 at Sun Aug  1 09:37:23 2021
>> join - Process-8 at Sun Aug  1 09:37:55 2021

>> start - Process-9 at Sun Aug  1 09:37:59 2021
>> join - Process-9 at Sun Aug  1 09:38:29 2021

>> start - Process-10 at Sun Aug  1 09:38:33 2021
>> join - Process-10 at Sun Aug  1 09:39:11 2021

weighted: {'accuracy': 0.8832, 'recall': 0.8832, 'precision': 0.876208835149525, 'f1': 0.8750873892649175}
macro: {'accuracy': 0.8832, 'recall': 0.7495969338398156, 'precision': 0.7878396743009519, 'f1': 0.7586475470662245}
log_loss : 0.39645755443326586
>> save prediction of testset as pickle
>> Finished! : Sun Aug  1 09:39:15 2021
>> whole process spend 2h 55m 25.7s

------------------------------

>> Start submit.py
features_to_npy at ./data/X_submit : 78/782
features_to_npy at ./data/X_submit : 156/782
features_to_npy at ./data/X_submit : 234/782
features_to_npy at ./data/X_submit : 312/782
features_to_npy at ./data/X_submit : 390/782
features_to_npy at ./data/X_submit : 468/782
features_to_npy at ./data/X_submit : 546/782
features_to_npy at ./data/X_submit : 624/782
features_to_npy at ./data/X_submit : 702/782
features_to_npy at ./data/X_submit : 780/782
features_to_npy at ./data/X_submit : 782/782
submit data size : 782
>>> svae file name submit_CNN_RNN_Resnet_5_use_mp_and_class_weight_bias_1_seed42_alpha_1E-01_res_num_5_opt_Adam
>> submit dataset predict
>> start - Process-11 at Sun Aug  1 09:45:37 2021
>> join - Process-11 at Sun Aug  1 09:45:54 2021

>> start - Process-12 at Sun Aug  1 09:45:58 2021
>> join - Process-12 at Sun Aug  1 09:46:15 2021

>> start - Process-13 at Sun Aug  1 09:46:19 2021
>> join - Process-13 at Sun Aug  1 09:46:36 2021

>> start - Process-14 at Sun Aug  1 09:46:40 2021
>> join - Process-14 at Sun Aug  1 09:46:57 2021

>> start - Process-15 at Sun Aug  1 09:47:01 2021
>> join - Process-15 at Sun Aug  1 09:47:17 2021

>> Finished!
>> model_summary
Model: "model_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_7 (InputLayer)            [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_9 (InputLayer)            [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_5 (InputLayer)            [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_11 (InputLayer)           [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_13 (InputLayer)           [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           [(None, 600, 3)]     0                                            
__________________________________________________________________________________________________
input_15 (InputLayer)           [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           [(None, 600, 1)]     0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 600, 5)       0           input_1[0][0]                    
                                                                 input_2[0][0]                    
                                                                 input_3[0][0]                    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 600, 5)       0           input_6[0][0]                    
                                                                 input_7[0][0]                    
                                                                 input_8[0][0]                    
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 600, 8)       0           input_4[0][0]                    
                                                                 input_9[0][0]                    
                                                                 input_5[0][0]                    
                                                                 input_10[0][0]                   
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 600, 4)       0           input_4[0][0]                    
                                                                 input_5[0][0]                    
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 600, 4)       0           input_9[0][0]                    
                                                                 input_10[0][0]                   
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 600, 10)      0           input_1[0][0]                    
                                                                 input_6[0][0]                    
                                                                 input_12[0][0]                   
                                                                 input_11[0][0]                   
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 600, 8)       0           input_13[0][0]                   
                                                                 input_14[0][0]                   
                                                                 input_15[0][0]                   
                                                                 input_16[0][0]                   
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 300, 20)      1820        concatenate[0][0]                
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 300, 20)      1820        concatenate_1[0][0]              
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 300, 20)      2900        concatenate_3[0][0]              
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 300, 20)      1460        concatenate_4[0][0]              
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 300, 20)      1460        concatenate_5[0][0]              
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 300, 20)      3620        concatenate_2[0][0]              
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 300, 20)      2900        concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 300, 20)      80          conv1d[0][0]                     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 300, 20)      80          conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 300, 20)      80          conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 300, 20)      80          conv1d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 300, 20)      80          conv1d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 300, 20)      80          conv1d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 300, 20)      80          conv1d_18[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu (LeakyReLU)         (None, 300, 20)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)       (None, 300, 20)      0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)      (None, 300, 20)      0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_24 (LeakyReLU)      (None, 300, 20)      0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_30 (LeakyReLU)      (None, 300, 20)      0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)      (None, 300, 20)      0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_36 (LeakyReLU)      (None, 300, 20)      0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
gru (GRU)                       (None, 300, 40)      7440        leaky_re_lu[0][0]                
__________________________________________________________________________________________________
gru_3 (GRU)                     (None, 300, 40)      7440        leaky_re_lu_6[0][0]              
__________________________________________________________________________________________________
gru_9 (GRU)                     (None, 300, 40)      7440        leaky_re_lu_18[0][0]             
__________________________________________________________________________________________________
gru_12 (GRU)                    (None, 300, 40)      7440        leaky_re_lu_24[0][0]             
__________________________________________________________________________________________________
gru_15 (GRU)                    (None, 300, 40)      7440        leaky_re_lu_30[0][0]             
__________________________________________________________________________________________________
gru_6 (GRU)                     (None, 300, 40)      7440        leaky_re_lu_12[0][0]             
__________________________________________________________________________________________________
gru_18 (GRU)                    (None, 300, 40)      7440        leaky_re_lu_36[0][0]             
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 300, 40)      160         gru[0][0]                        
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 300, 40)      160         gru_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 300, 40)      160         gru_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 300, 40)      160         gru_12[0][0]                     
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 300, 40)      160         gru_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 300, 40)      160         gru_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 300, 40)      160         gru_18[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)       (None, 300, 40)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)       (None, 300, 40)      0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)      (None, 300, 40)      0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_25 (LeakyReLU)      (None, 300, 40)      0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_31 (LeakyReLU)      (None, 300, 40)      0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)      (None, 300, 40)      0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_37 (LeakyReLU)      (None, 300, 40)      0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 150, 120)     43320       leaky_re_lu_1[0][0]              
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 150, 120)     43320       leaky_re_lu_7[0][0]              
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 150, 120)     43320       leaky_re_lu_19[0][0]             
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 150, 120)     43320       leaky_re_lu_25[0][0]             
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 150, 120)     43320       leaky_re_lu_31[0][0]             
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 150, 120)     43320       leaky_re_lu_13[0][0]             
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 150, 120)     43320       leaky_re_lu_37[0][0]             
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 150, 120)     480         conv1d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 150, 120)     480         conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 150, 120)     480         conv1d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 150, 120)     480         conv1d_13[0][0]                  
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 150, 120)     480         conv1d_16[0][0]                  
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 150, 120)     480         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 150, 120)     480         conv1d_19[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)       (None, 150, 120)     0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)       (None, 150, 120)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_26 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_32 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_38 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
gru_1 (GRU)                     (None, 150, 120)     87120       leaky_re_lu_2[0][0]              
__________________________________________________________________________________________________
gru_4 (GRU)                     (None, 150, 120)     87120       leaky_re_lu_8[0][0]              
__________________________________________________________________________________________________
gru_10 (GRU)                    (None, 150, 120)     87120       leaky_re_lu_20[0][0]             
__________________________________________________________________________________________________
gru_13 (GRU)                    (None, 150, 120)     87120       leaky_re_lu_26[0][0]             
__________________________________________________________________________________________________
gru_16 (GRU)                    (None, 150, 120)     87120       leaky_re_lu_32[0][0]             
__________________________________________________________________________________________________
gru_7 (GRU)                     (None, 150, 120)     87120       leaky_re_lu_14[0][0]             
__________________________________________________________________________________________________
gru_19 (GRU)                    (None, 150, 120)     87120       leaky_re_lu_38[0][0]             
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 150, 120)     480         gru_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 150, 120)     480         gru_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 150, 120)     480         gru_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 150, 120)     480         gru_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 150, 120)     480         gru_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 150, 120)     480         gru_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 150, 120)     480         gru_19[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)       (None, 150, 120)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)       (None, 150, 120)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_27 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_33 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_39 (LeakyReLU)      (None, 150, 120)     0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 75, 120)      129720      leaky_re_lu_3[0][0]              
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 75, 120)      129720      leaky_re_lu_9[0][0]              
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 75, 120)      129720      leaky_re_lu_21[0][0]             
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 75, 120)      129720      leaky_re_lu_27[0][0]             
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 75, 120)      129720      leaky_re_lu_33[0][0]             
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 75, 120)      129720      leaky_re_lu_15[0][0]             
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 75, 120)      129720      leaky_re_lu_39[0][0]             
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 75, 120)      480         conv1d_2[0][0]                   
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 75, 120)      480         conv1d_5[0][0]                   
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 75, 120)      480         conv1d_11[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 75, 120)      480         conv1d_14[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 75, 120)      480         conv1d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 75, 120)      480         conv1d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 75, 120)      480         conv1d_20[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)       (None, 75, 120)      0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_28 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_34 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_40 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
gru_2 (GRU)                     (None, 75, 120)      87120       leaky_re_lu_4[0][0]              
__________________________________________________________________________________________________
gru_5 (GRU)                     (None, 75, 120)      87120       leaky_re_lu_10[0][0]             
__________________________________________________________________________________________________
gru_11 (GRU)                    (None, 75, 120)      87120       leaky_re_lu_22[0][0]             
__________________________________________________________________________________________________
gru_14 (GRU)                    (None, 75, 120)      87120       leaky_re_lu_28[0][0]             
__________________________________________________________________________________________________
gru_17 (GRU)                    (None, 75, 120)      87120       leaky_re_lu_34[0][0]             
__________________________________________________________________________________________________
gru_8 (GRU)                     (None, 75, 120)      87120       leaky_re_lu_16[0][0]             
__________________________________________________________________________________________________
gru_20 (GRU)                    (None, 75, 120)      87120       leaky_re_lu_40[0][0]             
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 75, 120)      480         gru_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 75, 120)      480         gru_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 75, 120)      480         gru_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 75, 120)      480         gru_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 75, 120)      480         gru_17[0][0]                     
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 75, 120)      480         gru_8[0][0]                      
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 75, 120)      480         gru_20[0][0]                     
__________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)       (None, 75, 120)      0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_23 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_29 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_35 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
leaky_re_lu_41 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 75, 840)      0           leaky_re_lu_5[0][0]              
                                                                 leaky_re_lu_11[0][0]             
                                                                 leaky_re_lu_23[0][0]             
                                                                 leaky_re_lu_29[0][0]             
                                                                 leaky_re_lu_35[0][0]             
                                                                 leaky_re_lu_17[0][0]             
                                                                 leaky_re_lu_41[0][0]             
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 75, 60)       453660      concatenate_7[0][0]              
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 75, 60)       240         conv1d_22[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_42 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 75, 120)      43320       leaky_re_lu_42[0][0]             
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 75, 120)      480         conv1d_23[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_43 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 75, 60)       21660       leaky_re_lu_43[0][0]             
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 75, 60)       240         conv1d_24[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_44 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 75, 60)       907260      concatenate_7[0][0]              
__________________________________________________________________________________________________
add (Add)                       (None, 75, 60)       0           leaky_re_lu_44[0][0]             
                                                                 conv1d_21[0][0]                  
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 75, 60)       240         add[0][0]                        
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 75, 60)       32460       batch_normalization_45[0][0]     
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 75, 60)       240         conv1d_25[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_45 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv1d_26 (Conv1D)              (None, 75, 120)      43320       leaky_re_lu_45[0][0]             
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 75, 120)      480         conv1d_26[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_46 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv1d_27 (Conv1D)              (None, 75, 60)       21660       leaky_re_lu_46[0][0]             
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 75, 60)       240         conv1d_27[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_47 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
add_1 (Add)                     (None, 75, 60)       0           leaky_re_lu_47[0][0]             
                                                                 batch_normalization_45[0][0]     
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 75, 60)       240         add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_28 (Conv1D)              (None, 75, 60)       32460       batch_normalization_49[0][0]     
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 75, 60)       240         conv1d_28[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_48 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
conv1d_29 (Conv1D)              (None, 75, 120)      43320       leaky_re_lu_48[0][0]             
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 75, 120)      480         conv1d_29[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_49 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
conv1d_30 (Conv1D)              (None, 75, 60)       21660       leaky_re_lu_49[0][0]             
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 75, 60)       240         conv1d_30[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_50 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
add_2 (Add)                     (None, 75, 60)       0           leaky_re_lu_50[0][0]             
                                                                 batch_normalization_49[0][0]     
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 75, 60)       240         add_2[0][0]                      
__________________________________________________________________________________________________
conv1d_31 (Conv1D)              (None, 75, 60)       32460       batch_normalization_53[0][0]     
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 75, 60)       240         conv1d_31[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_51 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv1d_32 (Conv1D)              (None, 75, 120)      43320       leaky_re_lu_51[0][0]             
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 75, 120)      480         conv1d_32[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_52 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv1d_33 (Conv1D)              (None, 75, 60)       21660       leaky_re_lu_52[0][0]             
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 75, 60)       240         conv1d_33[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_53 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
add_3 (Add)                     (None, 75, 60)       0           leaky_re_lu_53[0][0]             
                                                                 batch_normalization_53[0][0]     
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 75, 60)       240         add_3[0][0]                      
__________________________________________________________________________________________________
conv1d_34 (Conv1D)              (None, 75, 60)       32460       batch_normalization_57[0][0]     
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 75, 60)       240         conv1d_34[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_54 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
conv1d_35 (Conv1D)              (None, 75, 120)      43320       leaky_re_lu_54[0][0]             
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 75, 120)      480         conv1d_35[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_55 (LeakyReLU)      (None, 75, 120)      0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
conv1d_36 (Conv1D)              (None, 75, 60)       21660       leaky_re_lu_55[0][0]             
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 75, 60)       240         conv1d_36[0][0]                  
__________________________________________________________________________________________________
leaky_re_lu_56 (LeakyReLU)      (None, 75, 60)       0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
add_4 (Add)                     (None, 75, 60)       0           leaky_re_lu_56[0][0]             
                                                                 batch_normalization_57[0][0]     
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 75, 60)       240         add_4[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 60)           0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
dense (Dense)                   (None, 61)           3721        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 4,339,521
Trainable params: 4,328,961
Non-trainable params: 10,560
__________________________________________________________________________________________________
