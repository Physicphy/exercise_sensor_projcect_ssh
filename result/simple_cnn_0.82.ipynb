{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import random as rn\n","from my_utils import Workout_dataset\n","\n","import os\n","\n","os.environ['PYTHONHASHSEED'] = str(42)\n","\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n","\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","rn.seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["train_dir = './data/train'\n","eval_dir = './data/train'\n","eval_label_dir = './data/data_y_train.csv'\n","label_dir = './data/data_y_train.csv'\n","\n","def scheduler(epoch, lr):\n","    if (epoch>20) and (lr > 0.00001):\n","        lr = lr*0.9\n","        return lr\n","    else:\n","        return lr\n","\n","train_y = pd.read_csv('./data/data_y_train.csv')  # label load\n","label_dict = dict()\n","for label, label_desc in zip(train_y.label, train_y.label_desc):\n","    label_dict[label] = label_desc\n","# 'Squat (kettlebell / goblet)'에서 [/]를 [,]으로 변경\n","label_dict[45] = 'Squat (kettlebell , goblet)'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","BATCH_SIZE = 64\n","\n","train_loader = Workout_dataset(\n","    train_dir, label_dir, mode='Train',\n","    fold=0, batch_size=BATCH_SIZE, augment=True, shuffle=True)\n","\n","valid_loader = Workout_dataset(\n","    train_dir, label_dir, mode='Valid',\n","    fold=0, batch_size=16, shuffle=True)\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","class_weights = compute_class_weight(\n","    'balanced',\n","    np.unique(train_y.label),\n","    train_y.label)\n","\n","class_weight_dict = dict(zip(\n","    list(range(61)),\n","    class_weights+1\n","    ))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, GlobalAveragePooling1D, BatchNormalization, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.initializers import he_normal"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 600, 3)]     0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 600, 1)]     0                                            \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            [(None, 600, 1)]     0                                            \n__________________________________________________________________________________________________\ninput_5 (InputLayer)            [(None, 600, 3)]     0                                            \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            [(None, 600, 1)]     0                                            \n__________________________________________________________________________________________________\ninput_7 (InputLayer)            [(None, 600, 1)]     0                                            \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 600, 3)]     0                                            \n__________________________________________________________________________________________________\ninput_8 (InputLayer)            [(None, 600, 3)]     0                                            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 600, 5)       0           input_1[0][0]                    \n                                                                 input_2[0][0]                    \n                                                                 input_3[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 600, 5)       0           input_5[0][0]                    \n                                                                 input_6[0][0]                    \n                                                                 input_7[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 600, 4)       0           input_2[0][0]                    \n                                                                 input_6[0][0]                    \n                                                                 input_3[0][0]                    \n                                                                 input_7[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_4 (Concatenate)     (None, 600, 6)       0           input_1[0][0]                    \n                                                                 input_5[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 600, 6)       0           input_4[0][0]                    \n                                                                 input_8[0][0]                    \n__________________________________________________________________________________________________\nconv1d (Conv1D)                 (None, 300, 10)      960         concatenate[0][0]                \n__________________________________________________________________________________________________\nconv1d_2 (Conv1D)               (None, 300, 10)      960         concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv1d_4 (Conv1D)               (None, 300, 10)      770         concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv1d_8 (Conv1D)               (None, 300, 10)      1150        concatenate_4[0][0]              \n__________________________________________________________________________________________________\nconv1d_6 (Conv1D)               (None, 300, 10)      1150        concatenate_3[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 300, 10)      40          conv1d[0][0]                     \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 300, 10)      40          conv1d_2[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 300, 10)      40          conv1d_4[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 300, 10)      40          conv1d_8[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 300, 10)      40          conv1d_6[0][0]                   \n__________________________________________________________________________________________________\ngru (GRU)                       (None, 300, 20)      1920        batch_normalization[0][0]        \n__________________________________________________________________________________________________\ngru_2 (GRU)                     (None, 300, 20)      1920        batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\ngru_4 (GRU)                     (None, 300, 20)      1920        batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\ngru_8 (GRU)                     (None, 300, 20)      1920        batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\ngru_6 (GRU)                     (None, 300, 20)      1920        batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 300, 20)      0           gru[0][0]                        \n__________________________________________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)       (None, 300, 20)      0           gru_2[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)       (None, 300, 20)      0           gru_4[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)       (None, 300, 20)      0           gru_8[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)       (None, 300, 20)      0           gru_6[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 300, 20)      80          leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 300, 20)      80          leaky_re_lu_2[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 300, 20)      80          leaky_re_lu_4[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 300, 20)      80          leaky_re_lu_8[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 300, 20)      80          leaky_re_lu_6[0][0]              \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 150, 60)      3660        batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv1d_3 (Conv1D)               (None, 150, 60)      3660        batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv1d_5 (Conv1D)               (None, 150, 60)      3660        batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv1d_9 (Conv1D)               (None, 150, 60)      3660        batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nconv1d_7 (Conv1D)               (None, 150, 60)      3660        batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 150, 60)      240         conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 150, 60)      240         conv1d_3[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 150, 60)      240         conv1d_5[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 150, 60)      240         conv1d_9[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 150, 60)      240         conv1d_7[0][0]                   \n__________________________________________________________________________________________________\ngru_1 (GRU)                     (None, 150, 60)      21960       batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\ngru_3 (GRU)                     (None, 150, 60)      21960       batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\ngru_5 (GRU)                     (None, 150, 60)      21960       batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\ngru_9 (GRU)                     (None, 150, 60)      21960       batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\ngru_7 (GRU)                     (None, 150, 60)      21960       batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)       (None, 150, 60)      0           gru_1[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)       (None, 150, 60)      0           gru_3[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)       (None, 150, 60)      0           gru_5[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)       (None, 150, 60)      0           gru_9[0][0]                      \n__________________________________________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)       (None, 150, 60)      0           gru_7[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 150, 60)      240         leaky_re_lu_1[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 150, 60)      240         leaky_re_lu_3[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 150, 60)      240         leaky_re_lu_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_19 (BatchNo (None, 150, 60)      240         leaky_re_lu_9[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 150, 60)      240         leaky_re_lu_7[0][0]              \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 150, 300)     0           batch_normalization_3[0][0]      \n                                                                 batch_normalization_7[0][0]      \n                                                                 batch_normalization_11[0][0]     \n                                                                 batch_normalization_19[0][0]     \n                                                                 batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv1d_10 (Conv1D)              (None, 75, 60)       90060       concatenate_5[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_20 (BatchNo (None, 75, 60)       240         conv1d_10[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 60)           0           batch_normalization_20[0][0]     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 61)           3721        global_average_pooling1d[0][0]   \n==================================================================================================\nTotal params: 239,711\nTrainable params: 238,091\nNon-trainable params: 1,620\n__________________________________________________________________________________________________\n"]}],"source":["kernel_size1 = 19\n","kernel_size2 = 3\n","kernel_size3 = 5\n","kk = 10\n","stride = 2\n","\n","acc_input_1 = layers.Input(shape=(600, 3)) # xyz\n","acc_input_2 = layers.Input(shape=(600, 1)) # tot mag\n","acc_input_3 = layers.Input(shape=(600, 1)) # vol\n","acc_input_4 = layers.Input(shape=(600, 3)) # ftt_xyz\n","gy_input_1 = layers.Input(shape=(600, 3)) \n","gy_input_2 = layers.Input(shape=(600, 1)) \n","gy_input_3 = layers.Input(shape=(600, 1))\n","gy_input_4 = layers.Input(shape=(600, 3))\n","\n","\n","\n","# conv1_inputs = layers.concatenate([acc_input_1, acc_input_2, gy_input_1, gy_input_2])\n","conv1_inputs = layers.concatenate([acc_input_1, acc_input_2, acc_input_3])\n","\n","conv1 = Conv1D(\n","    kk,kernel_size1,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(conv1_inputs)\n","conv1 = BatchNormalization()(conv1)\n","#conv1 = Dropout(0.2,seed=42)(conv1)\n","gru1 = layers.GRU(kk*2,return_sequences=True)(conv1)\n","gru1 = layers.LeakyReLU(0.2)(gru1)\n","gru1 = BatchNormalization()(gru1)\n","conv1 = Conv1D(\n","    kk*6,kernel_size2,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(gru1)\n","conv1 = BatchNormalization()(conv1)\n","#conv1 = Dropout(0.2,seed=42)(conv1)\n","gru1 = layers.GRU(kk*6,return_sequences=True)(conv1)\n","gru1 = layers.LeakyReLU(0.2)(gru1)\n","gru1 = BatchNormalization()(gru1)\n","\n","acc_model = Model([acc_input_1, acc_input_2, acc_input_3], gru1)\n","\n","\n","conv2_inputs = layers.concatenate([gy_input_1, gy_input_2, gy_input_3])\n","\n","conv2 = Conv1D(\n","    kk,kernel_size1,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(conv2_inputs)\n","conv2 = BatchNormalization()(conv2)\n","#conv2 = Dropout(0.1,seed=42)(conv2)\n","gru2 = layers.GRU(kk*2,return_sequences=True)(conv2)\n","gru2 = layers.LeakyReLU(0.2)(gru2)\n","gru2 = BatchNormalization()(gru2)\n","conv2 = Conv1D(\n","    kk*6,kernel_size2,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(gru2)\n","conv2 = BatchNormalization()(conv2)\n","#conv2 = Dropout(0.1,seed=42)(conv2)\n","gru2 = layers.GRU(kk*6,return_sequences=True)(conv2)\n","gru2 = layers.LeakyReLU(0.2)(gru2)\n","gru2 = BatchNormalization()(gru2)\n","\n","gy_model = Model([gy_input_1, gy_input_2, gy_input_3], gru2)\n","\n","conv3_inputs = layers.concatenate([acc_input_2,gy_input_2, acc_input_3, gy_input_3])\n","\n","conv3 = Conv1D(\n","    kk,kernel_size1,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(conv3_inputs)\n","conv3 = BatchNormalization()(conv3)\n","#conv3 = Dropout(0.1,seed=42)(conv3)\n","gru3 = layers.GRU(kk*2,return_sequences=True)(conv3)\n","gru3 = layers.LeakyReLU(0.2)(gru3)\n","gru3 = BatchNormalization()(gru3)\n","conv3 = Conv1D(\n","    kk*6,kernel_size2,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(gru3)\n","conv3 = BatchNormalization()(conv3)\n","#conv3 = Dropout(0.1,seed=42)(conv3)\n","gru3 = layers.GRU(kk*6,return_sequences=True)(conv3)\n","gru3 = layers.LeakyReLU(0.2)(gru3)\n","gru3 = BatchNormalization()(gru3)\n","\n","mag_model = Model([acc_input_2,gy_input_2, acc_input_3, gy_input_3], gru3)\n","\n","conv4_inputs = layers.concatenate([acc_input_4, gy_input_4])\n","\n","conv4 = Conv1D(\n","    kk,kernel_size1,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(conv4_inputs)\n","conv4 = BatchNormalization()(conv4)\n","#conv4 = Dropout(0.1,seed=42)(conv4)\n","gru4 = layers.GRU(kk*2,return_sequences=True)(conv4)\n","gru4 = layers.LeakyReLU(0.2)(gru4)\n","gru4 = BatchNormalization()(gru4)\n","conv4 = Conv1D(\n","    kk*6,kernel_size2,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(gru4)\n","conv4 = BatchNormalization()(conv4)\n","#conv4 = Dropout(0.1,seed=42)(conv4)\n","gru4 = layers.GRU(kk*6,return_sequences=True)(conv4)\n","gru4 = layers.LeakyReLU(0.2)(gru4)\n","gru4 = BatchNormalization()(gru4)\n","\n","fft_xyz_model = Model([acc_input_4, gy_input_4], gru4)\n","\n","conv5_inputs = layers.concatenate([acc_input_1, gy_input_1])\n","\n","conv5 = Conv1D(\n","    kk,kernel_size1,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(conv5_inputs)\n","conv5 = BatchNormalization()(conv5)\n","#conv4 = Dropout(0.1,seed=42)(conv4)\n","gru5 = layers.GRU(kk*2,return_sequences=True)(conv5)\n","gru5 = layers.LeakyReLU(0.2)(gru5)\n","gru5 = BatchNormalization()(gru5)\n","conv5 = Conv1D(\n","    kk*6,kernel_size2,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(gru5)\n","conv5 = BatchNormalization()(conv5)\n","#conv4 = Dropout(0.1,seed=42)(conv4)\n","gru5 = layers.GRU(kk*6,return_sequences=True)(conv5)\n","gru5 = layers.LeakyReLU(0.2)(gru5)\n","gru5 = BatchNormalization()(gru5)\n","\n","xyz_model = Model([acc_input_1, gy_input_1], gru5)\n","\n","\n","concat = layers.concatenate([\n","    acc_model.output,gy_model.output,\n","    mag_model.output,xyz_model.output,\n","    fft_xyz_model.output,\n","    ])\n","    \n","conv_tot = Conv1D(\n","    kk*6,kernel_size3,strides=stride,padding='same',activation='elu',\n","    kernel_initializer=he_normal(seed=42)\n","    )(concat)\n","conv_tot = BatchNormalization()(conv_tot)\n","\n","# pool = layers.AveragePooling1D(9,strides=3,padding='same')(conv1)\n","\n","# gru = layers.GRU(60)(concat)\n","# gru = layers.LeakyReLU(0.2)(gru)\n","# gru = BatchNormalization()(gru)\n","#gru = Dropout(0.2,seed=42)(gru)\n","# gru = layers.GRU(120,return_sequences=True)(concat)\n","# gru = layers.LeakyReLU(0.2)(gru)\n","# gru = BatchNormalization()(gru)\n","# gru = Dropout(0.5,seed=42)(gru)\n","# gru = layers.GRU(60)(gru)\n","# gru = layers.LeakyReLU(0.2)(gru)\n","# gru = BatchNormalization()(gru)\n","# gru = Dropout(0.2,seed=42)(gru)\n","# outputs = Dense(61, activation='softmax')(gru)\n","\n","pool = GlobalAveragePooling1D()(conv_tot)\n","outputs = Dense(61, activation='softmax')(pool)\n","\n","model = Model([acc_input_1,gy_input_1,acc_input_2,gy_input_2,acc_input_3,gy_input_3,acc_input_4,gy_input_4],outputs)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":["outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9398 - val_loss: 0.7298 - val_accuracy: 0.7852\n","\n","Epoch 00086: val_loss did not improve from 0.63265\n","Epoch 87/2000\n","34/34 [==============================] - 5s 155ms/step - loss: 0.6127 - accuracy: 0.9403 - val_loss: 0.6724 - val_accuracy: 0.8008\n","\n","Epoch 00087: val_loss did not improve from 0.63265\n","Epoch 88/2000\n","34/34 [==============================] - 6s 162ms/step - loss: 0.7237 - accuracy: 0.9242 - val_loss: 0.6665 - val_accuracy: 0.8008\n","\n","Epoch 00088: val_loss did not improve from 0.63265\n","Epoch 89/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5647 - accuracy: 0.9421 - val_loss: 0.6619 - val_accuracy: 0.8047\n","\n","Epoch 00089: val_loss did not improve from 0.63265\n","Epoch 90/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6283 - accuracy: 0.9439 - val_loss: 0.7113 - val_accuracy: 0.7930\n","\n","Epoch 00090: val_loss did not improve from 0.63265\n","Epoch 91/2000\n","34/34 [==============================] - 5s 153ms/step - loss: 0.5391 - accuracy: 0.9527 - val_loss: 0.7275 - val_accuracy: 0.7930\n","\n","Epoch 00091: val_loss did not improve from 0.63265\n","Epoch 92/2000\n","34/34 [==============================] - 5s 153ms/step - loss: 0.7214 - accuracy: 0.9251 - val_loss: 0.7091 - val_accuracy: 0.8008\n","\n","Epoch 00092: val_loss did not improve from 0.63265\n","Epoch 93/2000\n","34/34 [==============================] - 5s 155ms/step - loss: 0.5060 - accuracy: 0.9568 - val_loss: 0.6541 - val_accuracy: 0.8125\n","\n","Epoch 00093: val_loss did not improve from 0.63265\n","Epoch 94/2000\n","34/34 [==============================] - 7s 193ms/step - loss: 0.6232 - accuracy: 0.9508 - val_loss: 0.6917 - val_accuracy: 0.8008\n","\n","Epoch 00094: val_loss did not improve from 0.63265\n","Epoch 95/2000\n","34/34 [==============================] - 6s 162ms/step - loss: 0.7023 - accuracy: 0.9311 - val_loss: 0.7027 - val_accuracy: 0.7930\n","\n","Epoch 00095: val_loss did not improve from 0.63265\n","Epoch 96/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5861 - accuracy: 0.9449 - val_loss: 0.6793 - val_accuracy: 0.8047\n","\n","Epoch 00096: val_loss did not improve from 0.63265\n","Epoch 97/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5727 - accuracy: 0.9458 - val_loss: 0.7178 - val_accuracy: 0.7969\n","\n","Epoch 00097: val_loss did not improve from 0.63265\n","Epoch 98/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6125 - accuracy: 0.9412 - val_loss: 0.6291 - val_accuracy: 0.8086\n","\n","Epoch 00098: val_loss improved from 0.63265 to 0.62908, saving model to simple_cnn_best.hdf5\n","Epoch 99/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5821 - accuracy: 0.9444 - val_loss: 0.7081 - val_accuracy: 0.7969\n","\n","Epoch 00099: val_loss did not improve from 0.62908\n","Epoch 100/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.6370 - accuracy: 0.9338 - val_loss: 0.6767 - val_accuracy: 0.7930\n","\n","Epoch 00100: val_loss did not improve from 0.62908\n","Epoch 101/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.6975 - accuracy: 0.9260 - val_loss: 0.7077 - val_accuracy: 0.7930\n","\n","Epoch 00101: val_loss did not improve from 0.62908\n","Epoch 102/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6651 - accuracy: 0.9329 - val_loss: 0.6996 - val_accuracy: 0.7969\n","\n","Epoch 00102: val_loss did not improve from 0.62908\n","Epoch 103/2000\n","34/34 [==============================] - 5s 142ms/step - loss: 0.5645 - accuracy: 0.9449 - val_loss: 0.7042 - val_accuracy: 0.8086\n","\n","Epoch 00103: val_loss did not improve from 0.62908\n","Epoch 104/2000\n","34/34 [==============================] - 5s 142ms/step - loss: 0.5548 - accuracy: 0.9472 - val_loss: 0.7228 - val_accuracy: 0.8008\n","\n","Epoch 00104: val_loss did not improve from 0.62908\n","Epoch 105/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5955 - accuracy: 0.9439 - val_loss: 0.7037 - val_accuracy: 0.8047\n","\n","Epoch 00105: val_loss did not improve from 0.62908\n","Epoch 106/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.6261 - accuracy: 0.9416 - val_loss: 0.6977 - val_accuracy: 0.8008\n","\n","Epoch 00106: val_loss did not improve from 0.62908\n","Epoch 107/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5820 - accuracy: 0.9494 - val_loss: 0.7315 - val_accuracy: 0.7930\n","\n","Epoch 00107: val_loss did not improve from 0.62908\n","Epoch 108/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5881 - accuracy: 0.9453 - val_loss: 0.6825 - val_accuracy: 0.8047\n","\n","Epoch 00108: val_loss did not improve from 0.62908\n","Epoch 109/2000\n","34/34 [==============================] - 6s 169ms/step - loss: 0.5553 - accuracy: 0.9458 - val_loss: 0.6859 - val_accuracy: 0.7969\n","\n","Epoch 00109: val_loss did not improve from 0.62908\n","Epoch 110/2000\n","34/34 [==============================] - 6s 173ms/step - loss: 0.6144 - accuracy: 0.9380 - val_loss: 0.7003 - val_accuracy: 0.8008\n","\n","Epoch 00110: val_loss did not improve from 0.62908\n","Epoch 111/2000\n","34/34 [==============================] - 7s 190ms/step - loss: 0.5397 - accuracy: 0.9522 - val_loss: 0.6920 - val_accuracy: 0.7969\n","\n","Epoch 00111: val_loss did not improve from 0.62908\n","Epoch 112/2000\n","34/34 [==============================] - 5s 142ms/step - loss: 0.5326 - accuracy: 0.9513 - val_loss: 0.6867 - val_accuracy: 0.8047\n","\n","Epoch 00112: val_loss did not improve from 0.62908\n","Epoch 113/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.6026 - accuracy: 0.9370 - val_loss: 0.6925 - val_accuracy: 0.8008\n","\n","Epoch 00113: val_loss did not improve from 0.62908\n","Epoch 114/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5585 - accuracy: 0.9439 - val_loss: 0.7248 - val_accuracy: 0.7930\n","\n","Epoch 00114: val_loss did not improve from 0.62908\n","Epoch 115/2000\n","34/34 [==============================] - 5s 157ms/step - loss: 0.6504 - accuracy: 0.9334 - val_loss: 0.6753 - val_accuracy: 0.8086\n","\n","Epoch 00115: val_loss did not improve from 0.62908\n","Epoch 116/2000\n","34/34 [==============================] - 7s 201ms/step - loss: 0.5676 - accuracy: 0.9426 - val_loss: 0.7011 - val_accuracy: 0.7969\n","\n","Epoch 00116: val_loss did not improve from 0.62908\n","Epoch 117/2000\n","34/34 [==============================] - 6s 164ms/step - loss: 0.5940 - accuracy: 0.9462 - val_loss: 0.6901 - val_accuracy: 0.7969\n","\n","Epoch 00117: val_loss did not improve from 0.62908\n","Epoch 118/2000\n","34/34 [==============================] - 6s 164ms/step - loss: 0.6302 - accuracy: 0.9393 - val_loss: 0.7211 - val_accuracy: 0.7969\n","\n","Epoch 00118: val_loss did not improve from 0.62908\n","Epoch 119/2000\n","34/34 [==============================] - 6s 167ms/step - loss: 0.6711 - accuracy: 0.9338 - val_loss: 0.7065 - val_accuracy: 0.7930\n","\n","Epoch 00119: val_loss did not improve from 0.62908\n","Epoch 120/2000\n","34/34 [==============================] - 5s 156ms/step - loss: 0.5619 - accuracy: 0.9426 - val_loss: 0.6750 - val_accuracy: 0.8047\n","\n","Epoch 00120: val_loss did not improve from 0.62908\n","Epoch 121/2000\n","34/34 [==============================] - 5s 154ms/step - loss: 0.5793 - accuracy: 0.9449 - val_loss: 0.7038 - val_accuracy: 0.8047\n","\n","Epoch 00121: val_loss did not improve from 0.62908\n","Epoch 122/2000\n","34/34 [==============================] - 6s 171ms/step - loss: 0.5475 - accuracy: 0.9472 - val_loss: 0.7096 - val_accuracy: 0.8086\n","\n","Epoch 00122: val_loss did not improve from 0.62908\n","Epoch 123/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.6008 - accuracy: 0.9449 - val_loss: 0.7030 - val_accuracy: 0.7969\n","\n","Epoch 00123: val_loss did not improve from 0.62908\n","Epoch 124/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6533 - accuracy: 0.9334 - val_loss: 0.7232 - val_accuracy: 0.7852\n","\n","Epoch 00124: val_loss did not improve from 0.62908\n","Epoch 125/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5538 - accuracy: 0.9481 - val_loss: 0.6727 - val_accuracy: 0.8086\n","\n","Epoch 00125: val_loss did not improve from 0.62908\n","Epoch 126/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.6231 - accuracy: 0.9352 - val_loss: 0.7237 - val_accuracy: 0.7891\n","\n","Epoch 00126: val_loss did not improve from 0.62908\n","Epoch 127/2000\n","34/34 [==============================] - 6s 158ms/step - loss: 0.6192 - accuracy: 0.9458 - val_loss: 0.6960 - val_accuracy: 0.8008\n","\n","Epoch 00127: val_loss did not improve from 0.62908\n","Epoch 128/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5455 - accuracy: 0.9472 - val_loss: 0.7198 - val_accuracy: 0.7969\n","\n","Epoch 00128: val_loss did not improve from 0.62908\n","Epoch 129/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5739 - accuracy: 0.9430 - val_loss: 0.6937 - val_accuracy: 0.8047\n","\n","Epoch 00129: val_loss did not improve from 0.62908\n","Epoch 130/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5049 - accuracy: 0.9554 - val_loss: 0.6807 - val_accuracy: 0.8047\n","\n","Epoch 00130: val_loss did not improve from 0.62908\n","Epoch 131/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.6138 - accuracy: 0.9416 - val_loss: 0.6955 - val_accuracy: 0.7969\n","\n","Epoch 00131: val_loss did not improve from 0.62908\n","Epoch 132/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5675 - accuracy: 0.9430 - val_loss: 0.6538 - val_accuracy: 0.8086\n","\n","Epoch 00132: val_loss did not improve from 0.62908\n","Epoch 133/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5557 - accuracy: 0.9476 - val_loss: 0.6846 - val_accuracy: 0.7969\n","\n","Epoch 00133: val_loss did not improve from 0.62908\n","Epoch 134/2000\n","34/34 [==============================] - 5s 147ms/step - loss: 0.6512 - accuracy: 0.9380 - val_loss: 0.7042 - val_accuracy: 0.8008\n","\n","Epoch 00134: val_loss did not improve from 0.62908\n","Epoch 135/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.4978 - accuracy: 0.9522 - val_loss: 0.7195 - val_accuracy: 0.7930\n","\n","Epoch 00135: val_loss did not improve from 0.62908\n","Epoch 136/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5133 - accuracy: 0.9467 - val_loss: 0.6138 - val_accuracy: 0.8086\n","\n","Epoch 00136: val_loss improved from 0.62908 to 0.61376, saving model to simple_cnn_best.hdf5\n","Epoch 137/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.6168 - accuracy: 0.9361 - val_loss: 0.6897 - val_accuracy: 0.8008\n","\n","Epoch 00137: val_loss did not improve from 0.61376\n","Epoch 138/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.6839 - accuracy: 0.9274 - val_loss: 0.7150 - val_accuracy: 0.7852\n","\n","Epoch 00138: val_loss did not improve from 0.61376\n","Epoch 139/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5749 - accuracy: 0.9403 - val_loss: 0.6988 - val_accuracy: 0.7930\n","\n","Epoch 00139: val_loss did not improve from 0.61376\n","Epoch 140/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5800 - accuracy: 0.9476 - val_loss: 0.7342 - val_accuracy: 0.7852\n","\n","Epoch 00140: val_loss did not improve from 0.61376\n","Epoch 141/2000\n","34/34 [==============================] - 5s 149ms/step - loss: 0.6187 - accuracy: 0.9384 - val_loss: 0.6940 - val_accuracy: 0.7969\n","\n","Epoch 00141: val_loss did not improve from 0.61376\n","Epoch 142/2000\n","34/34 [==============================] - 5s 142ms/step - loss: 0.5677 - accuracy: 0.9412 - val_loss: 0.7196 - val_accuracy: 0.7891\n","\n","Epoch 00142: val_loss did not improve from 0.61376\n","Epoch 143/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.4786 - accuracy: 0.9586 - val_loss: 0.6688 - val_accuracy: 0.8008\n","\n","Epoch 00143: val_loss did not improve from 0.61376\n","Epoch 144/2000\n","34/34 [==============================] - 5s 145ms/step - loss: 0.5538 - accuracy: 0.9481 - val_loss: 0.6631 - val_accuracy: 0.8047\n","\n","Epoch 00144: val_loss did not improve from 0.61376\n","Epoch 145/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5315 - accuracy: 0.9545 - val_loss: 0.6987 - val_accuracy: 0.7969\n","\n","Epoch 00145: val_loss did not improve from 0.61376\n","Epoch 146/2000\n","34/34 [==============================] - 6s 176ms/step - loss: 0.6151 - accuracy: 0.9389 - val_loss: 0.7172 - val_accuracy: 0.7969\n","\n","Epoch 00146: val_loss did not improve from 0.61376\n","Epoch 147/2000\n","34/34 [==============================] - 5s 142ms/step - loss: 0.6774 - accuracy: 0.9347 - val_loss: 0.7272 - val_accuracy: 0.7930\n","\n","Epoch 00147: val_loss did not improve from 0.61376\n","Epoch 148/2000\n","34/34 [==============================] - 6s 161ms/step - loss: 0.6163 - accuracy: 0.9412 - val_loss: 0.7216 - val_accuracy: 0.8008\n","\n","Epoch 00148: val_loss did not improve from 0.61376\n","Epoch 149/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.6019 - accuracy: 0.9435 - val_loss: 0.7264 - val_accuracy: 0.7969\n","\n","Epoch 00149: val_loss did not improve from 0.61376\n","Epoch 150/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.5690 - accuracy: 0.9453 - val_loss: 0.7237 - val_accuracy: 0.8008\n","\n","Epoch 00150: val_loss did not improve from 0.61376\n","Epoch 151/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5539 - accuracy: 0.9444 - val_loss: 0.7357 - val_accuracy: 0.7852\n","\n","Epoch 00151: val_loss did not improve from 0.61376\n","Epoch 152/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6105 - accuracy: 0.9416 - val_loss: 0.7111 - val_accuracy: 0.7930\n","\n","Epoch 00152: val_loss did not improve from 0.61376\n","Epoch 153/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5693 - accuracy: 0.9467 - val_loss: 0.6936 - val_accuracy: 0.7969\n","\n","Epoch 00153: val_loss did not improve from 0.61376\n","Epoch 154/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.6276 - accuracy: 0.9389 - val_loss: 0.7224 - val_accuracy: 0.7891\n","\n","Epoch 00154: val_loss did not improve from 0.61376\n","Epoch 155/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5155 - accuracy: 0.9527 - val_loss: 0.6750 - val_accuracy: 0.8008\n","\n","Epoch 00155: val_loss did not improve from 0.61376\n","Epoch 156/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.6071 - accuracy: 0.9370 - val_loss: 0.7196 - val_accuracy: 0.7891\n","\n","Epoch 00156: val_loss did not improve from 0.61376\n","Epoch 157/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5539 - accuracy: 0.9517 - val_loss: 0.6477 - val_accuracy: 0.8047\n","\n","Epoch 00157: val_loss did not improve from 0.61376\n","Epoch 158/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5691 - accuracy: 0.9449 - val_loss: 0.7178 - val_accuracy: 0.7930\n","\n","Epoch 00158: val_loss did not improve from 0.61376\n","Epoch 159/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6162 - accuracy: 0.9370 - val_loss: 0.7182 - val_accuracy: 0.7969\n","\n","Epoch 00159: val_loss did not improve from 0.61376\n","Epoch 160/2000\n","34/34 [==============================] - 5s 150ms/step - loss: 0.5330 - accuracy: 0.9536 - val_loss: 0.6984 - val_accuracy: 0.8047\n","\n","Epoch 00160: val_loss did not improve from 0.61376\n","Epoch 161/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5708 - accuracy: 0.9476 - val_loss: 0.7097 - val_accuracy: 0.8008\n","\n","Epoch 00161: val_loss did not improve from 0.61376\n","Epoch 162/2000\n","34/34 [==============================] - 5s 146ms/step - loss: 0.5415 - accuracy: 0.9467 - val_loss: 0.7101 - val_accuracy: 0.7969\n","\n","Epoch 00162: val_loss did not improve from 0.61376\n","Epoch 163/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6184 - accuracy: 0.9380 - val_loss: 0.6809 - val_accuracy: 0.7930\n","\n","Epoch 00163: val_loss did not improve from 0.61376\n","Epoch 164/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5111 - accuracy: 0.9494 - val_loss: 0.6649 - val_accuracy: 0.8086\n","\n","Epoch 00164: val_loss did not improve from 0.61376\n","Epoch 165/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.5727 - accuracy: 0.9472 - val_loss: 0.7110 - val_accuracy: 0.7969\n","\n","Epoch 00165: val_loss did not improve from 0.61376\n","Epoch 166/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6059 - accuracy: 0.9449 - val_loss: 0.7265 - val_accuracy: 0.7891\n","\n","Epoch 00166: val_loss did not improve from 0.61376\n","Epoch 167/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.5579 - accuracy: 0.9426 - val_loss: 0.7108 - val_accuracy: 0.7930\n","\n","Epoch 00167: val_loss did not improve from 0.61376\n","Epoch 168/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5974 - accuracy: 0.9384 - val_loss: 0.6902 - val_accuracy: 0.8047\n","\n","Epoch 00168: val_loss did not improve from 0.61376\n","Epoch 169/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.5933 - accuracy: 0.9458 - val_loss: 0.7232 - val_accuracy: 0.7891\n","\n","Epoch 00169: val_loss did not improve from 0.61376\n","Epoch 170/2000\n","34/34 [==============================] - 5s 145ms/step - loss: 0.5696 - accuracy: 0.9462 - val_loss: 0.6709 - val_accuracy: 0.8047\n","\n","Epoch 00170: val_loss did not improve from 0.61376\n","Epoch 171/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.4781 - accuracy: 0.9550 - val_loss: 0.6825 - val_accuracy: 0.8086\n","\n","Epoch 00171: val_loss did not improve from 0.61376\n","Epoch 172/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6941 - accuracy: 0.9329 - val_loss: 0.7264 - val_accuracy: 0.7891\n","\n","Epoch 00172: val_loss did not improve from 0.61376\n","Epoch 173/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.5549 - accuracy: 0.9426 - val_loss: 0.7327 - val_accuracy: 0.7891\n","\n","Epoch 00173: val_loss did not improve from 0.61376\n","Epoch 174/2000\n","34/34 [==============================] - 5s 136ms/step - loss: 0.5850 - accuracy: 0.9435 - val_loss: 0.7084 - val_accuracy: 0.7891\n","\n","Epoch 00174: val_loss did not improve from 0.61376\n","Epoch 175/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6546 - accuracy: 0.9320 - val_loss: 0.7220 - val_accuracy: 0.7852\n","\n","Epoch 00175: val_loss did not improve from 0.61376\n","Epoch 176/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.6055 - accuracy: 0.9407 - val_loss: 0.6795 - val_accuracy: 0.8008\n","\n","Epoch 00176: val_loss did not improve from 0.61376\n","Epoch 177/2000\n","34/34 [==============================] - 5s 137ms/step - loss: 0.6606 - accuracy: 0.9320 - val_loss: 0.7151 - val_accuracy: 0.8008\n","\n","Epoch 00177: val_loss did not improve from 0.61376\n","Epoch 178/2000\n","34/34 [==============================] - 6s 168ms/step - loss: 0.6143 - accuracy: 0.9412 - val_loss: 0.7323 - val_accuracy: 0.7852\n","\n","Epoch 00178: val_loss did not improve from 0.61376\n","Epoch 179/2000\n","34/34 [==============================] - 6s 171ms/step - loss: 0.5242 - accuracy: 0.9485 - val_loss: 0.6576 - val_accuracy: 0.8047\n","\n","Epoch 00179: val_loss did not improve from 0.61376\n","Epoch 180/2000\n","34/34 [==============================] - 5s 150ms/step - loss: 0.5286 - accuracy: 0.9485 - val_loss: 0.6975 - val_accuracy: 0.8008\n","\n","Epoch 00180: val_loss did not improve from 0.61376\n","Epoch 181/2000\n","34/34 [==============================] - 5s 143ms/step - loss: 0.5241 - accuracy: 0.9531 - val_loss: 0.6780 - val_accuracy: 0.7969\n","\n","Epoch 00181: val_loss did not improve from 0.61376\n","Epoch 182/2000\n","34/34 [==============================] - 5s 141ms/step - loss: 0.6756 - accuracy: 0.9311 - val_loss: 0.6921 - val_accuracy: 0.8008\n","\n","Epoch 00182: val_loss did not improve from 0.61376\n","Epoch 183/2000\n","34/34 [==============================] - 5s 140ms/step - loss: 0.6134 - accuracy: 0.9370 - val_loss: 0.7224 - val_accuracy: 0.7891\n","\n","Epoch 00183: val_loss did not improve from 0.61376\n","Epoch 184/2000\n","34/34 [==============================] - 5s 139ms/step - loss: 0.5223 - accuracy: 0.9490 - val_loss: 0.7013 - val_accuracy: 0.7969\n","\n","Epoch 00184: val_loss did not improve from 0.61376\n","Epoch 185/2000\n","34/34 [==============================] - 5s 138ms/step - loss: 0.6321 - accuracy: 0.9380 - val_loss: 0.6968 - val_accuracy: 0.8086\n","\n","Epoch 00185: val_loss did not improve from 0.61376\n","Epoch 186/2000\n","34/34 [==============================] - 6s 181ms/step - loss: 0.5498 - accuracy: 0.9527 - val_loss: 0.6853 - val_accuracy: 0.8047\n","\n","Epoch 00186: val_loss did not improve from 0.61376\n","Epoch 00186: early stopping\n"]}],"source":["checkpoint_filepath = \"simple_cnn_best.hdf5\"\n","\n","save_best = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n","    save_weights_only=True, mode='auto', save_freq='epoch', options=None)\n","\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',min_delta=0.0001,\n","    patience=50,verbose=1)\n","\n","history = model.fit_generator(\n","    generator=train_loader,\n","    validation_data=valid_loader,\n","    epochs=2000,\n","    callbacks=[save_best,early_stop,lr_scheduler],\n","    class_weight=class_weight_dict)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["test_dir = './data/test'\n","test_label_dir = './data/data_y_test.csv'"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["test_loader = Workout_dataset(\n","        test_dir, test_label_dir, mode='Test', batch_size=625, shuffle=False)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1973: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n","  warnings.warn('`Model.evaluate_generator` is deprecated and '\n","1/1 [==============================] - 1s 1s/step - loss: 0.5896 - accuracy: 0.8208\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5895500183105469, 0.8208000063896179]"]},"metadata":{},"execution_count":10}],"source":["model.load_weights(checkpoint_filepath)\n","model.evaluate_generator(generator=test_loader,verbose=1)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":2},"orig_nbformat":4}}